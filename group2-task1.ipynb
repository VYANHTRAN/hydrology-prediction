{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6db987b",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: ENVIRONMENT SETUP & IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# --- Standard Library ---\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# --- Data Processing ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "# --- Deep Learning ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Progress Bar ---\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Suppress Warnings ---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# REPRODUCIBILITY & DEVICE SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device(use_cuda: bool = True) -> torch.device:\n",
    "    \"\"\"Get the best available device.\"\"\"\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def maybe_compile_model(model: nn.Module, enable: bool = True) -> nn.Module:\n",
    "    \"\"\"Apply torch.compile() for PyTorch 2.0+ optimization (optional).\"\"\"\n",
    "    if enable and hasattr(torch, 'compile'):\n",
    "        try:\n",
    "            model = torch.compile(model, mode='reduce-overhead')\n",
    "            print(\"Model compiled with torch.compile()\")\n",
    "        except Exception as e:\n",
    "            print(f\"torch.compile() failed, using eager mode: {e}\")\n",
    "    return model\n",
    "\n",
    "# --- Initialize ---\n",
    "SEED = 4321 \n",
    "set_seed(SEED)\n",
    "DEVICE = get_device(use_cuda=True)\n",
    "\n",
    "print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de37d5",
   "metadata": {},
   "source": [
    "## Section 2: Base Configuration & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: BASE CONFIGURATION & UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# --- PATH CONFIGURATION ---\n",
    "# All paths relative to notebook location\n",
    "BASE_DIR = Path('./basin_dataset_public')\n",
    "FORCING_DIR = BASE_DIR / 'basin_mean_forcing' / 'nldas'\n",
    "FLOW_DIR = BASE_DIR / 'usgs_streamflow'\n",
    "BAD_BASINS_FILE = BASE_DIR / 'basin_size_errors_10_percent.txt'\n",
    "CACHE_DIR = Path('./cache')\n",
    "RESULTS_DIR = Path('./results')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- PHYSICAL CONSTANTS ---\n",
    "CFS_TO_CMS = 0.0283168  # Cubic feet/second to cubic meters/second\n",
    "\n",
    "# --- DATA SPLIT DATES (Hydrological Years) ---\n",
    "TRAIN_START = '1980-10-01'\n",
    "TRAIN_END = '1995-09-30'\n",
    "VAL_START = '1995-10-01'\n",
    "VAL_END = '2000-09-30'\n",
    "TEST_START = '2000-10-01'\n",
    "TEST_END = '2010-09-30'\n",
    "\n",
    "# --- FEATURE DEFINITIONS ---\n",
    "# Pure forcing features (known from weather forecasts)\n",
    "FORCING_FEATURES = ['PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp']\n",
    "\n",
    "# All dynamic input features (forcing + engineered + autoregressive)\n",
    "DYNAMIC_FEATURES = [\n",
    "    'PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp',  # Forcing\n",
    "    'PRCP_roll3', 'PRCP_roll7',             # Rolling precipitation\n",
    "    'Q_lag1', 'Q_lag2', 'Q_lag3'            # Lagged flow\n",
    "]\n",
    "\n",
    "# Static basin attributes\n",
    "STATIC_FEATURES = [\n",
    "    'area_gages2', 'elev_mean', 'slope_mean',\n",
    "    'sand_frac', 'clay_frac', 'frac_forest',\n",
    "    'lai_max', 'p_mean', 'aridity'\n",
    "]\n",
    "\n",
    "TARGET = 'Q_cms'\n",
    "\n",
    "# =============================================================================\n",
    "# CACHING UTILITIES (with auto-invalidation)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_config_hash(config_dict: Dict) -> str:\n",
    "    \"\"\"Compute hash of configuration for cache invalidation.\"\"\"\n",
    "    config_str = json.dumps(config_dict, sort_keys=True, default=str)\n",
    "    return hashlib.md5(config_str.encode()).hexdigest()[:8]\n",
    "\n",
    "def get_or_create_cache(\n",
    "    cache_name: str,\n",
    "    create_fn: callable,\n",
    "    config_dict: Dict,\n",
    "    *args, **kwargs\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Generic caching utility with auto-invalidation based on config hash.\n",
    "    \n",
    "    Args:\n",
    "        cache_name: Base name for cache file (without extension)\n",
    "        create_fn: Function to call if cache miss\n",
    "        config_dict: Configuration dict for invalidation check\n",
    "        *args, **kwargs: Arguments to pass to create_fn\n",
    "    \n",
    "    Returns:\n",
    "        Cached or freshly computed data\n",
    "    \"\"\"\n",
    "    config_hash = compute_config_hash(config_dict)\n",
    "    cache_path = CACHE_DIR / f\"{cache_name}_{config_hash}.pkl\"\n",
    "    \n",
    "    if cache_path.exists():\n",
    "        print(f\"    Loading cached data from {cache_path.name}\")\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    print(f\"    Computing {cache_name} (will be cached)...\")\n",
    "    data = create_fn(*args, **kwargs)\n",
    "    \n",
    "    # Clean old caches with same base name\n",
    "    for old_cache in CACHE_DIR.glob(f\"{cache_name}_*.pkl\"):\n",
    "        if old_cache != cache_path:\n",
    "            old_cache.unlink()\n",
    "            print(f\"    Removed outdated cache: {old_cache.name}\")\n",
    "    \n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"    Cached to {cache_path.name}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# =============================================================================\n",
    "# METRIC UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def calc_nse(obs: np.ndarray, sim: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Nash-Sutcliffe Efficiency.\n",
    "    \n",
    "    NSE = 1 - (sum((obs - sim)^2) / sum((obs - mean(obs))^2))\n",
    "    \n",
    "    Returns:\n",
    "        NSE value in range (-inf, 1], where 1 is perfect prediction\n",
    "    \"\"\"\n",
    "    obs = np.asarray(obs).flatten()\n",
    "    sim = np.asarray(sim).flatten()\n",
    "    \n",
    "    # Remove NaN pairs\n",
    "    mask = ~(np.isnan(obs) | np.isnan(sim))\n",
    "    obs, sim = obs[mask], sim[mask]\n",
    "    \n",
    "    if len(obs) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2) + 1e-10\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    return float(1 - (numerator / denominator))\n",
    "\n",
    "def calc_metrics(obs: np.ndarray, sim: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Calculate comprehensive regression metrics.\"\"\"\n",
    "    obs = np.asarray(obs).flatten()\n",
    "    sim = np.asarray(sim).flatten()\n",
    "    \n",
    "    # Remove NaN pairs\n",
    "    mask = ~(np.isnan(obs) | np.isnan(sim))\n",
    "    obs, sim = obs[mask], sim[mask]\n",
    "    \n",
    "    if len(obs) == 0:\n",
    "        return {'NSE': np.nan, 'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan, 'Bias': np.nan}\n",
    "    \n",
    "    nse = calc_nse(obs, sim)\n",
    "    mse = np.mean((sim - obs) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(sim - obs))\n",
    "    \n",
    "    # R² (coefficient of determination)\n",
    "    ss_res = np.sum((obs - sim) ** 2)\n",
    "    ss_tot = np.sum((obs - np.mean(obs)) ** 2) + 1e-10\n",
    "    r2 = float(1 - ss_res / ss_tot)\n",
    "    \n",
    "    # Bias (mean error)\n",
    "    bias = float(np.mean(sim - obs))\n",
    "    \n",
    "    return {\n",
    "        'NSE': float(round(nse, 4)),\n",
    "        'RMSE': float(round(rmse, 4)),\n",
    "        'MAE': float(round(mae, 4)),\n",
    "        'R2': float(round(r2, 4)),\n",
    "        'Bias': float(round(bias, 4))\n",
    "    }\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    return obj\n",
    "\n",
    "# =============================================================================\n",
    "# DATALOADER FACTORY (Optimized)\n",
    "# =============================================================================\n",
    "\n",
    "def create_dataloader(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = True\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create optimized DataLoader from numpy arrays.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (N, Seq, Features) or tuple for Seq2Seq\n",
    "        y: Targets (N,) or (N, Steps)\n",
    "        batch_size: Batch size\n",
    "        shuffle: Whether to shuffle data\n",
    "        num_workers: Number of parallel workers (0 for Windows compatibility)\n",
    "        pin_memory: Pin memory for faster GPU transfer\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch DataLoader\n",
    "    \"\"\"\n",
    "    if isinstance(X, tuple):\n",
    "        # Seq2Seq case: (X_past, X_future, Static)\n",
    "        tensors = [torch.FloatTensor(arr) for arr in X]\n",
    "        tensors.append(torch.FloatTensor(y))\n",
    "        dataset = TensorDataset(*tensors)\n",
    "    else:\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    \n",
    "    # Windows compatibility: num_workers=0 avoids multiprocessing issues\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory if torch.cuda.is_available() else False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE RESULTS EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "def save_comprehensive_results(\n",
    "    task_name: str,\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    config: Dict,\n",
    "    metrics: Dict,\n",
    "    history: Dict,\n",
    "    predictions: Dict,\n",
    "    epoch: int,\n",
    "    scheduler: Optional[Any] = None\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Save comprehensive experiment results with timestamp.\n",
    "    \n",
    "    Creates a timestamped folder with:\n",
    "    - best_model.pth (model + optimizer + scheduler state)\n",
    "    - config.json (full hyperparameters)\n",
    "    - metrics.json (train/val/test metrics)\n",
    "    - training_history.json (per-epoch logs)\n",
    "    - predictions.npz (predictions + ground truth)\n",
    "    - environment.json (system info)\n",
    "    \n",
    "    Returns:\n",
    "        Path to results folder\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = RESULTS_DIR / task_name / f'run_{timestamp}'\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Save Model Checkpoint (complete state)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'config': config\n",
    "    }\n",
    "    if scheduler is not None:\n",
    "        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "    torch.save(checkpoint, run_dir / 'best_model.pth')\n",
    "    \n",
    "    # 2. Save Configuration\n",
    "    with open(run_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2, default=str)\n",
    "    \n",
    "    # 3. Save Metrics\n",
    "    with open(run_dir / 'metrics.json', 'w') as f:\n",
    "        json.dump(convert_to_serializable(metrics), f, indent=2)\n",
    "    \n",
    "    # 4. Save Training History\n",
    "    with open(run_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(convert_to_serializable(history), f, indent=2)\n",
    "    \n",
    "    # 5. Save Predictions\n",
    "    np.savez(\n",
    "        run_dir / 'predictions.npz',\n",
    "        **predictions\n",
    "    )\n",
    "    \n",
    "    # 6. Save Environment Info\n",
    "    env_info = {\n",
    "        'timestamp': timestamp,\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None,\n",
    "        'device': str(DEVICE),\n",
    "        'random_seed': SEED,\n",
    "        'numpy_version': np.__version__,\n",
    "        'pandas_version': pd.__version__\n",
    "    }\n",
    "    with open(run_dir / 'environment.json', 'w') as f:\n",
    "        json.dump(env_info, f, indent=2)\n",
    "    \n",
    "    # 7. Append to experiment summary CSV\n",
    "    summary_file = RESULTS_DIR / task_name / 'experiments_summary.csv'\n",
    "    summary_row = {\n",
    "        'timestamp': timestamp,\n",
    "        'run_dir': str(run_dir),\n",
    "        **{f'config_{k}': v for k, v in config.items() if not isinstance(v, (list, dict))},\n",
    "        **{f'test_{k}': v for k, v in metrics.get('test', {}).items()}\n",
    "    }\n",
    "    \n",
    "    if summary_file.exists():\n",
    "        df_summary = pd.read_csv(summary_file)\n",
    "        df_summary = pd.concat([df_summary, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "    else:\n",
    "        df_summary = pd.DataFrame([summary_row])\n",
    "    df_summary.to_csv(summary_file, index=False)\n",
    "    \n",
    "    print(f\"\\n  Results saved to: {run_dir}\")\n",
    "    return run_dir\n",
    "\n",
    "print(\"Section 2: Base Configuration & Utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af253bc8",
   "metadata": {},
   "source": [
    "## Section 3: Data Loading & Feature Engineering Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: DATA LOADING & FEATURE ENGINEERING CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "class CamelsLoader:\n",
    "    \"\"\"\n",
    "    Loads CAMELS dataset: dynamic forcing data and static basin attributes.\n",
    "    \n",
    "    Methods:\n",
    "        load_bad_basins(): Get list of basins to exclude\n",
    "        get_basin_list(): Scan directories for valid basins\n",
    "        load_dynamic_data(): Load streamflow + forcing for a basin\n",
    "        load_static_attributes(): Load static catchment attributes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_dir = BASE_DIR\n",
    "        self.forcing_dir = FORCING_DIR\n",
    "        self.flow_dir = FLOW_DIR\n",
    "        self.bad_basins_file = BAD_BASINS_FILE\n",
    "    \n",
    "    def load_bad_basins(self) -> List[str]:\n",
    "        \"\"\"Returns a list of basin IDs to exclude (size errors > 10%).\"\"\"\n",
    "        if not self.bad_basins_file.exists():\n",
    "            return []\n",
    "        \n",
    "        bad_ids = []\n",
    "        try:\n",
    "            with open(self.bad_basins_file, 'r') as f:\n",
    "                next(f)  # Skip header\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        bad_ids.append(parts[1])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to parse bad basins file: {e}\")\n",
    "        return bad_ids\n",
    "    \n",
    "    def get_basin_list(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scans directories and filters out bad basins.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns ['gauge_id', 'region']\n",
    "        \"\"\"\n",
    "        bad_basins = self.load_bad_basins()\n",
    "        \n",
    "        search_path = str(self.flow_dir / '**' / '*_streamflow_qc.txt')\n",
    "        files = glob.glob(search_path, recursive=True)\n",
    "        \n",
    "        basins = []\n",
    "        for f in files:\n",
    "            parts = Path(f).parts\n",
    "            region = parts[-2]\n",
    "            gauge_id = parts[-1].split('_')[0]\n",
    "            \n",
    "            if gauge_id not in bad_basins:\n",
    "                basins.append({'gauge_id': gauge_id, 'region': region})\n",
    "        \n",
    "        return pd.DataFrame(basins)\n",
    "    \n",
    "    def load_dynamic_data(self, gauge_id: str, region: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads Streamflow + Forcing data for a single basin.\n",
    "        \n",
    "        Args:\n",
    "            gauge_id: 8-digit USGS gauge identifier\n",
    "            region: 2-digit HUC region code\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with DatetimeIndex and columns [Q_cms, PRCP, SRAD, Tmax, Tmin, Vp]\n",
    "            or None if loading fails\n",
    "        \"\"\"\n",
    "        # 1. Load Streamflow\n",
    "        flow_path = self.flow_dir / region / f'{gauge_id}_streamflow_qc.txt'\n",
    "        try:\n",
    "            df_flow = pd.read_csv(flow_path, delim_whitespace=True, header=None,\n",
    "                                  names=['gauge_id', 'Year', 'Month', 'Day', 'Q_cfs', 'QC'])\n",
    "        except Exception:\n",
    "            return None\n",
    "        \n",
    "        df_flow['Date'] = pd.to_datetime(df_flow[['Year', 'Month', 'Day']])\n",
    "        df_flow.set_index('Date', inplace=True)\n",
    "        # Convert CFS to CMS, handle missing values (-999)\n",
    "        df_flow['Q_cms'] = df_flow['Q_cfs'].replace(-999, np.nan) * CFS_TO_CMS\n",
    "        \n",
    "        # 2. Load Forcing (NLDAS)\n",
    "        forcing_path = self.forcing_dir / region / f'{gauge_id}_lump_nldas_forcing_leap.txt'\n",
    "        if not forcing_path.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df_force = pd.read_csv(forcing_path, delim_whitespace=True, skiprows=3)\n",
    "        except Exception:\n",
    "            return None\n",
    "        \n",
    "        # Normalize column names (handle various case patterns in NLDAS files)\n",
    "        col_map_lower = {\n",
    "            'mnth': 'Month', 'month': 'Month', 'mo': 'Month',\n",
    "            'year': 'Year', 'yr': 'Year',\n",
    "            'day': 'Day', 'dy': 'Day', 'hr': 'Hr',\n",
    "            'prcp(mm/day)': 'PRCP', 'srad(w/m2)': 'SRAD',\n",
    "            'tmax(c)': 'Tmax', 'tmin(c)': 'Tmin', 'vp(pa)': 'Vp',\n",
    "            'dayl(s)': 'Dayl', 'swe(mm)': 'SWE'\n",
    "        }\n",
    "        \n",
    "        new_cols = {}\n",
    "        for c in df_force.columns:\n",
    "            clean = c.strip().lower()\n",
    "            if clean in col_map_lower:\n",
    "                new_cols[c] = col_map_lower[clean]\n",
    "        df_force.rename(columns=new_cols, inplace=True)\n",
    "        \n",
    "        # Create Date Index\n",
    "        try:\n",
    "            df_force['Date'] = pd.to_datetime(df_force[['Year', 'Month', 'Day']])\n",
    "            df_force.set_index('Date', inplace=True)\n",
    "        except KeyError:\n",
    "            return None\n",
    "        \n",
    "        # 3. Merge (inner join for alignment)\n",
    "        cols_to_use = [c for c in FORCING_FEATURES if c in df_force.columns]\n",
    "        df_merged = df_flow[['Q_cms']].join(df_force[cols_to_use], how='inner')\n",
    "        \n",
    "        return df_merged\n",
    "    \n",
    "    def load_static_attributes(self, basins_list: Optional[List[str]] = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads all attribute files, merges them, and filters for requested features.\n",
    "        \n",
    "        Args:\n",
    "            basins_list: Optional list of gauge_ids to filter\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame indexed by gauge_id with static features\n",
    "        \"\"\"\n",
    "        files = ['camels_topo.txt', 'camels_soil.txt', 'camels_clim.txt',\n",
    "                 'camels_vege.txt', 'camels_geol.txt']\n",
    "        \n",
    "        dfs = []\n",
    "        for filename in files:\n",
    "            path = self.base_dir / filename\n",
    "            if path.exists():\n",
    "                try:\n",
    "                    df = pd.read_csv(path, sep=';')\n",
    "                    df.columns = [c.strip() for c in df.columns]\n",
    "                    if 'gauge_id' in df.columns:\n",
    "                        df['gauge_id'] = df['gauge_id'].astype(str).str.zfill(8)\n",
    "                        df.set_index('gauge_id', inplace=True)\n",
    "                        dfs.append(df)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "        if not dfs:\n",
    "            return None\n",
    "        \n",
    "        # Merge all static files\n",
    "        df_static = pd.concat(dfs, axis=1)\n",
    "        df_static = df_static.loc[:, ~df_static.columns.duplicated()]\n",
    "        \n",
    "        # Filter for configured features\n",
    "        available_feats = [f for f in STATIC_FEATURES if f in df_static.columns]\n",
    "        df_final = df_static[available_feats]\n",
    "        \n",
    "        if basins_list is not None:\n",
    "            df_final = df_final.reindex(basins_list)\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Engineers additional features from raw dynamic data.\n",
    "    \n",
    "    Features created:\n",
    "        - PRCP_roll3, PRCP_roll7: Rolling precipitation averages\n",
    "        - Q_lag1, Q_lag2, Q_lag3: Lagged streamflow values\n",
    "        - Time encodings: sin/cos of day-of-year, day-of-week\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_time_encoding: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_time_encoding: Whether to add cyclical time features\n",
    "        \"\"\"\n",
    "        self.use_time_encoding = use_time_encoding\n",
    "    \n",
    "    def add_time_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Adds cyclical time features using sin/cos transformations.\n",
    "        \n",
    "        This encodes temporal patterns that repeat periodically:\n",
    "        - Day of Year (365-day cycle): Captures seasonal patterns\n",
    "        - Day of Week (7-day cycle): Captures weekly patterns\n",
    "        \n",
    "        Using sin/cos ensures continuity (Dec 31 is close to Jan 1).\n",
    "        \"\"\"\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            return df\n",
    "        \n",
    "        # Day of Year encoding (seasonal patterns)\n",
    "        day_of_year = df.index.dayofyear\n",
    "        df['sin_doy'] = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "        df['cos_doy'] = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "        \n",
    "        # Day of Week encoding (weekly patterns)\n",
    "        day_of_week = df.index.dayofweek  # 0=Monday, 6=Sunday\n",
    "        df['sin_dow'] = np.sin(2 * np.pi * day_of_week / 7.0)\n",
    "        df['cos_dow'] = np.cos(2 * np.pi * day_of_week / 7.0)\n",
    "        \n",
    "        # Month encoding (for coarser seasonal signal)\n",
    "        month = df.index.month\n",
    "        df['sin_month'] = np.sin(2 * np.pi * month / 12.0)\n",
    "        df['cos_month'] = np.cos(2 * np.pi * month / 12.0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates rolling statistics for Precipitation.\n",
    "        Represents accumulated soil moisture / wetness.\n",
    "        \"\"\"\n",
    "        if 'PRCP' in df.columns:\n",
    "            # 3-Day Rolling Mean (Short-term wetness)\n",
    "            df['PRCP_roll3'] = df['PRCP'].rolling(window=3, min_periods=1).mean()\n",
    "            # 7-Day Rolling Mean (Medium-term saturation)\n",
    "            df['PRCP_roll7'] = df['PRCP'].rolling(window=7, min_periods=1).mean()\n",
    "        return df\n",
    "    \n",
    "    def add_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Adds explicit lag features for Streamflow.\n",
    "        Gives the model explicit access to past flow values.\n",
    "        \"\"\"\n",
    "        if TARGET in df.columns:\n",
    "            df['Q_lag1'] = df[TARGET].shift(1)  # Flow yesterday (t-1)\n",
    "            df['Q_lag2'] = df[TARGET].shift(2)  # Flow 2 days ago (t-2)\n",
    "            df['Q_lag3'] = df[TARGET].shift(3)  # Flow 3 days ago (t-3)\n",
    "        return df\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run all engineering steps.\"\"\"\n",
    "        if self.use_time_encoding:\n",
    "            df = self.add_time_encoding(df)\n",
    "        df = self.add_rolling_features(df)\n",
    "        df = self.add_lag_features(df)\n",
    "        return df\n",
    "\n",
    "\n",
    "class CamelsPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor for CAMELS data with normalization and sequence creation.\n",
    "    \n",
    "    Features:\n",
    "        - Physical constraint cleaning (negative values, outliers)\n",
    "        - Missing data handling (interpolation)\n",
    "        - Z-score normalization (global dynamic, per-basin target, static)\n",
    "        - VECTORIZED sequence creation using numpy stride_tricks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Physical constraints for data validation\n",
    "    PHYSICAL_LIMITS = {\n",
    "        'PRCP': {'min': 0.0, 'max': None},\n",
    "        'Q_cms': {'min': 0.0, 'max': None},\n",
    "        'Tmax': {'min': -60.0, 'max': 60.0},\n",
    "        'Tmin': {'min': -60.0, 'max': 60.0}\n",
    "    }\n",
    "    MAX_INTERPOLATE_GAP = 2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers: Dict[str, np.ndarray] = {}\n",
    "        self.basin_scalers: Dict[str, Dict[str, float]] = {}\n",
    "    \n",
    "    def add_date_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Encode day-of-year as cyclical sin/cos features (skips if already present).\"\"\"\n",
    "        # Skip if already added by FeatureEngineer.add_time_encoding()\n",
    "        if 'sin_doy' not in df.columns:\n",
    "            day_of_year = df.index.dayofyear\n",
    "            df['sin_doy'] = np.sin(2 * np.pi * day_of_year / 365.0)\n",
    "            df['cos_doy'] = np.cos(2 * np.pi * day_of_year / 365.0)\n",
    "        return df\n",
    "    \n",
    "    def clean_physical_outliers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ensure data respects physical constraints.\"\"\"\n",
    "        # 1. Negative Rain/Flow -> 0\n",
    "        for col in ['PRCP', TARGET]:\n",
    "            if col in df.columns:\n",
    "                mask = df[col] < 0\n",
    "                if mask.any():\n",
    "                    df.loc[mask, col] = 0.0\n",
    "        \n",
    "        # 2. Unrealistic Temp -> NaN\n",
    "        for col in ['Tmax', 'Tmin']:\n",
    "            if col in df.columns:\n",
    "                limits = self.PHYSICAL_LIMITS[col]\n",
    "                mask = (df[col] < limits['min']) | (df[col] > limits['max'])\n",
    "                if mask.any():\n",
    "                    df.loc[mask, col] = np.nan\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Use linear interpolation to fill short gaps.\"\"\"\n",
    "        cols_to_fix = [TARGET] + DYNAMIC_FEATURES\n",
    "        cols_to_fix = [c for c in cols_to_fix if c in df.columns]\n",
    "        \n",
    "        for col in cols_to_fix:\n",
    "            df[col] = df[col].interpolate(method='linear', limit=self.MAX_INTERPOLATE_GAP, limit_direction='forward')\n",
    "            df[col] = df[col].ffill().bfill()\n",
    "        return df\n",
    "    \n",
    "    def fit(self, dynamic_data_dict: Dict[str, pd.DataFrame], static_df: Optional[pd.DataFrame] = None) -> None:\n",
    "        \"\"\"\n",
    "        Compute normalization statistics from training data only.\n",
    "        \n",
    "        Args:\n",
    "            dynamic_data_dict: Dict mapping gauge_id -> DataFrame\n",
    "            static_df: Optional DataFrame of static features\n",
    "        \"\"\"\n",
    "        # 1. Dynamic feature stats (global across all basins)\n",
    "        dyn_vals = []\n",
    "        for gid, df in dynamic_data_dict.items():\n",
    "            train_slice = df.loc[TRAIN_START:TRAIN_END]\n",
    "            if not train_slice.empty:\n",
    "                valid_cols = [c for c in DYNAMIC_FEATURES if c in train_slice.columns]\n",
    "                valid_rows = train_slice[valid_cols].dropna()\n",
    "                if not valid_rows.empty:\n",
    "                    dyn_vals.append(valid_rows.values)\n",
    "        \n",
    "        if dyn_vals:\n",
    "            all_dyn = np.vstack(dyn_vals)\n",
    "            self.scalers['dynamic_mean'] = np.mean(all_dyn, axis=0)\n",
    "            self.scalers['dynamic_std'] = np.std(all_dyn, axis=0) + 1e-6\n",
    "        else:\n",
    "            self.scalers['dynamic_mean'] = np.zeros(len(DYNAMIC_FEATURES))\n",
    "            self.scalers['dynamic_std'] = np.ones(len(DYNAMIC_FEATURES))\n",
    "        \n",
    "        # 2. Static feature stats (log-transform area first)\n",
    "        if static_df is not None:\n",
    "            static_df = static_df.copy()\n",
    "            if 'area_gages2' in static_df.columns:\n",
    "                static_df['area_gages2'] = np.log10(np.maximum(static_df['area_gages2'], 1e-3))\n",
    "            self.scalers['static_mean'] = static_df.mean().values\n",
    "            self.scalers['static_std'] = static_df.std().values + 1e-6\n",
    "        \n",
    "        # 3. Per-basin target stats (for denormalization)\n",
    "        for gid, df in dynamic_data_dict.items():\n",
    "            train_slice = df.loc[TRAIN_START:TRAIN_END]\n",
    "            clean_target = train_slice[TARGET].dropna()\n",
    "            \n",
    "            if not clean_target.empty:\n",
    "                self.basin_scalers[gid] = {'mean': float(clean_target.mean()), 'std': float(clean_target.std() + 1e-6)}\n",
    "            else:\n",
    "                self.basin_scalers[gid] = {'mean': 0.0, 'std': 1.0}\n",
    "    \n",
    "    def transform(self, df_dynamic: pd.DataFrame, df_static: Optional[pd.DataFrame] = None, \n",
    "                  gauge_id: Optional[str] = None) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Normalize data and create feature matrix.\n",
    "        \n",
    "        Returns:\n",
    "            data_matrix: (Time, Features) array with [Dynamic_Norm, Date, Target_Norm]\n",
    "            static_norm: Normalized static vector or None\n",
    "        \"\"\"\n",
    "        # 1. Normalize dynamic features\n",
    "        dyn_cols = [c for c in DYNAMIC_FEATURES if c in df_dynamic.columns]\n",
    "        X_dyn = df_dynamic[dyn_cols].values\n",
    "        X_dyn_norm = (X_dyn - self.scalers['dynamic_mean']) / self.scalers['dynamic_std']\n",
    "        \n",
    "        # 2. Normalize target (per-basin)\n",
    "        target = df_dynamic[TARGET].values\n",
    "        b_stats = self.basin_scalers.get(gauge_id, {'mean': 0, 'std': 1})\n",
    "        y_norm = (target - b_stats['mean']) / b_stats['std']\n",
    "        \n",
    "        # 3. Normalize static (if provided)\n",
    "        X_stat_norm = None\n",
    "        if df_static is not None and gauge_id in df_static.index:\n",
    "            static_vals = df_static.loc[gauge_id].values.copy().astype(float)\n",
    "            if 'area_gages2' in df_static.columns:\n",
    "                area_idx = df_static.columns.get_loc('area_gages2')\n",
    "                static_vals[area_idx] = np.log10(np.maximum(static_vals[area_idx], 1e-3))\n",
    "            X_stat_norm = (static_vals - self.scalers['static_mean']) / self.scalers['static_std']\n",
    "        \n",
    "        # 4. Date features\n",
    "        date_feats = df_dynamic[['sin_doy', 'cos_doy']].values\n",
    "        \n",
    "        # Matrix: [Dynamic_Norm, Date, Target_Norm]\n",
    "        data_matrix = np.column_stack([X_dyn_norm, date_feats, y_norm])\n",
    "        \n",
    "        return data_matrix, X_stat_norm\n",
    "    \n",
    "    def create_sequences_vectorized(\n",
    "        self, \n",
    "        data_matrix: np.ndarray, \n",
    "        static_vec: Optional[np.ndarray] = None,\n",
    "        mode: str = 'task1',\n",
    "        seq_length: int = 90,\n",
    "        predict_horizon: int = 2,\n",
    "        predict_steps: int = 5\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        VECTORIZED sequence creation using numpy stride_tricks.\n",
    "        \n",
    "        ~10-100x faster than loop-based approach.\n",
    "        \n",
    "        Args:\n",
    "            data_matrix: (Time, Features) normalized data\n",
    "            static_vec: Optional static feature vector\n",
    "            mode: 'task1' (single-step) or 'task2' (multi-step)\n",
    "            seq_length: Lookback window in days\n",
    "            predict_horizon: For task1, predict t+horizon\n",
    "            predict_steps: For task2, predict t+1 to t+predict_steps\n",
    "        \n",
    "        Returns:\n",
    "            X: (N_samples, Seq_Len, Features) or with static features appended\n",
    "            y: (N_samples,) for task1 or (N_samples, Steps) for task2\n",
    "        \n",
    "        Shape documentation:\n",
    "            - data_matrix: (T, F) where T=timesteps, F=features\n",
    "            - X output: (N, seq_length, F + static_dim) where N=valid samples\n",
    "            - y output: (N,) for task1, (N, predict_steps) for task2\n",
    "        \"\"\"\n",
    "        T, n_features = data_matrix.shape\n",
    "        \n",
    "        if mode == 'task1':\n",
    "            horizon = predict_horizon\n",
    "            # Total valid sequences\n",
    "            n_sequences = T - seq_length - horizon + 1\n",
    "            \n",
    "            if n_sequences <= 0:\n",
    "                return np.array([]), np.array([])\n",
    "            \n",
    "            # Create sliding windows for X using stride_tricks\n",
    "            # Shape: (n_sequences, seq_length, n_features)\n",
    "            X_windows = sliding_window_view(data_matrix[:T - horizon], (seq_length, n_features))\n",
    "            X_windows = X_windows.squeeze(1)  # Remove singleton dimension\n",
    "            \n",
    "            # Target is horizon steps after end of each window\n",
    "            # y[i] = data_matrix[seq_length + horizon - 1 + i, -1]\n",
    "            y = data_matrix[seq_length + horizon - 1:, -1]\n",
    "            \n",
    "            # Ensure same length\n",
    "            min_len = min(len(X_windows), len(y))\n",
    "            X_windows = X_windows[:min_len]\n",
    "            y = y[:min_len]\n",
    "        \n",
    "        elif mode == 'task2':\n",
    "            steps = predict_steps\n",
    "            n_sequences = T - seq_length - steps + 1\n",
    "            \n",
    "            if n_sequences <= 0:\n",
    "                return np.array([]), np.array([])\n",
    "            \n",
    "            # Sliding windows for past sequence\n",
    "            X_windows = sliding_window_view(data_matrix[:T - steps], (seq_length, n_features))\n",
    "            X_windows = X_windows.squeeze(1)\n",
    "            \n",
    "            # Multi-step targets: (N, steps)\n",
    "            y_list = []\n",
    "            for i in range(n_sequences):\n",
    "                y_seq = data_matrix[seq_length + i : seq_length + i + steps, -1]\n",
    "                y_list.append(y_seq)\n",
    "            y = np.stack(y_list)\n",
    "            \n",
    "            # Ensure same length\n",
    "            min_len = min(len(X_windows), len(y))\n",
    "            X_windows = X_windows[:min_len]\n",
    "            y = y[:min_len]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "        \n",
    "        # Vectorized NaN check (remove any sequence with NaN in X or y)\n",
    "        if mode == 'task1':\n",
    "            valid_mask = ~(np.isnan(X_windows).any(axis=(1, 2)) | np.isnan(y))\n",
    "        else:\n",
    "            valid_mask = ~(np.isnan(X_windows).any(axis=(1, 2)) | np.isnan(y).any(axis=1))\n",
    "        \n",
    "        X_windows = X_windows[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        # Append static features (broadcast across sequence length)\n",
    "        if static_vec is not None:\n",
    "            n_samples = X_windows.shape[0]\n",
    "            # Expand static to (N, seq_length, static_dim)\n",
    "            static_expanded = np.broadcast_to(\n",
    "                static_vec.reshape(1, 1, -1),\n",
    "                (n_samples, seq_length, len(static_vec))\n",
    "            ).copy()\n",
    "            X_windows = np.concatenate([X_windows, static_expanded], axis=2)\n",
    "        \n",
    "        return X_windows, y\n",
    "\n",
    "\n",
    "print(\"✅ Section 3: Data Loading & Feature Engineering Classes loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcd3a3",
   "metadata": {},
   "source": [
    "## Section 4: Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec56d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: MODEL ARCHITECTURES\n",
    "# =============================================================================\n",
    "\n",
    "def init_weights(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initialize weights using Xavier/Kaiming initialization for faster convergence.\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.LSTM):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "    elif isinstance(module, nn.LSTMCell):\n",
    "        nn.init.xavier_uniform_(module.weight_ih)\n",
    "        nn.init.orthogonal_(module.weight_hh)\n",
    "        nn.init.zeros_(module.bias_ih)\n",
    "        nn.init.zeros_(module.bias_hh)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention layer for sequence modeling.\n",
    "    Computes attention weights over the sequence dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lstm_output: (Batch, Seq_Len, Hidden_Dim)\n",
    "        \n",
    "        Returns:\n",
    "            context: (Batch, Hidden_Dim) - Weighted sum of lstm outputs\n",
    "            weights: (Batch, Seq_Len) - Attention weights\n",
    "        \"\"\"\n",
    "        # Compute attention scores: (Batch, Seq_Len, 1)\n",
    "        scores = self.attention(lstm_output)\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)  # (Batch, Seq_Len)\n",
    "        \n",
    "        # Weighted sum: (Batch, Hidden_Dim)\n",
    "        context = torch.bmm(weights.unsqueeze(1), lstm_output).squeeze(1)\n",
    "        \n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block for feed-forward layers.\n",
    "    Implements: output = LayerNorm(x + FFN(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, dropout: float = 0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.norm(x + self.ffn(x))\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for single-step prediction with optional self-attention and residual learning.\n",
    "    \n",
    "    Architecture:\n",
    "        (Optional InputNorm) -> LSTM -> (Optional LayerNorm) -> \n",
    "        (Optional Self-Attention) -> (Optional Residual Blocks) ->\n",
    "        Dropout -> Linear -> Output\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features (dynamic + static if used)\n",
    "        hidden_dim: LSTM hidden units\n",
    "        num_layers: Number of stacked LSTM layers\n",
    "        dropout: Dropout probability\n",
    "        bidirectional: Whether to use bidirectional LSTM\n",
    "        use_attention: Whether to use self-attention layer\n",
    "        use_layer_norm: Whether to apply layer normalization after LSTM\n",
    "        use_input_norm: Whether to apply layer normalization to input\n",
    "        activation: Activation function for output head ('relu', 'gelu', 'tanh', 'none')\n",
    "        use_residual: Whether to use residual blocks after LSTM\n",
    "        num_residual_blocks: Number of residual blocks to stack\n",
    "    \n",
    "    Shape:\n",
    "        Input: (Batch, Seq_Len, input_dim)\n",
    "        Output: (Batch, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.2,\n",
    "        bidirectional: bool = False,\n",
    "        use_attention: bool = False,\n",
    "        use_layer_norm: bool = False,\n",
    "        use_input_norm: bool = False,\n",
    "        activation: str = 'none',\n",
    "        use_residual: bool = False,\n",
    "        num_residual_blocks: int = 2\n",
    "    ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_input_norm = use_input_norm\n",
    "        self.use_residual = use_residual\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # Optional input normalization\n",
    "        if use_input_norm:\n",
    "            self.input_norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Output dimension after LSTM\n",
    "        lstm_output_dim = hidden_dim * self.num_directions\n",
    "        \n",
    "        # Optional layer normalization\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(lstm_output_dim)\n",
    "        \n",
    "        # Optional self-attention\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttention(lstm_output_dim)\n",
    "        \n",
    "        # Optional residual blocks\n",
    "        if use_residual:\n",
    "            self.residual_blocks = nn.Sequential(\n",
    "                *[ResidualBlock(lstm_output_dim, dropout) for _ in range(num_residual_blocks)]\n",
    "            )\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU(0.1)\n",
    "        else:\n",
    "            self.activation = nn.Identity()\n",
    "        \n",
    "        # Dropout and output head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(lstm_output_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (Batch, Seq_Len, Features)\n",
    "        \n",
    "        Returns:\n",
    "            prediction: (Batch, 1)\n",
    "        \"\"\"\n",
    "        # Optional input normalization\n",
    "        if self.use_input_norm:\n",
    "            x = self.input_norm(x)\n",
    "        \n",
    "        # LSTM forward: out shape (Batch, Seq_Len, Hidden * num_directions)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Optional layer normalization\n",
    "        if self.use_layer_norm:\n",
    "            out = self.layer_norm(out)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # Use attention-weighted context\n",
    "            context, _ = self.attention(out)\n",
    "        else:\n",
    "            # Use last hidden state\n",
    "            context = out[:, -1, :]  # (Batch, Hidden * num_directions)\n",
    "        \n",
    "        # Optional residual blocks\n",
    "        if self.use_residual:\n",
    "            context = self.residual_blocks(context)\n",
    "        \n",
    "        # Activation, Dropout and predict\n",
    "        out = self.activation(context)\n",
    "        out = self.dropout(out)\n",
    "        prediction = self.head(out)  # (Batch, 1)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau-style cross-attention for Seq2Seq decoder.\n",
    "    \n",
    "    Computes attention weights between decoder hidden state and encoder outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden: torch.Tensor, encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden: Decoder hidden state (Batch, Hidden)\n",
    "            encoder_outputs: All encoder states (Batch, Seq_Len, Hidden)\n",
    "        \n",
    "        Returns:\n",
    "            context: (Batch, Hidden) - Context vector\n",
    "            weights: (Batch, Seq_Len) - Attention weights\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Expand decoder hidden state: (Batch, Seq_Len, Hidden)\n",
    "        hidden_expanded = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Compute energy: score = v * tanh(W * [h_dec; h_enc])\n",
    "        combined = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "        attention = self.v(energy).squeeze(2)  # (Batch, Seq_Len)\n",
    "        \n",
    "        # Softmax to get weights\n",
    "        weights = F.softmax(attention, dim=1)\n",
    "        \n",
    "        # Context vector: weighted sum of encoder outputs\n",
    "        context = torch.bmm(weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class LSTM_Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder LSTM with Cross-Attention for multi-step prediction.\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: LSTM over past sequence\n",
    "        Decoder: LSTMCell with cross-attention, autoregressive decoding\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input features for encoder (past dynamic + static)\n",
    "        hidden_dim: LSTM hidden units\n",
    "        future_forcing_dim: Number of known future features (weather forecasts)\n",
    "        static_dim: Number of static basin attributes\n",
    "        output_steps: Prediction horizon (default 5 for t+1 to t+5)\n",
    "        dropout: Dropout probability\n",
    "        encoder_layers: Number of encoder LSTM layers\n",
    "        use_layer_norm: Whether to apply layer normalization\n",
    "        use_input_norm: Whether to apply input normalization\n",
    "        residual_connection: Whether to use residual connections in decoder\n",
    "    \n",
    "    Shape:\n",
    "        x_past: (Batch, Seq_Len, input_dim)\n",
    "        x_future_forcing: (Batch, output_steps, future_forcing_dim)\n",
    "        static_features: (Batch, static_dim)\n",
    "        Output: (Batch, output_steps)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        future_forcing_dim: int,\n",
    "        static_dim: int,\n",
    "        output_steps: int = 5,\n",
    "        dropout: float = 0.2,\n",
    "        encoder_layers: int = 1,\n",
    "        use_layer_norm: bool = False,\n",
    "        use_input_norm: bool = False,\n",
    "        residual_connection: bool = False\n",
    "    ):\n",
    "        super(LSTM_Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.output_steps = output_steps\n",
    "        self.static_dim = static_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_input_norm = use_input_norm\n",
    "        self.residual_connection = residual_connection\n",
    "        self.encoder_layers = encoder_layers\n",
    "        \n",
    "        # Optional input normalization\n",
    "        if use_input_norm:\n",
    "            self.input_norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_dim, hidden_dim, \n",
    "            num_layers=encoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if encoder_layers > 1 else 0\n",
    "        )\n",
    "        self.encoder_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Optional layer norm for encoder output\n",
    "        if use_layer_norm:\n",
    "            self.encoder_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Decoder input: Previous_Flow (1) + Future_Forcing + Static\n",
    "        decoder_input_dim = 1 + future_forcing_dim + static_dim\n",
    "        \n",
    "        self.decoder_cell = nn.LSTMCell(decoder_input_dim, hidden_dim)\n",
    "        self.attention = CrossAttention(hidden_dim)\n",
    "        \n",
    "        # Optional layer norm for decoder\n",
    "        if use_layer_norm:\n",
    "            self.decoder_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Output projection: [Decoder_Hidden + Context] -> 1\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.output_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x_past: torch.Tensor,\n",
    "        x_future_forcing: torch.Tensor,\n",
    "        static_features: Optional[torch.Tensor] = None,\n",
    "        target_seq: Optional[torch.Tensor] = None,\n",
    "        teacher_forcing_ratio: float = 0.5\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with optional teacher forcing.\n",
    "        \n",
    "        Args:\n",
    "            x_past: Past sequence (Batch, Past_Seq, Input_Dim)\n",
    "            x_future_forcing: Future weather (Batch, output_steps, Forcing_Dim)\n",
    "            static_features: Static attributes (Batch, Static_Dim) or None\n",
    "            target_seq: Ground truth for teacher forcing (Batch, output_steps)\n",
    "            teacher_forcing_ratio: Probability of using ground truth input\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (Batch, output_steps) - Predicted flow sequence\n",
    "        \"\"\"\n",
    "        batch_size = x_past.size(0)\n",
    "        device = x_past.device\n",
    "        \n",
    "        # Optional input normalization\n",
    "        if self.use_input_norm:\n",
    "            x_past = self.input_norm(x_past)\n",
    "        \n",
    "        # 1. Encode past sequence\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(x_past)\n",
    "        encoder_outputs = self.encoder_dropout(encoder_outputs)\n",
    "        \n",
    "        # Optional layer normalization\n",
    "        if self.use_layer_norm:\n",
    "            encoder_outputs = self.encoder_layer_norm(encoder_outputs)\n",
    "        \n",
    "        # Remove layer dimension for LSTMCell (take last layer if multi-layer)\n",
    "        hidden = hidden[-1]  # (Batch, Hidden)\n",
    "        cell = cell[-1]      # (Batch, Hidden)\n",
    "        \n",
    "        # Initialize outputs\n",
    "        outputs = torch.zeros(batch_size, self.output_steps, device=device)\n",
    "        \n",
    "        # First decoder input: last observed flow (from x_past)\n",
    "        # Assuming flow is the LAST column in x_past\n",
    "        decoder_input_flow = x_past[:, -1, -1].unsqueeze(1)  # (Batch, 1)\n",
    "        \n",
    "        # 2. Decode step by step\n",
    "        for t in range(self.output_steps):\n",
    "            # Prepare decoder input: [Flow(t-1), Future_Forcing(t), Static]\n",
    "            current_forcing = x_future_forcing[:, t, :]\n",
    "            \n",
    "            inputs_list = [decoder_input_flow, current_forcing]\n",
    "            if self.static_dim > 0 and static_features is not None:\n",
    "                inputs_list.append(static_features)\n",
    "            \n",
    "            dec_input = torch.cat(inputs_list, dim=1)\n",
    "            \n",
    "            # Store previous hidden for residual\n",
    "            prev_hidden = hidden if self.residual_connection else None\n",
    "            \n",
    "            # Decoder cell step\n",
    "            hidden, cell = self.decoder_cell(dec_input, (hidden, cell))\n",
    "            \n",
    "            # Optional residual connection\n",
    "            if self.residual_connection and prev_hidden is not None:\n",
    "                hidden = hidden + prev_hidden\n",
    "            \n",
    "            # Optional layer normalization\n",
    "            if self.use_layer_norm:\n",
    "                hidden = self.decoder_layer_norm(hidden)\n",
    "            \n",
    "            # Cross-attention\n",
    "            context, _ = self.attention(hidden, encoder_outputs)\n",
    "            \n",
    "            # Predict\n",
    "            combined = torch.cat((hidden, context), dim=1)\n",
    "            combined = self.output_dropout(combined)\n",
    "            prediction = self.fc_out(combined)  # (Batch, 1)\n",
    "            \n",
    "            outputs[:, t] = prediction.squeeze(1)\n",
    "            \n",
    "            # Teacher forcing decision\n",
    "            if target_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                decoder_input_flow = target_seq[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input_flow = prediction\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping utility to save the best model and stop training when\n",
    "    validation loss stops improving.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.0, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait before stopping\n",
    "            min_delta: Minimum change to qualify as an improvement\n",
    "            verbose: Whether to print messages\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss: float, model: nn.Module, epoch: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if training should stop.\n",
    "        \n",
    "        Args:\n",
    "            val_loss: Current validation loss\n",
    "            model: Model to save if best\n",
    "            epoch: Current epoch number\n",
    "        \n",
    "        Returns:\n",
    "            True if training should stop\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, epoch)\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, epoch)\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "    def save_checkpoint(self, model: nn.Module, epoch: int) -> None:\n",
    "        \"\"\"Save model state dict.\"\"\"\n",
    "        self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        self.best_epoch = epoch\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss improved. Saving model (epoch {epoch})\")\n",
    "    \n",
    "    def load_best_model(self, model: nn.Module) -> None:\n",
    "        \"\"\"Load the best model state.\"\"\"\n",
    "        if self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n",
    "print(\"✅ Section 4: Model Architectures loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e6b77",
   "metadata": {},
   "source": [
    "## Section 5: Universal Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: UNIVERSAL TRAINING ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch_task1(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    scaler: Optional[GradScaler] = None,\n",
    "    grad_clip: Optional[float] = None,\n",
    "    accumulation_steps: int = 1\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train one epoch for Task 1 (single-step prediction).\n",
    "    \n",
    "    Supports:\n",
    "        - Mixed precision training (AMP)\n",
    "        - Gradient clipping\n",
    "        - Gradient accumulation\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_loader: Training data loader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        device: Training device\n",
    "        scaler: GradScaler for mixed precision (None to disable)\n",
    "        grad_clip: Max gradient norm (None to disable)\n",
    "        accumulation_steps: Number of steps for gradient accumulation\n",
    "    \n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass with optional mixed precision\n",
    "        if scaler is not None:\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch) / accumulation_steps\n",
    "            \n",
    "            # Backward pass with scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == n_batches:\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch) / accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == n_batches:\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "\n",
    "def validate_epoch_task1(\n",
    "    model: nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate one epoch for Task 1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation loss, NSE score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate NSE\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    nse = calc_nse(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, nse\n",
    "\n",
    "\n",
    "def train_epoch_task2(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    teacher_forcing_ratio: float = 0.5,\n",
    "    scaler: Optional[GradScaler] = None,\n",
    "    grad_clip: Optional[float] = None,\n",
    "    accumulation_steps: int = 1\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train one epoch for Task 2 (multi-step Seq2Seq prediction).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (x_past, x_future, static, y) in enumerate(train_loader):\n",
    "        x_past = x_past.to(device)\n",
    "        x_future = x_future.to(device)\n",
    "        static = static.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(x_past, x_future, static, target_seq=y, \n",
    "                               teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "                loss = criterion(outputs, y) / accumulation_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == n_batches:\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            outputs = model(x_past, x_future, static, target_seq=y,\n",
    "                           teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            loss = criterion(outputs, y) / accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == n_batches:\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "\n",
    "def validate_epoch_task2(\n",
    "    model: nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    output_steps: int = 5\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Validate one epoch for Task 2.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation loss, dict with per-step and average NSE)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_past, x_future, static, y in val_loader:\n",
    "            x_past = x_past.to(device)\n",
    "            x_future = x_future.to(device)\n",
    "            static = static.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = model(x_past, x_future, static, target_seq=None, teacher_forcing_ratio=0.0)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate per-step NSE\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    nse_dict = {}\n",
    "    for step in range(output_steps):\n",
    "        step_nse = calc_nse(all_targets[:, step], all_preds[:, step])\n",
    "        nse_dict[f'NSE_step{step+1}'] = step_nse\n",
    "    \n",
    "    # Average NSE\n",
    "    nse_dict['NSE_avg'] = np.mean([nse_dict[f'NSE_step{i+1}'] for i in range(output_steps)])\n",
    "    \n",
    "    return avg_loss, nse_dict\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: Dict,\n",
    "    device: torch.device,\n",
    "    task: str = 'task1'\n",
    ") -> Tuple[nn.Module, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Universal training function for both Task 1 and Task 2.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        config: Training configuration dictionary containing:\n",
    "            - epochs: Number of training epochs\n",
    "            - lr: Learning rate\n",
    "            - lr_min: Minimum learning rate (for schedulers)\n",
    "            - weight_decay: L2 regularization\n",
    "            - patience: Early stopping patience\n",
    "            - use_amp: Whether to use mixed precision\n",
    "            - grad_clip: Gradient clipping value (None to disable)\n",
    "            - optimizer_type: 'adam', 'adamw', 'sgd', 'rmsprop'\n",
    "            - scheduler_type: 'plateau', 'cosine', 'step', 'onecycle', 'exponential', 'none'\n",
    "            - scheduler_patience: Patience for ReduceLROnPlateau\n",
    "            - scheduler_factor: Factor for ReduceLROnPlateau/StepLR\n",
    "            - scheduler_step_size: Step size for StepLR\n",
    "            - teacher_forcing_ratio: (Task 2 only) TF ratio\n",
    "            - tf_decay: (Task 2 only) TF decay per epoch\n",
    "        device: Training device\n",
    "        task: 'task1' or 'task2'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained model, training history, final metrics)\n",
    "    \"\"\"\n",
    "    # Extract config\n",
    "    epochs = config.get('epochs', 100)\n",
    "    lr = config.get('lr', 0.001)\n",
    "    lr_min = config.get('lr_min', 1e-6)\n",
    "    weight_decay = config.get('weight_decay', 0.0)\n",
    "    patience = config.get('patience', 15)\n",
    "    use_amp = config.get('use_amp', False) and torch.cuda.is_available()\n",
    "    grad_clip = config.get('grad_clip', None)\n",
    "    accumulation_steps = config.get('accumulation_steps', 1)\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optimizer_type = config.get('optimizer_type', 'adam').lower()\n",
    "    \n",
    "    # Scheduler settings\n",
    "    scheduler_type = config.get('scheduler_type', 'plateau').lower()\n",
    "    scheduler_patience = config.get('scheduler_patience', 5)\n",
    "    scheduler_factor = config.get('scheduler_factor', 0.5)\n",
    "    scheduler_step_size = config.get('scheduler_step_size', 10)\n",
    "    \n",
    "    # Task 2 specific\n",
    "    tf_ratio = config.get('teacher_forcing_ratio', 0.5)\n",
    "    tf_decay = config.get('tf_decay', 1.0)  # 1.0 = no decay\n",
    "    output_steps = config.get('predict_steps', 5)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # OPTIMIZER SETUP\n",
    "    # =========================================================================\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        momentum = config.get('momentum', 0.9)\n",
    "        nesterov = config.get('nesterov', True)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, \n",
    "                              momentum=momentum, nesterov=nesterov)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer type: {optimizer_type}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOSS FUNCTION SETUP\n",
    "    # =========================================================================\n",
    "    loss_type = config.get('loss_type', 'mse').lower()\n",
    "    huber_delta = config.get('huber_delta', 1.0)\n",
    "    \n",
    "    if loss_type == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_type == 'huber':\n",
    "        criterion = nn.HuberLoss(delta=huber_delta)\n",
    "    elif loss_type == 'smoothl1':\n",
    "        criterion = nn.SmoothL1Loss(beta=huber_delta)\n",
    "    elif loss_type == 'mae' or loss_type == 'l1':\n",
    "        criterion = nn.L1Loss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss type: {loss_type}. Choose from 'mse', 'huber', 'smoothl1', 'mae'\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SCHEDULER SETUP\n",
    "    # =========================================================================\n",
    "    scheduler = None\n",
    "    if scheduler_type == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=scheduler_factor, \n",
    "            patience=scheduler_patience, min_lr=lr_min\n",
    "        )\n",
    "    elif scheduler_type == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epochs, eta_min=lr_min\n",
    "        )\n",
    "    elif scheduler_type == 'cosine_warm':\n",
    "        # Cosine with warm restarts\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=config.get('T_0', 10), T_mult=config.get('T_mult', 2), eta_min=lr_min\n",
    "        )\n",
    "    elif scheduler_type == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=scheduler_step_size, gamma=scheduler_factor\n",
    "        )\n",
    "    elif scheduler_type == 'multistep':\n",
    "        milestones = config.get('milestones', [30, 60, 90])\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=milestones, gamma=scheduler_factor\n",
    "        )\n",
    "    elif scheduler_type == 'exponential':\n",
    "        gamma = config.get('exp_gamma', 0.95)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    elif scheduler_type == 'onecycle':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=lr, epochs=epochs,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=config.get('pct_start', 0.3),\n",
    "            anneal_strategy=config.get('anneal_strategy', 'cos')\n",
    "        )\n",
    "    elif scheduler_type == 'none':\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")\n",
    "    \n",
    "    scaler = GradScaler(device='cuda') if use_amp else None\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_nse': [],\n",
    "        'learning_rates': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {task.upper()} | Epochs: {epochs} | LR: {lr}\")\n",
    "    print(f\"Optimizer: {optimizer_type} | Scheduler: {scheduler_type}\")\n",
    "    print(f\"AMP: {use_amp} | Grad Clip: {grad_clip}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = datetime.now()\n",
    "        current_tf = tf_ratio * (tf_decay ** epoch) if task == 'task2' else None\n",
    "        \n",
    "        # Training\n",
    "        if task == 'task1':\n",
    "            train_loss = train_epoch_task1(\n",
    "                model, train_loader, optimizer, criterion, device,\n",
    "                scaler=scaler, grad_clip=grad_clip, accumulation_steps=accumulation_steps\n",
    "            )\n",
    "            val_loss, val_nse = validate_epoch_task1(model, val_loader, criterion, device)\n",
    "            nse_display = f\"NSE: {val_nse:.4f}\"\n",
    "        else:\n",
    "            train_loss = train_epoch_task2(\n",
    "                model, train_loader, optimizer, criterion, device,\n",
    "                teacher_forcing_ratio=current_tf,\n",
    "                scaler=scaler, grad_clip=grad_clip, accumulation_steps=accumulation_steps\n",
    "            )\n",
    "            val_loss, nse_dict = validate_epoch_task2(model, val_loader, criterion, device, output_steps)\n",
    "            val_nse = nse_dict['NSE_avg']\n",
    "            nse_display = f\"NSE_avg: {val_nse:.4f}\"\n",
    "        \n",
    "        epoch_time = (datetime.now() - epoch_start).total_seconds()\n",
    "        \n",
    "        # Update scheduler (handle different scheduler types)\n",
    "        if scheduler is not None:\n",
    "            if scheduler_type == 'plateau':\n",
    "                scheduler.step(val_loss)\n",
    "            elif scheduler_type == 'onecycle':\n",
    "                pass  # OneCycleLR steps internally per batch\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Log history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_nse'].append(val_nse)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print progress\n",
    "        tf_str = f\" | TF: {current_tf:.2f}\" if task == 'task2' else \"\"\n",
    "        print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "              f\"Train: {train_loss:.4f} | Val: {val_loss:.4f} | {nse_display} | \"\n",
    "              f\"LR: {current_lr:.2e} | Time: {epoch_time:.1f}s{tf_str}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(val_loss, model, epoch + 1):\n",
    "            print(f\"\\n Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    early_stopping.load_best_model(model)\n",
    "    history['best_epoch'] = early_stopping.best_epoch\n",
    "    history['total_epochs'] = epoch + 1\n",
    "    \n",
    "    print(f\"\\n Training complete. Best epoch: {early_stopping.best_epoch}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    task: str = 'task1',\n",
    "    output_steps: int = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (predictions, targets, metrics dict)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if task == 'task1':\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "                all_targets.append(y_batch.numpy())\n",
    "        else:\n",
    "            for x_past, x_future, static, y in test_loader:\n",
    "                x_past = x_past.to(device)\n",
    "                x_future = x_future.to(device)\n",
    "                static = static.to(device)\n",
    "                \n",
    "                outputs = model(x_past, x_future, static, target_seq=None, teacher_forcing_ratio=0.0)\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "                all_targets.append(y.numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    if task == 'task1':\n",
    "        metrics = calc_metrics(all_targets, all_preds)\n",
    "    else:\n",
    "        metrics = {'per_step': {}}\n",
    "        for step in range(output_steps):\n",
    "            step_metrics = calc_metrics(all_targets[:, step], all_preds[:, step])\n",
    "            metrics['per_step'][f'step_{step+1}'] = step_metrics\n",
    "        \n",
    "        # Average metrics\n",
    "        avg_preds = all_preds.flatten()\n",
    "        avg_targets = all_targets.flatten()\n",
    "        metrics['average'] = calc_metrics(avg_targets, avg_preds)\n",
    "    \n",
    "    return all_preds, all_targets, metrics\n",
    "\n",
    "\n",
    "print(\"Section 5: Universal Training Engine loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af37779",
   "metadata": {},
   "source": [
    "## Section 6: Data Preparation Pipeline (Run ONCE)\n",
    "\n",
    "This section loads, preprocesses, and caches all data. **Run this once** - the cached data will be reused for all training experiments.\n",
    "\n",
    "**Cached items:**\n",
    "- Raw parsed data (skips CSV parsing on subsequent runs)\n",
    "- Fitted preprocessor scalers\n",
    "- Train/Val/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fe614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: DATA PREPARATION PIPELINE\n",
    "# =============================================================================\n",
    "# This cell prepares ALL data once. Results are cached for fast re-runs.\n",
    "# Change NUM_BASINS or USE_STATIC to invalidate cache and reload.\n",
    "\n",
    "# ========================\n",
    "# DATA PIPELINE CONFIG\n",
    "# ========================\n",
    "NUM_BASINS = 50        # Number of basins to load (0 = all basins, ~671)\n",
    "USE_STATIC = False     # Include static catchment attributes\n",
    "\n",
    "# ========================\n",
    "# PIPELINE FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "def load_raw_data(num_basins: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Load and preprocess raw basin data.\n",
    "    \n",
    "    Returns dict with keys:\n",
    "        - train_data: Dict[gauge_id, DataFrame]\n",
    "        - val_data: Dict[gauge_id, DataFrame]  \n",
    "        - test_data: Dict[gauge_id, DataFrame]\n",
    "        - basin_ids: List of gauge IDs\n",
    "    \"\"\"\n",
    "    loader = CamelsLoader()\n",
    "    engineer = FeatureEngineer(use_time_encoding=True)\n",
    "    preprocessor = CamelsPreprocessor()\n",
    "    \n",
    "    df_basins = loader.get_basin_list()\n",
    "    if df_basins.empty:\n",
    "        raise ValueError(\"No basins found in the dataset directory!\")\n",
    "    \n",
    "    print(f\"Found {len(df_basins)} total basins\")\n",
    "    \n",
    "    if num_basins > 0:\n",
    "        df_basins = df_basins.sample(n=num_basins, random_state=42)\n",
    "        print(f\"Using {num_basins} basins sampled randomly\")\n",
    "    \n",
    "    basin_ids = df_basins['gauge_id'].tolist()\n",
    "    \n",
    "    # Load data for each basin\n",
    "    train_data, val_data, test_data = {}, {}, {}\n",
    "    \n",
    "    print(\"Loading and preprocessing basins...\")\n",
    "    for _, row in tqdm(df_basins.iterrows(), total=len(df_basins), desc=\"Loading\"):\n",
    "        gid = row['gauge_id']\n",
    "        region = row['region']\n",
    "        \n",
    "        # 1. Load raw dynamic data\n",
    "        df = loader.load_dynamic_data(gid, region)\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        # 2. Clean physical outliers\n",
    "        df = preprocessor.clean_physical_outliers(df)\n",
    "        \n",
    "        # 3. Feature engineering (rolling averages, lags, time encoding if enabled)\n",
    "        df = engineer.transform(df)\n",
    "        \n",
    "        # 4. Add cyclical date features (additional sin/cos for DOY)\n",
    "        df = preprocessor.add_date_features(df)\n",
    "        \n",
    "        # 5. Split by date (strict chronological)\n",
    "        df_train = df.loc[TRAIN_START:TRAIN_END].copy()\n",
    "        df_val = df.loc[VAL_START:VAL_END].copy()\n",
    "        df_test = df.loc[TEST_START:TEST_END].copy()\n",
    "        \n",
    "        # 6. Handle missing data per split\n",
    "        if not df_train.empty:\n",
    "            df_train = preprocessor.handle_missing_data(df_train)\n",
    "            train_data[gid] = df_train\n",
    "        if not df_val.empty:\n",
    "            df_val = preprocessor.handle_missing_data(df_val)\n",
    "            val_data[gid] = df_val\n",
    "        if not df_test.empty:\n",
    "            df_test = preprocessor.handle_missing_data(df_test)\n",
    "            test_data[gid] = df_test\n",
    "    \n",
    "    print(f\"Loaded {len(train_data)} basins with training data\")\n",
    "    \n",
    "    return {\n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data,\n",
    "        'test_data': test_data,\n",
    "        'basin_ids': basin_ids\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_preprocessor(raw_data: Dict, static_df: Optional[pd.DataFrame]) -> CamelsPreprocessor:\n",
    "    \"\"\"\n",
    "    Fit preprocessor on training data only (no data leakage).\n",
    "    \"\"\"\n",
    "    preprocessor = CamelsPreprocessor()\n",
    "    print(\"Fitting preprocessor on training data...\")\n",
    "    preprocessor.fit(raw_data['train_data'], static_df)\n",
    "    print(f\"   - Dynamic features: {len(preprocessor.scalers.get('dynamic_mean', []))} dims\")\n",
    "    print(f\"   - Static features: {len(preprocessor.scalers.get('static_mean', []))} dims\" if static_df is not None else \"   - Static features: disabled\")\n",
    "    print(f\"   - Per-basin target scalers: {len(preprocessor.basin_scalers)} basins\")\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# ========================\n",
    "# RUN DATA PIPELINE\n",
    "# ========================\n",
    "\n",
    "# Configuration for cache invalidation\n",
    "data_config = {\n",
    "    'num_basins': NUM_BASINS,\n",
    "    'use_static': USE_STATIC,\n",
    "    'train_start': TRAIN_START,\n",
    "    'train_end': TRAIN_END,\n",
    "    'val_start': VAL_START,\n",
    "    'val_end': VAL_END,\n",
    "    'test_start': TEST_START,\n",
    "    'test_end': TEST_END,\n",
    "    'dynamic_features': DYNAMIC_FEATURES,\n",
    "    'static_features': STATIC_FEATURES if USE_STATIC else []\n",
    "}\n",
    "\n",
    "# Load raw data (cached)\n",
    "raw_data = get_or_create_cache(\n",
    "    cache_name='raw_data',\n",
    "    create_fn=load_raw_data,\n",
    "    config_dict=data_config,\n",
    "    num_basins=NUM_BASINS\n",
    ")\n",
    "\n",
    "# Unpack\n",
    "train_data = raw_data['train_data']\n",
    "val_data = raw_data['val_data']\n",
    "test_data = raw_data['test_data']\n",
    "basin_ids = raw_data['basin_ids']\n",
    "\n",
    "# Load static attributes\n",
    "loader = CamelsLoader()\n",
    "static_df = loader.load_static_attributes(basin_ids) if USE_STATIC else None\n",
    "\n",
    "if static_df is not None:\n",
    "    print(f\"Static features loaded: {static_df.shape}\")\n",
    "\n",
    "# Fit preprocessor (cached)\n",
    "preprocessor = get_or_create_cache(\n",
    "    cache_name='preprocessor',\n",
    "    create_fn=prepare_preprocessor,\n",
    "    config_dict=data_config,\n",
    "    raw_data=raw_data,\n",
    "    static_df=static_df\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# DATA SUMMARY\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Basins loaded: {len(train_data)}\")\n",
    "print(f\"  Training period: {TRAIN_START} to {TRAIN_END}\")\n",
    "print(f\"  Validation period: {VAL_START} to {VAL_END}\")\n",
    "print(f\"  Test period: {TEST_START} to {TEST_END}\")\n",
    "print(f\"  Dynamic features: {len(DYNAMIC_FEATURES)}\")\n",
    "print(f\"  Static features: {len(STATIC_FEATURES) if USE_STATIC else 'disabled'}\")\n",
    "\n",
    "# Sample data shape (with error handling)\n",
    "if len(train_data) > 0:\n",
    "    sample_gid = list(train_data.keys())[0]\n",
    "    sample_df = train_data[sample_gid]\n",
    "    print(f\"\\n  Sample basin ({sample_gid}):\")\n",
    "    print(f\"    Training samples: {len(sample_df)}\")\n",
    "    print(f\"    Columns: {list(sample_df.columns)}\")\n",
    "else:\n",
    "    print(\"\\n  No basins loaded! Check data paths and file formats.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(train_data) == 0:\n",
    "    raise ValueError(\"No training data loaded. Cannot proceed with training.\")\n",
    "\n",
    "print(\"\\n Data pipeline complete. Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42c9c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Task 1 - Single-Step Prediction (t+2)\n",
    "\n",
    "**Goal:** Predict streamflow at time t+2 given the past 30 days of data.\n",
    "\n",
    "**Architecture:** LSTM with optional self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905aedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7A: TASK 1 HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "# Modify these parameters and re-run this cell + training cells to experiment\n",
    "\n",
    "TASK1_CONFIG = {\n",
    "    # =========================================================================\n",
    "    # SEQUENCE PARAMETERS\n",
    "    # =========================================================================\n",
    "    'seq_length': 30,             # Lookback window (days) - how many past days to consider\n",
    "    'predict_step': 2,            # Predict t + predict_step (e.g., 2 = predict 2 days ahead)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FEATURE ENGINEERING\n",
    "    # =========================================================================\n",
    "    'use_time_encoding': True,    # Add sin/cos time features (day-of-year, day-of-week, month)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MODEL ARCHITECTURE\n",
    "    # =========================================================================\n",
    "    'hidden_dim': 8,             # LSTM hidden units (32, 64, 128, 256)\n",
    "    'num_layers': 12,              # Number of stacked LSTM layers (1-4)\n",
    "    'bidirectional': False,        # Use bidirectional LSTM (doubles hidden dim output)\n",
    "    'use_attention': True,        # Use self-attention layer for better context\n",
    "    'dropout': 0.2,               # Dropout probability (0.0 - 0.5)\n",
    "    \n",
    "    # Layer normalization options\n",
    "    'use_layer_norm': True,       # Layer normalization after LSTM\n",
    "    'use_input_norm': False,      # Layer normalization on input features\n",
    "    'activation': 'leaky_relu',         # Output activation: 'none', 'relu', 'gelu', 'tanh', 'leaky_relu'\n",
    "    \n",
    "    # Residual learning options\n",
    "    'use_residual': True,         # Use residual blocks after LSTM (helps gradient flow)\n",
    "    'num_residual_blocks': 4,     # Number of residual blocks to stack (1-4)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOSS FUNCTION\n",
    "    # =========================================================================\n",
    "    # Options: 'mse', 'huber', 'smoothl1', 'mae'\n",
    "    # Huber/SmoothL1 are more robust to outliers than MSE\n",
    "    'loss_type': 'huber',         # Loss function type\n",
    "    'huber_delta': 1.0,           # Delta parameter for Huber loss (threshold for quadratic vs linear)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # OPTIMIZER SETTINGS\n",
    "    # =========================================================================\n",
    "    'optimizer_type': 'adamw',    # Options: 'adam', 'adamw', 'sgd', 'rmsprop'\n",
    "    'lr': 0.001,                  # Initial learning rate\n",
    "    'lr_min': 1e-6,               # Minimum learning rate (for schedulers)\n",
    "    'weight_decay': 0.01,         # L2 regularization (AdamW recommended: 0.01-0.1)\n",
    "    'momentum': 0.9,              # Momentum (only for SGD)\n",
    "    'nesterov': True,             # Nesterov momentum (only for SGD)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LEARNING RATE SCHEDULER\n",
    "    # =========================================================================\n",
    "    # Options: 'plateau', 'cosine', 'cosine_warm', 'step', 'multistep', 'exponential', 'onecycle', 'none'\n",
    "    'scheduler_type': 'cosine_warm',   \n",
    "    \n",
    "    # ReduceLROnPlateau settings (scheduler_type='plateau')\n",
    "    'scheduler_patience': 5,      # Epochs to wait before reducing LR\n",
    "    'scheduler_factor': 0.5,      # Factor to reduce LR by\n",
    "    \n",
    "    # StepLR settings (scheduler_type='step')\n",
    "    'scheduler_step_size': 20,    # Reduce LR every N epochs\n",
    "    \n",
    "    # MultiStepLR settings (scheduler_type='multistep')\n",
    "    'milestones': [30, 60, 90],   # Epochs at which to reduce LR\n",
    "    \n",
    "    # CosineAnnealingWarmRestarts settings (scheduler_type='cosine_warm')\n",
    "    'T_0': 10,                    # Number of epochs for first restart\n",
    "    'T_mult': 2,                  # Factor to increase T_0 after each restart\n",
    "    \n",
    "    # ExponentialLR settings (scheduler_type='exponential')\n",
    "    'exp_gamma': 0.95,            # Decay factor per epoch\n",
    "    \n",
    "    # OneCycleLR settings (scheduler_type='onecycle')\n",
    "    'pct_start': 0.3,             # Fraction of cycle spent increasing LR\n",
    "    'anneal_strategy': 'cos',     # 'cos' or 'linear'\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TRAINING PARAMETERS\n",
    "    # =========================================================================\n",
    "    'epochs': 100,                # Maximum training epochs\n",
    "    'batch_size': 512,            # Batch size (256, 512, 1024)\n",
    "    'patience': 20,               # Early stopping patience\n",
    "    \n",
    "    # =========================================================================\n",
    "    # OPTIMIZATION TECHNIQUES\n",
    "    # =========================================================================\n",
    "    'use_amp': True,              # Mixed precision training (GPU only, faster)\n",
    "    'grad_clip': 1.0,             # Gradient clipping (None to disable)\n",
    "    'accumulation_steps': 1,      # Gradient accumulation (effective batch = batch_size * steps)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ADVANCED\n",
    "    # =========================================================================\n",
    "    'compile_model': False,       # Use torch.compile() for PyTorch 2.0+\n",
    "}\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"Task 1 Configuration loaded\")\n",
    "print(f\"\\n Model Architecture:\")\n",
    "print(f\"   LSTM: hidden={TASK1_CONFIG['hidden_dim']}, layers={TASK1_CONFIG['num_layers']}, \"\n",
    "      f\"bidirectional={TASK1_CONFIG['bidirectional']}\")\n",
    "print(f\"   Attention: {TASK1_CONFIG['use_attention']}, LayerNorm: {TASK1_CONFIG['use_layer_norm']}\")\n",
    "print(f\"   Residual: {TASK1_CONFIG['use_residual']} ({TASK1_CONFIG['num_residual_blocks']} blocks)\")\n",
    "print(f\"   Dropout: {TASK1_CONFIG['dropout']}, Activation: {TASK1_CONFIG['activation']}\")\n",
    "\n",
    "print(f\"\\n🎯 Features:\")\n",
    "print(f\"   Time Encoding: {TASK1_CONFIG['use_time_encoding']} (sin/cos day-of-year, day-of-week, month)\")\n",
    "\n",
    "print(f\"\\n📉 Loss Function: {TASK1_CONFIG['loss_type'].upper()}\")\n",
    "if TASK1_CONFIG['loss_type'] == 'huber':\n",
    "    print(f\"   Delta: {TASK1_CONFIG['huber_delta']}\")\n",
    "\n",
    "print(f\"\\n⚙️ Optimizer: {TASK1_CONFIG['optimizer_type'].upper()}\")\n",
    "print(f\"   LR: {TASK1_CONFIG['lr']} → {TASK1_CONFIG['lr_min']} (min)\")\n",
    "print(f\"   Weight Decay: {TASK1_CONFIG['weight_decay']}\")\n",
    "\n",
    "print(f\"\\n📈 Scheduler: {TASK1_CONFIG['scheduler_type'].upper()}\")\n",
    "if TASK1_CONFIG['scheduler_type'] == 'plateau':\n",
    "    print(f\"   Patience: {TASK1_CONFIG['scheduler_patience']}, Factor: {TASK1_CONFIG['scheduler_factor']}\")\n",
    "elif TASK1_CONFIG['scheduler_type'] == 'cosine':\n",
    "    print(f\"   T_max: {TASK1_CONFIG['epochs']}, eta_min: {TASK1_CONFIG['lr_min']}\")\n",
    "elif TASK1_CONFIG['scheduler_type'] == 'step':\n",
    "    print(f\"   Step Size: {TASK1_CONFIG['scheduler_step_size']}, Gamma: {TASK1_CONFIG['scheduler_factor']}\")\n",
    "\n",
    "print(f\"\\n🏋️ Training: epochs={TASK1_CONFIG['epochs']}, batch={TASK1_CONFIG['batch_size']}, \"\n",
    "      f\"patience={TASK1_CONFIG['patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37afb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7B: TASK 1 DATA PREPARATION & TRAINING\n",
    "# =============================================================================\n",
    "import gc\n",
    "# Release all memory before training\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Preparing Task 1 sequences...\")\n",
    "task1_start_time = datetime.now()\n",
    "\n",
    "# Generate sequences for each split using vectorized method\n",
    "def get_task1_sequences(data_dict: Dict, df_static: Optional[pd.DataFrame], \n",
    "                        preprocessor: CamelsPreprocessor, basin_ids: List[str],\n",
    "                        seq_length: int, predict_step: int,\n",
    "                        use_static: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate Task 1 sequences for all basins.\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    for gid in basin_ids:\n",
    "        if gid not in data_dict:\n",
    "            continue\n",
    "        df = data_dict[gid]\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Transform to normalized matrix\n",
    "        data_matrix, static_vec = preprocessor.transform(df, df_static, gid)\n",
    "        \n",
    "        # Create sequences (vectorized!)\n",
    "        X, y = preprocessor.create_sequences_vectorized(\n",
    "            data_matrix,\n",
    "            static_vec=static_vec if use_static else None,\n",
    "            mode='task1',\n",
    "            seq_length=seq_length,\n",
    "            predict_horizon=predict_step\n",
    "        )\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "    \n",
    "    if not X_list:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "# Generate sequences\n",
    "X_train_t1, y_train_t1 = get_task1_sequences(\n",
    "    train_data, static_df, preprocessor, basin_ids,\n",
    "    TASK1_CONFIG['seq_length'], TASK1_CONFIG['predict_step'], USE_STATIC\n",
    ")\n",
    "X_val_t1, y_val_t1 = get_task1_sequences(\n",
    "    val_data, static_df, preprocessor, basin_ids,\n",
    "    TASK1_CONFIG['seq_length'], TASK1_CONFIG['predict_step'], USE_STATIC\n",
    ")\n",
    "X_test_t1, y_test_t1 = get_task1_sequences(\n",
    "    test_data, static_df, preprocessor, basin_ids,\n",
    "    TASK1_CONFIG['seq_length'], TASK1_CONFIG['predict_step'], USE_STATIC\n",
    ")\n",
    "\n",
    "print(f\"   Train: X={X_train_t1.shape}, y={y_train_t1.shape}\")\n",
    "print(f\"   Val:   X={X_val_t1.shape}, y={y_val_t1.shape}\")\n",
    "print(f\"   Test:  X={X_test_t1.shape}, y={y_test_t1.shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_t1 = create_dataloader(X_train_t1, y_train_t1, TASK1_CONFIG['batch_size'], shuffle=True)\n",
    "val_loader_t1 = create_dataloader(X_val_t1, y_val_t1, TASK1_CONFIG['batch_size'], shuffle=False)\n",
    "test_loader_t1 = create_dataloader(X_test_t1, y_test_t1, TASK1_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Initialize model with all configurable parameters\n",
    "input_dim_t1 = X_train_t1.shape[2]\n",
    "model_t1 = LSTMModel(\n",
    "    input_dim=input_dim_t1,\n",
    "    hidden_dim=TASK1_CONFIG['hidden_dim'],\n",
    "    num_layers=TASK1_CONFIG['num_layers'],\n",
    "    dropout=TASK1_CONFIG['dropout'],\n",
    "    bidirectional=TASK1_CONFIG['bidirectional'],\n",
    "    use_attention=TASK1_CONFIG['use_attention'],\n",
    "    use_layer_norm=TASK1_CONFIG.get('use_layer_norm', False),\n",
    "    use_input_norm=TASK1_CONFIG.get('use_input_norm', False),\n",
    "    activation=TASK1_CONFIG.get('activation', 'none'),\n",
    "    use_residual=TASK1_CONFIG.get('use_residual', False),\n",
    "    num_residual_blocks=TASK1_CONFIG.get('num_residual_blocks', 2)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optional: compile model for PyTorch 2.0+\n",
    "if TASK1_CONFIG['compile_model']:\n",
    "    model_t1 = maybe_compile_model(model_t1)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model_t1.parameters() if p.requires_grad)\n",
    "print(f\"\\n Model Architecture:\")\n",
    "print(f\"   Input dim: {input_dim_t1}\")\n",
    "print(f\"   Trainable parameters: {n_params:,}\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n Starting Task 1 Training...\")\n",
    "model_t1, history_t1 = train_model(\n",
    "    model=model_t1,\n",
    "    train_loader=train_loader_t1,\n",
    "    val_loader=val_loader_t1,\n",
    "    config=TASK1_CONFIG,\n",
    "    device=DEVICE,\n",
    "    task='task1'\n",
    ")\n",
    "\n",
    "\n",
    "task1_train_time = (datetime.now() - task1_start_time).total_seconds()\n",
    "print(f\"\\n Total Task 1 time: {task1_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7C: TASK 1 EVALUATION & RESULTS EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Evaluating Task 1 on Test Set...\")\n",
    "\n",
    "# Evaluate\n",
    "preds_t1, targets_t1, metrics_t1 = evaluate_model(\n",
    "    model_t1, test_loader_t1, DEVICE, task='task1'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1 TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in metrics_t1.items():\n",
    "    print(f\"   {metric}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare comprehensive results\n",
    "task1_full_config = {\n",
    "    **TASK1_CONFIG,\n",
    "    'input_dim': input_dim_t1,\n",
    "    'n_parameters': n_params,\n",
    "    'num_basins': len(basin_ids),\n",
    "    'use_static': USE_STATIC,\n",
    "    'train_samples': len(X_train_t1),\n",
    "    'val_samples': len(X_val_t1),\n",
    "    'test_samples': len(X_test_t1),\n",
    "}\n",
    "\n",
    "task1_metrics = {\n",
    "    'train': {'final_loss': history_t1['train_loss'][-1], 'final_nse': history_t1['val_nse'][-1]},\n",
    "    'val': {'best_loss': min(history_t1['val_loss']), 'best_nse': max(history_t1['val_nse'])},\n",
    "    'test': metrics_t1\n",
    "}\n",
    "\n",
    "task1_predictions = {\n",
    "    'y_pred': preds_t1,\n",
    "    'y_true': targets_t1,\n",
    "    'basin_scalers': {k: v for k, v in preprocessor.basin_scalers.items()}\n",
    "}\n",
    "\n",
    "# Create optimizer for saving\n",
    "optimizer_t1 = optim.Adam(model_t1.parameters(), lr=TASK1_CONFIG['lr'])\n",
    "\n",
    "# Save comprehensive results\n",
    "task1_run_dir = save_comprehensive_results(\n",
    "    task_name='task1',\n",
    "    model=model_t1,\n",
    "    optimizer=optimizer_t1,\n",
    "    config=task1_full_config,\n",
    "    metrics=task1_metrics,\n",
    "    history=history_t1,\n",
    "    predictions=task1_predictions,\n",
    "    epoch=history_t1['best_epoch']\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Training curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history_t1['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history_t1['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.axvline(history_t1['best_epoch']-1, color='r', linestyle='--', alpha=0.7, label=f\"Best Epoch ({history_t1['best_epoch']})\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. NSE curve\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history_t1['val_nse'], label='Validation NSE', linewidth=2, color='green')\n",
    "ax2.axhline(metrics_t1['NSE'], color='red', linestyle='--', alpha=0.7, label=f\"Test NSE ({metrics_t1['NSE']:.4f})\")\n",
    "ax2.axvline(history_t1['best_epoch']-1, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('NSE')\n",
    "ax2.set_title('Validation NSE over Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(targets_t1, preds_t1, alpha=0.3, s=1)\n",
    "ax3.plot([targets_t1.min(), targets_t1.max()], [targets_t1.min(), targets_t1.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax3.set_xlabel('Observed (normalized)')\n",
    "ax3.set_ylabel('Predicted (normalized)')\n",
    "ax3.set_title(f'Predicted vs Observed (R² = {metrics_t1[\"R2\"]:.4f})')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Sample time series\n",
    "ax4 = axes[1, 1]\n",
    "sample_size = min(500, len(preds_t1))\n",
    "ax4.plot(range(sample_size), targets_t1[:sample_size], label='Observed', alpha=0.8, linewidth=1)\n",
    "ax4.plot(range(sample_size), preds_t1[:sample_size], label='Predicted', alpha=0.8, linewidth=1)\n",
    "ax4.set_xlabel('Sample Index')\n",
    "ax4.set_ylabel('Streamflow (normalized)')\n",
    "ax4.set_title('Sample Time Series Comparison')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(task1_run_dir / 'training_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Task 1 complete! Results saved to: {task1_run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
