{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4bc358",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd74411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS (Standard Libraries Only - No Local Modules)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional, Any, Union\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4373f1",
   "metadata": {},
   "source": [
    "### 1.1 Configurations & Results Containers\n",
    "\n",
    "Task-specific configuration dataclasses and result containers for Task 1 (single-step) and Task 2 (multi-step Seq2Seq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53dcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1Config and Task2Config defined.\n",
      "Task1Config and Task2Config defined.\n",
      "Task1Config and Task2Config defined.\n",
      "Task1Config and Task2Config defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TASK 1 CONFIGURATION (Separate, Full Control)\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Task1Config:\n",
    "    \"\"\"\n",
    "    Configuration for Task 1: Single-step LSTM prediction (t+2).\n",
    "    \n",
    "    All parameters for Task 1 are consolidated here for full control.\n",
    "    Instantiate with custom values for easy experimentation:\n",
    "    \n",
    "        cfg1 = Task1Config(num_basins=10, epochs=50, hidden_dim=64)\n",
    "    \"\"\"\n",
    "    # === EXPERIMENT SETTINGS ===\n",
    "    num_basins: int = 5              # 0 = use ALL basins, >0 = limit for quick testing\n",
    "    use_static: bool = True          # Whether to include static catchment attributes\n",
    "    \n",
    "    # === TRAINING HYPERPARAMETERS ===\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 256\n",
    "    learning_rate: float = 0.001\n",
    "    dropout: float = 0.2\n",
    "    \n",
    "    # === EARLY STOPPING ===\n",
    "    patience: int = 10               # Stop if val_loss doesn't improve for this many epochs\n",
    "    \n",
    "    # === MODEL ARCHITECTURE ===\n",
    "    hidden_dim: int = 64             # Hidden dimension for LSTM\n",
    "    use_bidirectional: bool = True   # Use bidirectional LSTM\n",
    "    use_self_attention: bool = True  # Apply multi-head self-attention after LSTM\n",
    "    num_attention_heads: int = 4     # Number of heads for self-attention\n",
    "    use_layer_norm: bool = True      # Apply LayerNorm in model\n",
    "    num_lstm_layers: int = 1         # Number of stacked LSTM layers\n",
    "    \n",
    "    # === ADVANCED TRAINING TECHNIQUES (Toggleable) ===\n",
    "    use_weight_decay: bool = True    # Use AdamW with weight decay (else Adam)\n",
    "    weight_decay: float = 0.01       # L2 regularization strength\n",
    "    use_scheduler: bool = True       # Use ReduceLROnPlateau scheduler\n",
    "    scheduler_patience: int = 5      # Scheduler patience before reducing LR\n",
    "    scheduler_factor: float = 0.5    # Factor to reduce LR by\n",
    "    use_gradient_clipping: bool = True  # Apply gradient clipping\n",
    "    gradient_clip_value: float = 1.0    # Max gradient norm\n",
    "    \n",
    "    # === VISUALIZATION ===\n",
    "    show_plots: bool = True          # Set False to suppress plots during batch experiments\n",
    "    \n",
    "    # === DEVICE ===\n",
    "    device_type: str = 'auto'        # 'auto', 'cuda', 'cpu'\n",
    "    \n",
    "    # === PATHS ===\n",
    "    base_dir: str = './basin_dataset_public'\n",
    "    results_dir: str = './results/task1'\n",
    "    \n",
    "    # === SEQUENCE PARAMETERS ===\n",
    "    seq_length: int = 60             # Lookback window (days)\n",
    "    predict_horizon: int = 2         # Predict t + k\n",
    "    \n",
    "    # === DATA SPLIT (Hydrological Years) ===\n",
    "    train_start: str = '1980-10-01'\n",
    "    train_end: str = '1995-09-30'\n",
    "    val_start: str = '1995-10-01'\n",
    "    val_end: str = '2000-09-30'\n",
    "    test_start: str = '2000-10-01'\n",
    "    test_end: str = '2010-09-30'\n",
    "    \n",
    "    # === FEATURE SELECTION ===\n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp',   # Original Forcing\n",
    "        'PRCP_roll3', 'PRCP_roll7',             # Rolling Stats\n",
    "        'Q_lag1', 'Q_lag2', 'Q_lag3'            # Lag Features\n",
    "    ])\n",
    "    \n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'area_gages2',  # Catchment Area (Will be Log transformed)\n",
    "        'elev_mean',    # Mean Elevation\n",
    "        'slope_mean',   # Topography\n",
    "        'sand_frac',    # Soil Type\n",
    "        'clay_frac',    # Soil Type\n",
    "        'frac_forest',  # Vegetation\n",
    "        'lai_max',      # Leaf Area Index\n",
    "        'p_mean',       # Long-term climate\n",
    "        'aridity'       # Climate Index\n",
    "    ])\n",
    "    \n",
    "    target: str = 'Q_cms'\n",
    "    \n",
    "    # === CONSTANTS ===\n",
    "    cfs_to_cms: float = 0.0283168    # Cubic feet/sec to cubic meters/sec\n",
    "    epsilon: float = 1e-6            # Small constant for numerical stability\n",
    "    \n",
    "    # === DERIVED PATHS (computed in __post_init__) ===\n",
    "    forcing_dir: str = field(init=False)\n",
    "    flow_dir: str = field(init=False)\n",
    "    meta_dir: str = field(init=False)\n",
    "    bad_basins_file: str = field(init=False)\n",
    "    device: torch.device = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Compute derived attributes after initialization.\"\"\"\n",
    "        self.forcing_dir = os.path.join(self.base_dir, 'basin_mean_forcing', 'nldas')\n",
    "        self.flow_dir = os.path.join(self.base_dir, 'usgs_streamflow')\n",
    "        self.meta_dir = self.base_dir\n",
    "        self.bad_basins_file = os.path.join(self.base_dir, 'basin_size_errors_10_percent.txt')\n",
    "        \n",
    "        if self.device_type == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(self.device_type)\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Pretty print the configuration.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"TASK 1 CONFIGURATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n--- Experiment Settings ---\")\n",
    "        print(f\"Number of Basins: {self.num_basins} {'(ALL)' if self.num_basins == 0 else '(limited)'}\")\n",
    "        print(f\"Use Static Features: {self.use_static}\")\n",
    "        \n",
    "        print(f\"\\n--- Training Hyperparameters ---\")\n",
    "        print(f\"Epochs: {self.epochs}, Batch Size: {self.batch_size}\")\n",
    "        print(f\"Learning Rate: {self.learning_rate}, Dropout: {self.dropout}\")\n",
    "        print(f\"Early Stopping Patience: {self.patience}\")\n",
    "        \n",
    "        print(f\"\\n--- Model Architecture ---\")\n",
    "        print(f\"LSTM Layers: {self.num_lstm_layers}\")\n",
    "        print(f\"Bidirectional LSTM: {self.use_bidirectional}\")\n",
    "        print(f\"Self-Attention: {self.use_self_attention}\" + \n",
    "              (f\" (Heads: {self.num_attention_heads})\" if self.use_self_attention else \"\"))\n",
    "        print(f\"Layer Normalization: {self.use_layer_norm}\")\n",
    "        \n",
    "        print(f\"\\n--- Advanced Training Techniques ---\")\n",
    "        print(f\"Weight Decay (AdamW): {self.use_weight_decay}\" + \n",
    "              (f\" (λ={self.weight_decay})\" if self.use_weight_decay else \"\"))\n",
    "        print(f\"LR Scheduler: {self.use_scheduler}\" + \n",
    "              (f\" (patience={self.scheduler_patience}, factor={self.scheduler_factor})\" if self.use_scheduler else \"\"))\n",
    "        print(f\"Gradient Clipping: {self.use_gradient_clipping}\" + \n",
    "              (f\" (max_norm={self.gradient_clip_value})\" if self.use_gradient_clipping else \"\"))\n",
    "        \n",
    "        print(f\"\\n--- Device ---\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        if self.device.type == 'cuda':\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        print(f\"\\n--- Sequence Parameters ---\")\n",
    "        print(f\"Sequence Length: {self.seq_length} days\")\n",
    "        print(f\"Prediction Horizon: t+{self.predict_horizon}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TASK 2 CONFIGURATION (Separate, Full Control)\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Task2Config:\n",
    "    \"\"\"\n",
    "    Configuration for Task 2: Multi-step Seq2Seq prediction (t+1 to t+5).\n",
    "    \n",
    "    All parameters for Task 2 are consolidated here for full control.\n",
    "    Instantiate with custom values for easy experimentation:\n",
    "    \n",
    "        cfg2 = Task2Config(num_basins=10, epochs=50, hidden_dim=128)\n",
    "    \"\"\"\n",
    "    # === EXPERIMENT SETTINGS ===\n",
    "    num_basins: int = 5              # 0 = use ALL basins, >0 = limit for quick testing\n",
    "    use_static: bool = True          # Whether to include static catchment attributes\n",
    "    \n",
    "    # === TRAINING HYPERPARAMETERS ===\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 256\n",
    "    learning_rate: float = 0.001\n",
    "    dropout: float = 0.2\n",
    "    \n",
    "    # === EARLY STOPPING ===\n",
    "    patience: int = 10               # Stop if val_loss doesn't improve for this many epochs\n",
    "    \n",
    "    # === MODEL ARCHITECTURE ===\n",
    "    hidden_dim: int = 128            # Hidden dimension for Seq2Seq (larger for complexity)\n",
    "    use_bidirectional_encoder: bool = True   # Use bidirectional encoder\n",
    "    use_self_attention: bool = True          # Apply self-attention in encoder\n",
    "    num_attention_heads: int = 4             # Number of heads for self-attention\n",
    "    use_layer_norm: bool = True              # Apply LayerNorm in model\n",
    "    num_encoder_layers: int = 1      # Number of stacked LSTM layers in encoder\n",
    "    num_decoder_layers: int = 1      # Number of stacked LSTMCell layers in decoder\n",
    "    \n",
    "    # === TEACHER FORCING ===\n",
    "    teacher_forcing_ratio: float = 0.5  # Initial teacher forcing probability\n",
    "    tf_decay: bool = True                # Whether to decay TF ratio over epochs\n",
    "    \n",
    "    # === ADVANCED TRAINING TECHNIQUES (Toggleable) ===\n",
    "    use_weight_decay: bool = True    # Use AdamW with weight decay (else Adam)\n",
    "    weight_decay: float = 0.01       # L2 regularization strength\n",
    "    use_scheduler: bool = True       # Use ReduceLROnPlateau scheduler\n",
    "    scheduler_patience: int = 5      # Scheduler patience before reducing LR\n",
    "    scheduler_factor: float = 0.5    # Factor to reduce LR by\n",
    "    use_gradient_clipping: bool = True  # Apply gradient clipping\n",
    "    gradient_clip_value: float = 1.0    # Max gradient norm\n",
    "    \n",
    "    # === VISUALIZATION ===\n",
    "    show_plots: bool = True          # Set False to suppress plots during batch experiments\n",
    "    \n",
    "    # === DEVICE ===\n",
    "    device_type: str = 'auto'        # 'auto', 'cuda', 'cpu'\n",
    "    \n",
    "    # === PATHS ===\n",
    "    base_dir: str = './basin_dataset_public'\n",
    "    results_dir: str = './results/task2'\n",
    "    \n",
    "    # === SEQUENCE PARAMETERS ===\n",
    "    seq_length: int = 60             # Lookback window (days)\n",
    "    predict_steps: int = 5           # Predict sequence t+1...t+5\n",
    "    \n",
    "    # === DATA SPLIT (Hydrological Years) ===\n",
    "    train_start: str = '1980-10-01'\n",
    "    train_end: str = '1995-09-30'\n",
    "    val_start: str = '1995-10-01'\n",
    "    val_end: str = '2000-09-30'\n",
    "    test_start: str = '2000-10-01'\n",
    "    test_end: str = '2010-09-30'\n",
    "    \n",
    "    # === FEATURE SELECTION ===\n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp',   # Original Forcing\n",
    "        'PRCP_roll3', 'PRCP_roll7',             # Rolling Stats\n",
    "        'Q_lag1', 'Q_lag2', 'Q_lag3'            # Lag Features\n",
    "    ])\n",
    "    \n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'area_gages2',  # Catchment Area (Will be Log transformed)\n",
    "        'elev_mean',    # Mean Elevation\n",
    "        'slope_mean',   # Topography\n",
    "        'sand_frac',    # Soil Type\n",
    "        'clay_frac',    # Soil Type\n",
    "        'frac_forest',  # Vegetation\n",
    "        'lai_max',      # Leaf Area Index\n",
    "        'p_mean',       # Long-term climate\n",
    "        'aridity'       # Climate Index\n",
    "    ])\n",
    "    \n",
    "    # Future forcing features (for Seq2Seq decoder)\n",
    "    future_forcing_features: List[str] = field(default_factory=lambda: [\n",
    "        'PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp'\n",
    "    ])\n",
    "    target: str = 'Q_cms'\n",
    "    target: str = 'Q_cms'\n",
    "    \n",
    "    # === CONSTANTS ===\n",
    "    cfs_to_cms: float = 0.0283168    # Cubic feet/sec to cubic meters/sec\n",
    "    epsilon: float = 1e-6            # Small constant for numerical stability\n",
    "    \n",
    "    # === DERIVED PATHS (computed in __post_init__) ===\n",
    "    forcing_dir: str = field(init=False)\n",
    "    flow_dir: str = field(init=False)\n",
    "    meta_dir: str = field(init=False)\n",
    "    bad_basins_file: str = field(init=False)\n",
    "    device: torch.device = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Compute derived attributes after initialization.\"\"\"\n",
    "        self.forcing_dir = os.path.join(self.base_dir, 'basin_mean_forcing', 'nldas')\n",
    "        self.flow_dir = os.path.join(self.base_dir, 'usgs_streamflow')\n",
    "        self.meta_dir = self.base_dir\n",
    "        self.bad_basins_file = os.path.join(self.base_dir, 'basin_size_errors_10_percent.txt')\n",
    "        \n",
    "        if self.device_type == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(self.device_type)\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Pretty print the configuration.\"\"\"\n",
    "        print(\"TASK 2 CONFIGURATION\")\n",
    "        print(\"TASK 2 CONFIGURATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n--- Experiment Settings ---\")\n",
    "        print(f\"Number of Basins: {self.num_basins} {'(ALL)' if self.num_basins == 0 else '(limited)'}\")\n",
    "        print(f\"Use Static Features: {self.use_static}\")\n",
    "        \n",
    "        print(f\"\\n--- Training Hyperparameters ---\")\n",
    "        print(f\"Epochs: {self.epochs}, Batch Size: {self.batch_size}\")\n",
    "        print(f\"Learning Rate: {self.learning_rate}, Dropout: {self.dropout}\")\n",
    "        print(f\"Early Stopping Patience: {self.patience}\")\n",
    "        \n",
    "        print(f\"Encoder Layers: {self.num_encoder_layers}, Decoder Layers: {self.num_decoder_layers}\")\n",
    "        print(f\"Bidirectional Encoder: {self.use_bidirectional_encoder}\")\n",
    "        print(f\"Encoder Self-Attention: {self.use_self_attention}\" + \n",
    "              (f\" (Heads: {self.num_attention_heads})\" if self.use_self_attention else \"\"))\n",
    "        print(f\"Layer Normalization: {self.use_layer_norm}\")\n",
    "        print(f\"Teacher Forcing: {self.teacher_forcing_ratio} (Decay: {self.tf_decay})\")\n",
    "        \n",
    "        print(f\"\\n--- Advanced Training Techniques ---\")\n",
    "        print(f\"Weight Decay (AdamW): {self.use_weight_decay}\" + \n",
    "              (f\" (λ={self.weight_decay})\" if self.use_weight_decay else \"\"))\n",
    "        print(f\"LR Scheduler: {self.use_scheduler}\" + \n",
    "              (f\" (patience={self.scheduler_patience}, factor={self.scheduler_factor})\" if self.use_scheduler else \"\"))\n",
    "        print(f\"Gradient Clipping: {self.use_gradient_clipping}\" + \n",
    "              (f\" (max_norm={self.gradient_clip_value})\" if self.use_gradient_clipping else \"\"))\n",
    "        \n",
    "        print(f\"\\n--- Device ---\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        if self.device.type == 'cuda':\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        print(f\"\\n--- Sequence Parameters ---\")\n",
    "        print(f\"Sequence Length: {self.seq_length} days\")\n",
    "        print(f\"Prediction Steps: {self.predict_steps} (t+1 to t+{self.predict_steps})\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"Task1Config and Task2Config defined.\")\n",
    "\n",
    "print(\"Task1Config and Task2Config defined.\")\n",
    "print(\"Task1Config and Task2Config defined.\")\n",
    "print(\"Task1Config and Task2Config defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99d0f0",
   "metadata": {},
   "source": [
    "#### (Continued) Results Containers\n",
    "\n",
    "Dataclasses for storing pipeline outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4b3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1Results and Task2Results containers defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TASK 1 RESULTS CONTAINER\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Task1Results:\n",
    "    \"\"\"\n",
    "    Container for Task 1 pipeline outputs.\n",
    "    \n",
    "    Stores the trained model, training history, evaluation metrics,\n",
    "    predictions, and metadata from the Task 1 pipeline.\n",
    "    \"\"\"\n",
    "    model: nn.Module                         # Trained LSTMWithAttention model\n",
    "    history: Dict[str, List[float]]          # {'train_loss': [...], 'val_loss': [...]}\n",
    "    metrics: Dict[str, float]                # {'MSE', 'RMSE', 'MAE', 'NSE', 'R2'}\n",
    "    predictions: np.ndarray                  # (N,) predictions on test set\n",
    "    targets: np.ndarray                      # (N,) ground truth on test set\n",
    "    basin_ids: List[str]                     # List of basin IDs used\n",
    "    preprocessor: Any                        # CamelsPreprocessor for inverse transform\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print a summary of Task 1 results.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TASK 1 RESULTS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Basins processed: {len(self.basin_ids)}\")\n",
    "        print(f\"Test samples: {len(self.predictions)}\")\n",
    "        print(f\"\\nTest Metrics:\")\n",
    "        for k, v in self.metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        print(f\"\\nTraining epochs: {len(self.history['train_loss'])}\")\n",
    "        print(f\"Best val loss: {min(self.history['val_loss']):.6f}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TASK 2 RESULTS CONTAINER\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Task2Results:\n",
    "    \"\"\"\n",
    "    Container for Task 2 pipeline outputs.\n",
    "    \n",
    "    Stores the trained model, training history, evaluation metrics (overall and per-step),\n",
    "    predictions, and metadata from the Task 2 pipeline.\n",
    "    \"\"\"\n",
    "    model: nn.Module                              # Trained LSTM_Seq2Seq model\n",
    "    history: Dict[str, List[float]]               # {'train_loss': [...], 'val_loss': [...]}\n",
    "    metrics: Dict[str, float]                     # Averaged metrics across all forecast steps\n",
    "    metrics_per_step: Dict[int, Dict[str, float]] # {1: {...}, 2: {...}, ...} per-horizon metrics\n",
    "    predictions: np.ndarray                       # (N, 5) predictions on test set\n",
    "    targets: np.ndarray                           # (N, 5) ground truth on test set\n",
    "    basin_ids: List[str]                          # List of basin IDs used\n",
    "    preprocessor: Any                             # CamelsPreprocessor for inverse transform\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print a summary of Task 2 results.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TASK 2 RESULTS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Basins processed: {len(self.basin_ids)}\")\n",
    "        print(f\"Test samples: {len(self.predictions)}\")\n",
    "        print(f\"\\nTest Metrics (averaged across {len(self.metrics_per_step)} steps):\")\n",
    "        for k, v in self.metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "        print(f\"\\nPer-Step NSE:\")\n",
    "        for step, step_metrics in self.metrics_per_step.items():\n",
    "            print(f\"  t+{step}: NSE={step_metrics['NSE']:.4f}, RMSE={step_metrics['RMSE']:.4f}\")\n",
    "        print(f\"\\nTraining epochs: {len(self.history['train_loss'])}\")\n",
    "        print(f\"Best val loss: {min(self.history['val_loss']):.6f}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"Task1Results and Task2Results containers defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3b712",
   "metadata": {},
   "source": [
    "### 1.2 Data Pipeline (Loader, Engineering, Preprocessing)\n",
    "\n",
    "Classes for loading CAMELS data, engineering features, and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ffee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamelsLoader class defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADER CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class CamelsLoader:\n",
    "    \"\"\"\n",
    "    Data loader for CAMELS dataset.\n",
    "    Handles loading streamflow, forcing data, and static attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Union[Task1Config, Task2Config]):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def load_bad_basins(self) -> List[str]:\n",
    "        \"\"\"Returns a list of basin IDs to exclude due to data quality issues.\"\"\"\n",
    "        if not os.path.exists(self.cfg.bad_basins_file):\n",
    "            return []\n",
    "        \n",
    "        bad_ids = []\n",
    "        try:\n",
    "            with open(self.cfg.bad_basins_file, 'r') as f:\n",
    "                next(f)  # Skip header\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        bad_ids.append(parts[1])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse bad basins file: {e}\")\n",
    "        return bad_ids\n",
    "\n",
    "    def get_basin_list(self) -> pd.DataFrame:\n",
    "        \"\"\"Scans directories and returns valid basin list (excluding bad basins).\"\"\"\n",
    "        bad_basins = self.load_bad_basins()\n",
    "        \n",
    "        search_path = os.path.join(self.cfg.flow_dir, '**', '*_streamflow_qc.txt')\n",
    "        files = glob.glob(search_path, recursive=True)\n",
    "        \n",
    "        basins = []\n",
    "        for f in files:\n",
    "            parts = f.split(os.sep)\n",
    "            region = parts[-2]\n",
    "            gauge_id = parts[-1].split('_')[0]\n",
    "            \n",
    "            if gauge_id not in bad_basins:\n",
    "                basins.append({'gauge_id': gauge_id, 'region': region})\n",
    "                \n",
    "        return pd.DataFrame(basins)\n",
    "\n",
    "    def load_dynamic_data(self, gauge_id: str, region: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads Streamflow + Forcing data for a single basin.\n",
    "        \n",
    "        Args:\n",
    "            gauge_id: USGS gauge identifier\n",
    "            region: HUC region code\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with columns: Q_cms, PRCP, SRAD, Tmax, Tmin, Vp\n",
    "            Index: DatetimeIndex\n",
    "        \"\"\"\n",
    "        # 1. Load Streamflow\n",
    "        flow_path = os.path.join(self.cfg.flow_dir, region, f'{gauge_id}_streamflow_qc.txt')\n",
    "        try:\n",
    "            df_flow = pd.read_csv(flow_path, sep=r'\\s+', header=None,\n",
    "                                  names=['gauge_id', 'Year', 'Month', 'Day', 'Q_cfs', 'QC'])\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        df_flow['Date'] = pd.to_datetime(df_flow[['Year', 'Month', 'Day']])\n",
    "        df_flow.set_index('Date', inplace=True)\n",
    "        # Convert to CMS and mark missing values\n",
    "        df_flow['Q_cms'] = df_flow['Q_cfs'].replace(-999, np.nan) * self.cfg.cfs_to_cms\n",
    "\n",
    "        # 2. Load Forcing (NLDAS)\n",
    "        forcing_path = os.path.join(self.cfg.forcing_dir, region, f'{gauge_id}_lump_nldas_forcing_leap.txt')\n",
    "        if not os.path.exists(forcing_path):\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            df_force = pd.read_csv(forcing_path, sep=r'\\s+', skiprows=3)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        # Standardize column names\n",
    "        col_map = {\n",
    "            'Mnth': 'Month', 'month': 'Month', 'mo': 'Month',\n",
    "            'year': 'Year', 'yr': 'Year',\n",
    "            'day': 'Day', 'dy': 'Day',\n",
    "            'prcp(mm/day)': 'PRCP', 'srad(w/m2)': 'SRAD', \n",
    "            'tmax(c)': 'Tmax', 'tmin(c)': 'Tmin', 'vp(pa)': 'Vp'\n",
    "        }\n",
    "        \n",
    "        new_cols = {}\n",
    "        for c in df_force.columns:\n",
    "            clean = c.strip()\n",
    "            if clean.lower() in col_map:\n",
    "                new_cols[c] = col_map[clean.lower()]\n",
    "            elif clean in col_map:\n",
    "                new_cols[c] = col_map[clean]\n",
    "        \n",
    "        df_force.rename(columns=new_cols, inplace=True)\n",
    "        \n",
    "        # Create Date Index\n",
    "        try:\n",
    "            df_force['Date'] = pd.to_datetime(df_force[['Year', 'Month', 'Day']])\n",
    "            df_force.set_index('Date', inplace=True)\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "        # 3. Merge (inner join to ensure alignment)\n",
    "        base_forcing = ['PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp']\n",
    "        cols_to_use = [c for c in base_forcing if c in df_force.columns]\n",
    "        df_merged = df_flow[['Q_cms']].join(df_force[cols_to_use], how='inner')\n",
    "        \n",
    "        return df_merged\n",
    "\n",
    "    def load_static_attributes(self, basins_list: List[str] = None) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads all static attribute files, merges them, and filters for configured features.\n",
    "        \n",
    "        Args:\n",
    "            basins_list: List of gauge_ids to include (filters result)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with static features, indexed by gauge_id\n",
    "        \"\"\"\n",
    "        files = ['camels_topo.txt', 'camels_soil.txt', 'camels_clim.txt', \n",
    "                 'camels_vege.txt', 'camels_geol.txt']\n",
    "        \n",
    "        dfs = []\n",
    "        for filename in files:\n",
    "            path = os.path.join(self.cfg.meta_dir, filename)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    df = pd.read_csv(path, sep=';')\n",
    "                    df.columns = [c.strip() for c in df.columns]\n",
    "                    if 'gauge_id' in df.columns:\n",
    "                        df['gauge_id'] = df['gauge_id'].astype(str).str.zfill(8)\n",
    "                        df.set_index('gauge_id', inplace=True)\n",
    "                        dfs.append(df)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if not dfs:\n",
    "            return None\n",
    "\n",
    "        # Merge all static files\n",
    "        df_static = pd.concat(dfs, axis=1)\n",
    "        # Remove duplicate columns\n",
    "        df_static = df_static.loc[:, ~df_static.columns.duplicated()]\n",
    "\n",
    "        # Filter for configured features\n",
    "        available_feats = [f for f in self.cfg.static_features if f in df_static.columns]\n",
    "        df_final = df_static[available_feats]\n",
    "\n",
    "        if basins_list is not None:\n",
    "            df_final = df_final.reindex(basins_list)\n",
    "            \n",
    "        return df_final\n",
    "\n",
    "print(\"CamelsLoader class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66e77e",
   "metadata": {},
   "source": [
    "#### (Continued) Feature Engineering\n",
    "\n",
    "Rolling statistics and lag feature computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75030383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureEngineer class defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Creates derived features from raw hydrometeorological data.\n",
    "    \n",
    "    Features created:\n",
    "    - PRCP_roll3: 3-day rolling mean precipitation (short-term wetness)\n",
    "    - PRCP_roll7: 7-day rolling mean precipitation (medium-term saturation)\n",
    "    - Q_lag1, Q_lag2, Q_lag3: Lagged streamflow values\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Union[Task1Config, Task2Config]):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def add_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates rolling statistics for Precipitation.\n",
    "        Represents accumulated soil moisture / wetness.\n",
    "        \"\"\"\n",
    "        if 'PRCP' in df.columns:\n",
    "            # 3-Day Rolling Mean (Short-term wetness)\n",
    "            df['PRCP_roll3'] = df['PRCP'].rolling(window=3, min_periods=1).mean()\n",
    "            # 7-Day Rolling Mean (Medium-term saturation)\n",
    "            df['PRCP_roll7'] = df['PRCP'].rolling(window=7, min_periods=1).mean()\n",
    "        return df\n",
    "\n",
    "    def add_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Adds explicit lag features for Streamflow.\n",
    "        Gives the model explicit access to past flow values.\n",
    "        \n",
    "        Note: Shifting creates NaNs at the top of the dataframe.\n",
    "        These will be handled by the preprocessor's handle_missing_data().\n",
    "        \"\"\"\n",
    "        target = self.cfg.target  # 'Q_cms'\n",
    "        \n",
    "        if target in df.columns:\n",
    "            df['Q_lag1'] = df[target].shift(1)  # Flow yesterday (t-1)\n",
    "            df['Q_lag2'] = df[target].shift(2)  # Flow 2 days ago (t-2)\n",
    "            df['Q_lag3'] = df[target].shift(3)  # Flow 3 days ago (t-3)\n",
    "        return df\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Applies all feature engineering steps.\n",
    "        Should be run BEFORE missing data interpolation.\n",
    "        \"\"\"\n",
    "        df = self.add_rolling_features(df)\n",
    "        df = self.add_lag_features(df)\n",
    "        return df\n",
    "\n",
    "print(\"FeatureEngineer class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bcc23",
   "metadata": {},
   "source": [
    "#### (Continued) Preprocessor\n",
    "\n",
    "Target transformation, scaling, and data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cd5343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamelsPreprocessor class defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PREPROCESSOR CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class CamelsPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor for CAMELS data.\n",
    "    \n",
    "    Handles:\n",
    "    - Physical outlier cleaning (negative rain/flow → 0, unrealistic temps → NaN)\n",
    "    - Missing data interpolation (with gap limits)\n",
    "    - Cyclical date encoding (sin/cos of day-of-year)\n",
    "    - Normalization (global dynamic stats, basin-specific target stats)\n",
    "    - Sequence generation for LSTM input\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Union[Task1Config, Task2Config]):\n",
    "        self.cfg = cfg\n",
    "        self.scalers = {}\n",
    "        self.basin_scalers = {}\n",
    "        \n",
    "        # Physical Constraints for data cleaning\n",
    "        self.PHYSICAL_LIMITS = {\n",
    "            'PRCP': {'min': 0.0, 'max': None},\n",
    "            'Q_cms': {'min': 0.0, 'max': None},\n",
    "            'Tmax': {'min': -60.0, 'max': 60.0},\n",
    "            'Tmin': {'min': -60.0, 'max': 60.0}\n",
    "        }\n",
    "        self.MAX_INTERPOLATE_GAP = 5  # Max consecutive days to interpolate\n",
    "\n",
    "    def add_date_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Adds cyclical encoding of day-of-year.\"\"\"\n",
    "        day_of_year = df.index.dayofyear\n",
    "        df['sin_doy'] = np.sin(2 * np.pi * day_of_year / 365.0)\n",
    "        df['cos_doy'] = np.cos(2 * np.pi * day_of_year / 365.0)\n",
    "        return df\n",
    "\n",
    "    def clean_physical_outliers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Enforces physical constraints on the data.\"\"\"\n",
    "        # Negative Rain/Flow → 0\n",
    "        for col in ['PRCP', self.cfg.target]:\n",
    "            if col in df.columns:\n",
    "                mask = df[col] < 0\n",
    "                if mask.any():\n",
    "                    df.loc[mask, col] = 0.0\n",
    "        \n",
    "        # Unrealistic Temp → NaN\n",
    "        for col in ['Tmax', 'Tmin']:\n",
    "            if col in df.columns:\n",
    "                limits = self.PHYSICAL_LIMITS[col]\n",
    "                mask = (df[col] < limits['min']) | (df[col] > limits['max'])\n",
    "                if mask.any():\n",
    "                    df.loc[mask, col] = np.nan\n",
    "        return df\n",
    "\n",
    "    def handle_missing_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Interpolates missing values with gap limit.\"\"\"\n",
    "        cols_to_fix = [self.cfg.target] + self.cfg.dynamic_features\n",
    "        cols_to_fix = [c for c in cols_to_fix if c in df.columns]\n",
    "\n",
    "        for col in cols_to_fix:\n",
    "            # Linear interpolate short gaps only\n",
    "            df[col] = df[col].interpolate(method='linear', limit=self.MAX_INTERPOLATE_GAP, limit_direction='both')\n",
    "            # Handle edges with forward/backward fill\n",
    "            df[col] = df[col].ffill().bfill()\n",
    "        return df\n",
    "\n",
    "    def fit(self, dynamic_data_dict: Dict[str, pd.DataFrame], static_df: pd.DataFrame = None):\n",
    "        \"\"\"\n",
    "        Computes normalization statistics from TRAINING data only.\n",
    "        \n",
    "        Args:\n",
    "            dynamic_data_dict: {gauge_id: DataFrame} of processed dynamic data\n",
    "            static_df: DataFrame of static features (optional)\n",
    "        \"\"\"\n",
    "        print(\"Computing global statistics from training data...\")\n",
    "        \n",
    "        # 1. Dynamic Feature Stats (global across all basins)\n",
    "        dyn_vals = []\n",
    "        for gid, df in dynamic_data_dict.items():\n",
    "            train_slice = df.loc[self.cfg.train_start:self.cfg.train_end]\n",
    "            if not train_slice.empty:\n",
    "                # Only use configured dynamic features\n",
    "                available_cols = [c for c in self.cfg.dynamic_features if c in train_slice.columns]\n",
    "                valid_rows = train_slice[available_cols].dropna()\n",
    "                if not valid_rows.empty:\n",
    "                    dyn_vals.append(valid_rows.values)\n",
    "        \n",
    "        if dyn_vals:\n",
    "            all_dyn = np.vstack(dyn_vals)\n",
    "            self.scalers['dynamic_mean'] = np.mean(all_dyn, axis=0)\n",
    "            self.scalers['dynamic_std'] = np.std(all_dyn, axis=0) + self.cfg.epsilon\n",
    "        else:\n",
    "            self.scalers['dynamic_mean'] = 0\n",
    "            self.scalers['dynamic_std'] = 1\n",
    "\n",
    "        # 2. Static Feature Stats (with log-transform for area)\n",
    "        if static_df is not None:\n",
    "            static_df_copy = static_df.copy()\n",
    "            if 'area_gages2' in static_df_copy.columns:\n",
    "                static_df_copy['area_gages2'] = np.log10(np.maximum(static_df_copy['area_gages2'], 1e-3))\n",
    "            self.scalers['static_mean'] = static_df_copy.mean().values\n",
    "            self.scalers['static_std'] = static_df_copy.std().values + self.cfg.epsilon\n",
    "        else:\n",
    "            print(\"-> Skipping Static Stats (Static Data not provided)\")\n",
    "\n",
    "        # 3. Basin-specific Target Stats\n",
    "        print(\"Computing basin-specific target statistics...\")\n",
    "        for gid, df in dynamic_data_dict.items():\n",
    "            train_slice = df.loc[self.cfg.train_start:self.cfg.train_end]\n",
    "            clean_target = train_slice[self.cfg.target].dropna()\n",
    "            \n",
    "            if not clean_target.empty:\n",
    "                self.basin_scalers[gid] = {'mean': clean_target.mean(), 'std': clean_target.std() + self.cfg.epsilon}\n",
    "            else:\n",
    "                self.basin_scalers[gid] = {'mean': 0, 'std': 1}\n",
    "\n",
    "    def transform(self, df_dynamic: pd.DataFrame, df_static: pd.DataFrame = None, \n",
    "                  gauge_id: str = None) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Normalizes data and creates feature matrix.\n",
    "        \n",
    "        Returns:\n",
    "            data_matrix: (Time, Features) normalized array\n",
    "            static_norm: Normalized static vector or None\n",
    "        \"\"\"\n",
    "        # 1. Normalize Dynamic Features\n",
    "        dyn_cols = [c for c in self.cfg.dynamic_features if c in df_dynamic.columns]\n",
    "        X_dyn = df_dynamic[dyn_cols].values\n",
    "        X_dyn_norm = (X_dyn - self.scalers['dynamic_mean']) / self.scalers['dynamic_std']\n",
    "        \n",
    "        # 2. Normalize Target (basin-specific)\n",
    "        target = df_dynamic[self.cfg.target].values\n",
    "        b_stats = self.basin_scalers.get(gauge_id, {'mean': 0, 'std': 1})\n",
    "        y_norm = (target - b_stats['mean']) / b_stats['std']\n",
    "        \n",
    "        # 3. Normalize Static Features (if available)\n",
    "        X_stat_norm = None\n",
    "        if df_static is not None and gauge_id in df_static.index:\n",
    "            static_vals = df_static.loc[gauge_id].values.copy()\n",
    "            if 'area_gages2' in df_static.columns:\n",
    "                area_idx = df_static.columns.get_loc('area_gages2')\n",
    "                static_vals[area_idx] = np.log10(np.maximum(static_vals[area_idx], 1e-3))\n",
    "            X_stat_norm = (static_vals - self.scalers['static_mean']) / self.scalers['static_std']\n",
    "        \n",
    "        # 4. Get Date Features\n",
    "        date_feats = df_dynamic[['sin_doy', 'cos_doy']].values\n",
    "        \n",
    "        # Combine: [Dynamic_Norm, Date, Target_Norm]\n",
    "        data_matrix = np.column_stack([X_dyn_norm, date_feats, y_norm])\n",
    "        \n",
    "        return data_matrix, X_stat_norm\n",
    "\n",
    "    def create_sequences(self, data_matrix: np.ndarray, static_vec: np.ndarray = None, \n",
    "                         mode: str = 'task1') -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Creates sliding window sequences for LSTM.\n",
    "        \n",
    "        Args:\n",
    "            data_matrix: (Time, Features) from transform()\n",
    "            static_vec: Normalized static features or None\n",
    "            mode: 'task1' (single-step) or 'task2' (multi-step)\n",
    "            \n",
    "        Returns:\n",
    "            X_seq: (N_samples, Seq_Len, Features)\n",
    "            y_seq: (N_samples,) for task1 or (N_samples, 5) for task2\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        seq_len = self.cfg.seq_length\n",
    "        total_samples = len(data_matrix)\n",
    "        \n",
    "        use_static = (static_vec is not None)\n",
    "        if use_static:\n",
    "            static_repeated = np.tile(static_vec, (seq_len, 1))\n",
    "\n",
    "        if mode == 'task1':\n",
    "            horizon = self.cfg.predict_horizon\n",
    "            for t in range(seq_len, total_samples - horizon + 1):\n",
    "                window_data = data_matrix[t-seq_len:t, :]\n",
    "                target_val = data_matrix[t + horizon - 1, -1]\n",
    "                \n",
    "                # Skip sequences with NaN\n",
    "                if np.isnan(window_data).any() or np.isnan(target_val):\n",
    "                    continue\n",
    "                \n",
    "                if use_static:\n",
    "                    full_X = np.hstack([window_data, static_repeated])\n",
    "                else:\n",
    "                    full_X = window_data\n",
    "                \n",
    "                X_seq.append(full_X)\n",
    "                y_seq.append(target_val)\n",
    "\n",
    "        elif mode == 'task2':\n",
    "            steps = self.cfg.predict_steps\n",
    "            for t in range(seq_len, total_samples - steps + 1):\n",
    "                window_data = data_matrix[t-seq_len:t, :]\n",
    "                target_seq = data_matrix[t:t+steps, -1]\n",
    "                \n",
    "                if np.isnan(window_data).any() or np.isnan(target_seq).any():\n",
    "                    continue\n",
    "                \n",
    "                if use_static:\n",
    "                    full_X = np.hstack([window_data, static_repeated])\n",
    "                else:\n",
    "                    full_X = window_data\n",
    "                \n",
    "                X_seq.append(full_X)\n",
    "                y_seq.append(target_seq)\n",
    "                \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "print(\"CamelsPreprocessor class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ad2b6",
   "metadata": {},
   "source": [
    "### 1.3 Neural Network Modules\n",
    "\n",
    "Attention mechanisms and model architectures for Task 1 (LSTM) and Task 2 (Seq2Seq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190eec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadSelfAttention module defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MULTI-HEAD SELF-ATTENTION MODULE\n",
    "# ==============================================================================\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention for sequence modeling.\n",
    "    \n",
    "    Computes scaled dot-product attention over the input sequence,\n",
    "    allowing the model to attend to information from different positions.\n",
    "    \n",
    "    Architecture:\n",
    "        Input: (Batch, Seq_Len, Hidden)\n",
    "        Split into num_heads attention heads\n",
    "        Each head: Q, K, V projections → Scaled Dot-Product → Concat\n",
    "        Output: (Batch, Seq_Len, Hidden)\n",
    "    \n",
    "    Args:\n",
    "        hidden_dim: Dimension of input and output features\n",
    "        num_heads: Number of attention heads (must divide hidden_dim)\n",
    "        dropout: Dropout rate on attention weights\n",
    "        use_layer_norm: Whether to apply LayerNorm after attention\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 4, \n",
    "                 dropout: float = 0.1, use_layer_norm: bool = True):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \\\n",
    "            f\"hidden_dim ({hidden_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k) for scaling\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Optional Layer Normalization (pre-norm style)\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                return_weights: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass with optional attention weight return.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (Batch, Seq_Len, Hidden)\n",
    "            return_weights: If True, also return attention weights\n",
    "            \n",
    "        Returns:\n",
    "            out: Output tensor of shape (Batch, Seq_Len, Hidden)\n",
    "            weights: (optional) Attention weights (Batch, Heads, Seq, Seq)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Apply layer norm before attention (pre-norm)\n",
    "        residual = x\n",
    "        if self.use_layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        \n",
    "        # Project to Q, K, V: (Batch, Seq, Hidden)\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head: (Batch, Heads, Seq, Head_Dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        # scores: (Batch, Heads, Seq, Seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values: (Batch, Heads, Seq, Head_Dim)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads: (Batch, Seq, Hidden)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.out_proj(attn_output)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Residual connection\n",
    "        out = out + residual\n",
    "        \n",
    "        if return_weights:\n",
    "            return out, attn_weights\n",
    "        return out\n",
    "\n",
    "print(\"MultiHeadSelfAttention module defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8565a",
   "metadata": {},
   "source": [
    "#### (Continued) LSTMWithAttention Model (Task 1)\n",
    "\n",
    "Stacked LSTM with optional bidirectional, self-attention, and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be5cd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMWithAttention model class defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ENHANCED LSTM MODEL WITH ATTENTION (Task 1: Single-Step Prediction)\n",
    "# ==============================================================================\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced LSTM for Single-Step Prediction.\n",
    "    \n",
    "    Architecture Options:\n",
    "        - Bidirectional LSTM: Processes sequence forward and backward\n",
    "        - Stacked LSTM Layers: Multiple LSTM layers for deeper representations\n",
    "        - Multi-Head Self-Attention: Applied after LSTM for global context\n",
    "        - Layer Normalization: Stabilizes training\n",
    "    \n",
    "    Input: (Batch, Seq_Len, Features)\n",
    "    Output: Single scalar (normalized flow at t+k)\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features per timestep\n",
    "        hidden_dim: Number of LSTM hidden units\n",
    "        dropout: Dropout rate\n",
    "        num_layers: Number of stacked LSTM layers\n",
    "        use_bidirectional: Use bidirectional LSTM\n",
    "        use_self_attention: Apply self-attention after LSTM\n",
    "        num_attention_heads: Number of heads for multi-head attention\n",
    "        use_layer_norm: Apply layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dropout: float = 0.2,\n",
    "                 num_layers: int = 1, use_bidirectional: bool = True, \n",
    "                 use_self_attention: bool = True,\n",
    "                 num_attention_heads: int = 4, use_layer_norm: bool = True):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        \n",
    "        self.use_bidirectional = use_bidirectional\n",
    "        self.use_self_attention = use_self_attention\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional doubles the effective hidden dimension\n",
    "        self.lstm_output_dim = hidden_dim * 2 if use_bidirectional else hidden_dim\n",
    "        \n",
    "        # --- LSTM Layer(s) ---\n",
    "        # Dropout is applied between LSTM layers (not after the last layer)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=use_bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # --- Optional Layer Norm after LSTM ---\n",
    "        if use_layer_norm:\n",
    "            self.lstm_layer_norm = nn.LayerNorm(self.lstm_output_dim)\n",
    "        \n",
    "        # --- Optional Self-Attention ---\n",
    "        if use_self_attention:\n",
    "            # Ensure hidden dim is divisible by num_heads\n",
    "            if self.lstm_output_dim % num_attention_heads != 0:\n",
    "                # Adjust num_heads to a valid divisor\n",
    "                for n in range(num_attention_heads, 0, -1):\n",
    "                    if self.lstm_output_dim % n == 0:\n",
    "                        num_attention_heads = n\n",
    "                        break\n",
    "            \n",
    "            self.self_attention = MultiHeadSelfAttention(\n",
    "                hidden_dim=self.lstm_output_dim,\n",
    "                num_heads=num_attention_heads,\n",
    "                dropout=dropout,\n",
    "                use_layer_norm=use_layer_norm\n",
    "            )\n",
    "        \n",
    "        # --- Output Head ---\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Final projection layer\n",
    "        if use_layer_norm:\n",
    "            self.pre_head_norm = nn.LayerNorm(self.lstm_output_dim)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.lstm_output_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (Batch, Seq_Len, Features) input tensor\n",
    "            \n",
    "        Returns:\n",
    "            prediction: (Batch, 1) predicted value\n",
    "        \"\"\"\n",
    "        # 1. LSTM Encoding\n",
    "        # out: (Batch, Seq_Len, Hidden) or (Batch, Seq_Len, Hidden*2) if bidirectional\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # 2. Optional Layer Norm on LSTM output\n",
    "        if self.use_layer_norm:\n",
    "            out = self.lstm_layer_norm(out)\n",
    "        \n",
    "        # 3. Optional Self-Attention\n",
    "        if self.use_self_attention:\n",
    "            out = self.self_attention(out)  # (Batch, Seq_Len, Hidden)\n",
    "        \n",
    "        # 4. Take the Last Timestep: (Batch, Hidden)\n",
    "        last_hidden = out[:, -1, :]\n",
    "        \n",
    "        # 5. Dropout + Optional Pre-Head Norm\n",
    "        out = self.dropout(last_hidden)\n",
    "        if self.use_layer_norm:\n",
    "            out = self.pre_head_norm(out)\n",
    "        \n",
    "        # 6. Final Prediction: (Batch, 1)\n",
    "        prediction = self.head(out)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "print(\"LSTMWithAttention model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e0118",
   "metadata": {},
   "source": [
    "#### (Continued) Seq2Seq Model with Cross-Attention (Task 2)\n",
    "\n",
    "Encoder-Decoder with stacked layers, cross-attention, and teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438bd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttention module defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CROSS-ATTENTION MODULE\n",
    "# ==============================================================================\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau-style Additive Attention.\n",
    "    \n",
    "    Computes attention weights between decoder hidden state and encoder outputs,\n",
    "    then returns weighted context vector.\n",
    "    \n",
    "    Note: This module handles the case where decoder hidden dim differs from \n",
    "    encoder output dim (e.g., when encoder is bidirectional).\n",
    "    \n",
    "    Args:\n",
    "        encoder_dim: Dimension of encoder outputs (hidden*2 if bidirectional)\n",
    "        decoder_dim: Dimension of decoder hidden state (optional, defaults to encoder_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim: int, decoder_dim: int = None):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        \n",
    "        if decoder_dim is None:\n",
    "            decoder_dim = encoder_dim\n",
    "        \n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        # Project both to the same space for attention computation\n",
    "        self.attn = nn.Linear(encoder_dim + decoder_dim, encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden: torch.Tensor, encoder_outputs: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden: Decoder state at current step (Batch, Decoder_Dim)\n",
    "            encoder_outputs: All encoder states (Batch, Seq_Len, Encoder_Dim)\n",
    "            \n",
    "        Returns:\n",
    "            context: Weighted sum of encoder outputs (Batch, Encoder_Dim)\n",
    "            weights: Attention weights (Batch, Seq_Len)\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repeat decoder hidden state: (Batch, Seq_Len, Decoder_Dim)\n",
    "        hidden_expanded = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Calculate Energy: score = v * tanh(W * [h_dec; h_enc])\n",
    "        combined = torch.cat((hidden_expanded, encoder_outputs), dim=2)  # (Batch, Seq, Dec+Enc)\n",
    "        energy = torch.tanh(self.attn(combined))  # (Batch, Seq, Encoder_Dim)\n",
    "        attention = self.v(energy).squeeze(2)  # (Batch, Seq_Len)\n",
    "        \n",
    "        # Softmax to get weights\n",
    "        weights = F.softmax(attention, dim=1)\n",
    "        \n",
    "        # Compute Context Vector\n",
    "        # (Batch, 1, Seq_Len) @ (Batch, Seq_Len, Encoder_Dim) -> (Batch, 1, Encoder_Dim)\n",
    "        context = torch.bmm(weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        return context.squeeze(1), weights\n",
    "\n",
    "print(\"CrossAttention module defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1432844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Seq2Seq model class (enhanced with stacked layers) defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ENHANCED LSTM SEQ2SEQ MODEL WITH ATTENTION (Task 2: Multi-Step Prediction)\n",
    "# ==============================================================================\n",
    "\n",
    "class LSTM_Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Encoder-Decoder with Cross-Attention for multi-step prediction.\n",
    "    \n",
    "    Architecture Options:\n",
    "        - Bidirectional Encoder: Captures forward and backward context\n",
    "        - Stacked Encoder Layers: Multiple LSTM layers in encoder\n",
    "        - Stacked Decoder Layers: Multiple LSTMCell layers in decoder\n",
    "        - Encoder Self-Attention: Multi-head attention on encoded sequence\n",
    "        - Layer Normalization: Stabilizes training throughout\n",
    "        - Cross-Attention: Bahdanau-style attention decoder-to-encoder\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input features for Encoder (Past Dyn + Static)\n",
    "        hidden_dim: LSTM hidden units (will be doubled if bidirectional for encoder output)\n",
    "        future_forcing_dim: Number of known future features (weather)\n",
    "        static_dim: Number of static attributes\n",
    "        output_steps: Prediction horizon (default: 5)\n",
    "        num_encoder_layers: Number of stacked LSTM layers in encoder\n",
    "        num_decoder_layers: Number of stacked LSTMCell layers in decoder\n",
    "        use_bidirectional_encoder: Use bidirectional LSTM encoder\n",
    "        use_self_attention: Apply self-attention on encoder outputs\n",
    "        num_attention_heads: Number of heads for encoder self-attention\n",
    "        use_layer_norm: Apply layer normalization\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, future_forcing_dim: int, \n",
    "                 static_dim: int, output_steps: int = 5,\n",
    "                 num_encoder_layers: int = 1,\n",
    "                 num_decoder_layers: int = 1,\n",
    "                 use_bidirectional_encoder: bool = True,\n",
    "                 use_self_attention: bool = True,\n",
    "                 num_attention_heads: int = 4,\n",
    "                 use_layer_norm: bool = True,\n",
    "                 dropout: float = 0.2):\n",
    "        super(LSTM_Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.output_steps = output_steps\n",
    "        self.static_dim = static_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_bidirectional_encoder = use_bidirectional_encoder\n",
    "        self.use_self_attention = use_self_attention\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # Encoder output dimension (doubled if bidirectional)\n",
    "        self.encoder_output_dim = hidden_dim * 2 if use_bidirectional_encoder else hidden_dim\n",
    "        \n",
    "        # --- ENCODER (Stacked LSTM) ---\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_dim, hidden_dim,\n",
    "            num_layers=num_encoder_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=use_bidirectional_encoder,\n",
    "            dropout=dropout if num_encoder_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Optional Layer Norm after encoder\n",
    "        if use_layer_norm:\n",
    "            self.encoder_layer_norm = nn.LayerNorm(self.encoder_output_dim)\n",
    "        \n",
    "        # Optional Self-Attention on encoder outputs\n",
    "        if use_self_attention:\n",
    "            # Adjust num_heads if needed\n",
    "            adjusted_heads = num_attention_heads\n",
    "            if self.encoder_output_dim % num_attention_heads != 0:\n",
    "                for n in range(num_attention_heads, 0, -1):\n",
    "                    if self.encoder_output_dim % n == 0:\n",
    "                        adjusted_heads = n\n",
    "                        break\n",
    "            \n",
    "            self.encoder_self_attention = MultiHeadSelfAttention(\n",
    "                hidden_dim=self.encoder_output_dim,\n",
    "                num_heads=adjusted_heads,\n",
    "                dropout=dropout,\n",
    "                use_layer_norm=use_layer_norm\n",
    "            )\n",
    "        \n",
    "        # --- DECODER COMPONENTS ---\n",
    "        # For multi-layer encoder, we take the last layer's hidden state\n",
    "        if use_bidirectional_encoder:\n",
    "            # Project concatenated forward+backward states to decoder hidden size\n",
    "            self.hidden_projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "            self.cell_projection = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Decoder Input: Previous_Flow (1) + Future_Forcing + Static\n",
    "        decoder_input_dim = 1 + future_forcing_dim + static_dim\n",
    "        \n",
    "        # Stacked Decoder LSTMCells\n",
    "        # First layer takes decoder_input_dim, subsequent layers take hidden_dim\n",
    "        self.decoder_cells = nn.ModuleList()\n",
    "        for layer_idx in range(num_decoder_layers):\n",
    "            input_size = decoder_input_dim if layer_idx == 0 else hidden_dim\n",
    "            self.decoder_cells.append(nn.LSTMCell(input_size, hidden_dim))\n",
    "        \n",
    "        # Dropout between decoder layers (only if > 1 layer)\n",
    "        self.decoder_dropout = nn.Dropout(dropout) if num_decoder_layers > 1 else None\n",
    "        \n",
    "        # Cross-attention: encoder_dim = encoder_output_dim, decoder_dim = hidden_dim\n",
    "        self.attention = CrossAttention(\n",
    "            encoder_dim=self.encoder_output_dim,\n",
    "            decoder_dim=hidden_dim\n",
    "        )\n",
    "        \n",
    "        # Optional decoder layer norm (applied to top layer)\n",
    "        if use_layer_norm:\n",
    "            self.decoder_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Final projection: [Decoder_Hidden + Context] -> Output\n",
    "        # Context comes from encoder (encoder_output_dim), decoder hidden is hidden_dim\n",
    "        self.fc_out = nn.Linear(hidden_dim + self.encoder_output_dim, 1)\n",
    "\n",
    "    def forward(self, x_past: torch.Tensor, x_future_forcing: torch.Tensor, \n",
    "                static_features: torch.Tensor, target_seq: torch.Tensor = None, \n",
    "                teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with optional teacher forcing.\n",
    "        \n",
    "        Supports stacked encoder (num_encoder_layers) and stacked decoder (num_decoder_layers).\n",
    "        \n",
    "        Args:\n",
    "            x_past: (Batch, Past_Seq, Input_Dim) - Past sequence\n",
    "            x_future_forcing: (Batch, output_steps, Future_Forcing_Dim) - Known future weather\n",
    "            static_features: (Batch, Static_Dim) - Static catchment attributes\n",
    "            target_seq: (Batch, output_steps) - Ground truth (for teacher forcing)\n",
    "            teacher_forcing_ratio: Probability of using ground truth\n",
    "            \n",
    "        Returns:\n",
    "            outputs: (Batch, output_steps) - Predicted flow sequence\n",
    "        \"\"\"\n",
    "        batch_size = x_past.size(0)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 1. ENCODE\n",
    "        # =====================================================================\n",
    "        # encoder_outputs: (Batch, Seq, Hidden) or (Batch, Seq, Hidden*2) if bidirectional\n",
    "        # enc_hidden, enc_cell: (num_layers * num_directions, Batch, Hidden)\n",
    "        encoder_outputs, (enc_hidden, enc_cell) = self.encoder(x_past)\n",
    "        \n",
    "        # 2. Optional Layer Norm on encoder outputs\n",
    "        if self.use_layer_norm:\n",
    "            encoder_outputs = self.encoder_layer_norm(encoder_outputs)\n",
    "        \n",
    "        # 3. Optional Self-Attention on encoder outputs\n",
    "        if self.use_self_attention:\n",
    "            encoder_outputs = self.encoder_self_attention(encoder_outputs)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 4. Initialize decoder hidden states from encoder's last layer\n",
    "        # =====================================================================\n",
    "        # For multi-layer encoder, use only the last layer's hidden states\n",
    "        # For bidirectional, last layer indices are: [-2] (forward), [-1] (backward)\n",
    "        if self.use_bidirectional_encoder:\n",
    "            # Get last layer's forward and backward states\n",
    "            # Shape of enc_hidden: (num_layers * 2, Batch, Hidden)\n",
    "            h_forward = enc_hidden[-2]  # (Batch, Hidden)\n",
    "            h_backward = enc_hidden[-1]  # (Batch, Hidden)\n",
    "            c_forward = enc_cell[-2]\n",
    "            c_backward = enc_cell[-1]\n",
    "            \n",
    "            # Concatenate and project to decoder hidden size\n",
    "            hidden_cat = torch.cat((h_forward, h_backward), dim=1)  # (Batch, Hidden*2)\n",
    "            cell_cat = torch.cat((c_forward, c_backward), dim=1)    # (Batch, Hidden*2)\n",
    "            \n",
    "            init_hidden = self.hidden_projection(hidden_cat)  # (Batch, Hidden)\n",
    "            init_cell = self.cell_projection(cell_cat)        # (Batch, Hidden)\n",
    "        else:\n",
    "            # For unidirectional, take last layer: enc_hidden[-1]\n",
    "            init_hidden = enc_hidden[-1]  # (Batch, Hidden)\n",
    "            init_cell = enc_cell[-1]      # (Batch, Hidden)\n",
    "        \n",
    "        # Initialize hidden states for each decoder layer\n",
    "        # All layers start with the same initial state (from encoder)\n",
    "        decoder_hiddens = [init_hidden.clone() for _ in range(self.num_decoder_layers)]\n",
    "        decoder_cells = [init_cell.clone() for _ in range(self.num_decoder_layers)]\n",
    "        \n",
    "        # Initialize outputs container\n",
    "        outputs = torch.zeros(batch_size, self.output_steps).to(x_past.device)\n",
    "        \n",
    "        # First decoder input is the Last Observed Flow (last column of x_past)\n",
    "        decoder_input_flow = x_past[:, -1, -1].unsqueeze(1)  # (Batch, 1)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # 5. DECODE LOOP (t=0 to output_steps-1)\n",
    "        # =====================================================================\n",
    "        for t in range(self.output_steps):\n",
    "            # --- Prepare Decoder Input ---\n",
    "            # Structure: [Flow(t-1), Future_Forcing(t), Static]\n",
    "            current_forcing = x_future_forcing[:, t, :]  # (Batch, Forcing_Dim)\n",
    "            \n",
    "            inputs_list = [decoder_input_flow, current_forcing]\n",
    "            if self.static_dim > 0 and static_features is not None:\n",
    "                inputs_list.append(static_features)\n",
    "            \n",
    "            dec_input = torch.cat(inputs_list, dim=1)  # (Batch, decoder_input_dim)\n",
    "            \n",
    "            # --- Run Stacked Decoder Cells ---\n",
    "            layer_input = dec_input\n",
    "            for layer_idx in range(self.num_decoder_layers):\n",
    "                decoder_hiddens[layer_idx], decoder_cells[layer_idx] = self.decoder_cells[layer_idx](\n",
    "                    layer_input, (decoder_hiddens[layer_idx], decoder_cells[layer_idx])\n",
    "                )\n",
    "                # Apply dropout between layers (not after the last layer)\n",
    "                if self.decoder_dropout is not None and layer_idx < self.num_decoder_layers - 1:\n",
    "                    layer_input = self.decoder_dropout(decoder_hiddens[layer_idx])\n",
    "                else:\n",
    "                    layer_input = decoder_hiddens[layer_idx]\n",
    "            \n",
    "            # Use the top layer's hidden state for attention and prediction\n",
    "            top_hidden = decoder_hiddens[-1]\n",
    "            \n",
    "            # Optional layer norm on decoder hidden\n",
    "            if self.use_layer_norm:\n",
    "                hidden_normed = self.decoder_layer_norm(top_hidden)\n",
    "            else:\n",
    "                hidden_normed = top_hidden\n",
    "            \n",
    "            # --- Cross Attention ---\n",
    "            context, _ = self.attention(hidden_normed, encoder_outputs)\n",
    "            \n",
    "            # --- Dropout ---\n",
    "            hidden_dropped = self.dropout(hidden_normed)\n",
    "            context_dropped = self.dropout(context)\n",
    "            \n",
    "            # --- Predict ---\n",
    "            combined = torch.cat((hidden_dropped, context_dropped), dim=1)\n",
    "            prediction = self.fc_out(combined)  # (Batch, 1)\n",
    "            \n",
    "            # Store prediction\n",
    "            outputs[:, t] = prediction.squeeze(1)\n",
    "            \n",
    "            # --- Teacher Forcing Logic ---\n",
    "            if target_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                decoder_input_flow = target_seq[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input_flow = prediction\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"LSTM_Seq2Seq model class (enhanced with stacked layers) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c9002",
   "metadata": {},
   "source": [
    "### 1.4 Utility Functions (Metrics & Weight Init)\n",
    "\n",
    "Metrics calculation (NSE, RMSE, etc.) and weight initialization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f452acc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def calc_nse(obs: np.ndarray, sim: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Nash-Sutcliffe Efficiency (NSE).\n",
    "    \n",
    "    NSE = 1 - (sum((obs - sim)^2) / sum((obs - mean(obs))^2))\n",
    "    \n",
    "    Args:\n",
    "        obs: Observed values\n",
    "        sim: Simulated/predicted values\n",
    "        \n",
    "    Returns:\n",
    "        NSE value. Perfect prediction = 1.0, mean prediction = 0.0\n",
    "    \"\"\"\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2) + 1e-6\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "\n",
    "def calc_metrics(obs: np.ndarray, sim: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "    mse = np.mean((obs - sim) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(obs - sim))\n",
    "    nse = calc_nse(obs, sim)\n",
    "    \n",
    "    # R-squared\n",
    "    ss_res = np.sum((obs - sim) ** 2)\n",
    "    ss_tot = np.sum((obs - np.mean(obs)) ** 2) + 1e-6\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'NSE': nse,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6829fb9",
   "metadata": {},
   "source": [
    "#### (Continued) Weight Initialization\n",
    "\n",
    "Xavier/Kaiming initialization for LSTM and linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bae54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight initialization utilities defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# WEIGHT INITIALIZATION UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def init_weights(module: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Apply proper weight initialization to a single module.\n",
    "    \n",
    "    Initialization Strategy:\n",
    "        - Linear layers: Xavier uniform (good for tanh/sigmoid activations)\n",
    "        - LSTM layers: Orthogonal for weights, zeros for biases\n",
    "        - LayerNorm: weights=1, bias=0\n",
    "    \n",
    "    Args:\n",
    "        module: A single nn.Module instance\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # Xavier initialization for linear layers\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    elif isinstance(module, (nn.LSTM, nn.LSTMCell)):\n",
    "        # Orthogonal initialization for LSTM weights (prevents vanishing/exploding gradients)\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "                # Set forget gate bias to 1 for better gradient flow\n",
    "                # LSTM has 4 gates: [input, forget, cell, output]\n",
    "                n = param.size(0)\n",
    "                param.data[n // 4: n // 2].fill_(1.0)\n",
    "    \n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def apply_weight_init(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Apply weight initialization to all modules in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The full model to initialize\n",
    "        \n",
    "    Returns:\n",
    "        The initialized model (for chaining)\n",
    "    \"\"\"\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "print(\"Weight initialization utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27da606",
   "metadata": {},
   "source": [
    "### 1.5 Dataset Preparation\n",
    "\n",
    "Data loading and dataset generation functions for training/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0674014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_and_prepare_data function defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADING & PREPARATION FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_prepare_data(cfg: Union[Task1Config, Task2Config]) -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame, \n",
    "                                                                          CamelsPreprocessor, List[str]]:\n",
    "    \"\"\"\n",
    "    Complete data loading and preparation pipeline.\n",
    "    \n",
    "    This function orchestrates:\n",
    "    1. Loading basin list (excluding bad basins)\n",
    "    2. Selecting basins based on cfg.num_basins\n",
    "    3. Loading static attributes\n",
    "    4. Loading and processing dynamic data for all basins\n",
    "    5. Fitting the preprocessor on training data\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task1Config or Task2Config with all settings\n",
    "        \n",
    "    Returns:\n",
    "        dynamic_data: {gauge_id: DataFrame} of processed dynamic data\n",
    "        df_static: DataFrame of static features (or None if use_static=False)\n",
    "        preprocessor: Fitted CamelsPreprocessor\n",
    "        basin_ids: List of selected basin gauge IDs\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LOADING AND PREPARING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Initialize loader and get basin list\n",
    "    loader = CamelsLoader(cfg)\n",
    "    df_basins = loader.get_basin_list()\n",
    "    print(f\"\\nTotal valid basins found: {len(df_basins)}\")\n",
    "    \n",
    "    # 2. Select basins based on num_basins parameter\n",
    "    if cfg.num_basins > 0:\n",
    "        print(f\"Selecting first {cfg.num_basins} basins for experiment...\")\n",
    "        df_basins = df_basins.head(cfg.num_basins)\n",
    "    else:\n",
    "        print(f\"Using ALL {len(df_basins)} basins for full training...\")\n",
    "    \n",
    "    basin_ids = df_basins['gauge_id'].tolist()\n",
    "    print(f\"Selected basins: {basin_ids[:5]}{'...' if len(basin_ids) > 5 else ''}\")\n",
    "    \n",
    "    # 3. Load static attributes\n",
    "    if cfg.use_static:\n",
    "        df_static = loader.load_static_attributes(basin_ids)\n",
    "        print(f\"\\nStatic attributes loaded: {df_static.shape}\")\n",
    "    else:\n",
    "        df_static = None\n",
    "        print(\"\\nStatic features disabled (use_static=False)\")\n",
    "    \n",
    "    # 4. Initialize preprocessor and feature engineer\n",
    "    preprocessor = CamelsPreprocessor(cfg)\n",
    "    engineer = FeatureEngineer(cfg)\n",
    "    \n",
    "    # 5. Load and process all basins\n",
    "    dynamic_data = {}\n",
    "    print(f\"\\nLoading and preprocessing {len(df_basins)} basins...\")\n",
    "    print(\"Processing order: Clean Outliers → Feature Engineering → Interpolation → Date Features\")\n",
    "    \n",
    "    for _, row in tqdm(df_basins.iterrows(), total=len(df_basins), desc=\"Loading basins\"):\n",
    "        gid = row['gauge_id']\n",
    "        region = row['region']\n",
    "        \n",
    "        df = loader.load_dynamic_data(gid, region)\n",
    "        if df is not None:\n",
    "            # Order matters to avoid data leakage:\n",
    "            df = preprocessor.clean_physical_outliers(df)\n",
    "            df = engineer.transform(df)\n",
    "            df = preprocessor.handle_missing_data(df)\n",
    "            df = preprocessor.add_date_features(df)\n",
    "            dynamic_data[gid] = df\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded {len(dynamic_data)} basins\")\n",
    "    \n",
    "    # 6. Fit preprocessor on training data\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FITTING PREPROCESSOR\")\n",
    "    print(\"=\" * 60)\n",
    "    preprocessor.fit(dynamic_data, df_static)\n",
    "    \n",
    "    return dynamic_data, df_static, preprocessor, basin_ids\n",
    "\n",
    "print(\"load_and_prepare_data function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5443a",
   "metadata": {},
   "source": [
    "#### (Continued) Dataset Generation Functions\n",
    "\n",
    "Sequence creation for Task 1 (single-step) and Task 2 (multi-step with future forcing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f838bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATASET GENERATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def get_task1_dataset(cfg: Task1Config, dynamic_data: Dict[str, pd.DataFrame], \n",
    "                      df_static: pd.DataFrame, preprocessor: 'CamelsPreprocessor',\n",
    "                      basin_ids: List[str], split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate dataset for Task 1 (single-step prediction).\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task1Config\n",
    "        dynamic_data: {gauge_id: DataFrame} of processed data\n",
    "        df_static: Static features DataFrame\n",
    "        preprocessor: Fitted preprocessor\n",
    "        basin_ids: List of basin IDs\n",
    "        split: 'train', 'val', or 'test'\n",
    "        \n",
    "    Returns:\n",
    "        X: (N, Seq_Len, Features) input sequences\n",
    "        y: (N,) target values\n",
    "    \"\"\"\n",
    "    # Get date range for split\n",
    "    if split == 'train':\n",
    "        start, end = cfg.train_start, cfg.train_end\n",
    "    elif split == 'val':\n",
    "        start, end = cfg.val_start, cfg.val_end\n",
    "    else:\n",
    "        start, end = cfg.test_start, cfg.test_end\n",
    "    \n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    for gid in basin_ids:\n",
    "        if gid not in dynamic_data:\n",
    "            continue\n",
    "            \n",
    "        df = dynamic_data[gid]\n",
    "        df_slice = df.loc[start:end]\n",
    "        \n",
    "        if len(df_slice) < cfg.seq_length + cfg.predict_horizon:\n",
    "            continue\n",
    "        \n",
    "        # Transform and create sequences\n",
    "        data_matrix, static_norm = preprocessor.transform(df_slice, df_static, gid)\n",
    "        \n",
    "        if cfg.use_static and static_norm is not None:\n",
    "            X_seq, y_seq = preprocessor.create_sequences(data_matrix, static_norm, mode='task1')\n",
    "        else:\n",
    "            X_seq, y_seq = preprocessor.create_sequences(data_matrix, None, mode='task1')\n",
    "        \n",
    "        if len(X_seq) > 0:\n",
    "            X_list.append(X_seq)\n",
    "            y_list.append(y_seq)\n",
    "    \n",
    "    if not X_list:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "\n",
    "def get_task2_dataset(cfg: Task2Config, dynamic_data: Dict[str, pd.DataFrame], \n",
    "                      df_static: pd.DataFrame, preprocessor: 'CamelsPreprocessor',\n",
    "                      basin_ids: List[str], split: str) -> Tuple[np.ndarray, np.ndarray, \n",
    "                                                                   np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate dataset for Task 2 (multi-step prediction with future forcing).\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task2Config\n",
    "        dynamic_data: {gauge_id: DataFrame} of processed data\n",
    "        df_static: Static features DataFrame\n",
    "        preprocessor: Fitted preprocessor\n",
    "        basin_ids: List of basin IDs\n",
    "        split: 'train', 'val', or 'test'\n",
    "        \n",
    "    Returns:\n",
    "        X_past: (N, Seq_Len, Features) past sequences\n",
    "        X_future: (N, 5, Future_Forcing_Dim) future forcing\n",
    "        Static: (N, Static_Dim) static features\n",
    "        y: (N, 5) target sequences\n",
    "    \"\"\"\n",
    "    # Get date range for split\n",
    "    if split == 'train':\n",
    "        start, end = cfg.train_start, cfg.train_end\n",
    "    elif split == 'val':\n",
    "        start, end = cfg.val_start, cfg.val_end\n",
    "    else:\n",
    "        start, end = cfg.test_start, cfg.test_end\n",
    "    \n",
    "    X_past_list, X_future_list, Static_list, Y_list = [], [], [], []\n",
    "    \n",
    "    # Future forcing features (known weather forecasts)\n",
    "    future_forcing_cols = cfg.future_forcing_features\n",
    "    \n",
    "    for gid in basin_ids:\n",
    "        if gid not in dynamic_data:\n",
    "            continue\n",
    "            \n",
    "        df = dynamic_data[gid]\n",
    "        df_slice = df.loc[start:end]\n",
    "        \n",
    "        if len(df_slice) < cfg.seq_length + cfg.predict_steps:\n",
    "            continue\n",
    "        \n",
    "        # Transform\n",
    "        data_matrix, static_norm = preprocessor.transform(df_slice, df_static, gid)\n",
    "        \n",
    "        # Create sequences\n",
    "        if cfg.use_static and static_norm is not None:\n",
    "            X_seq, y_seq = preprocessor.create_sequences(data_matrix, static_norm, mode='task2')\n",
    "        else:\n",
    "            X_seq, y_seq = preprocessor.create_sequences(data_matrix, None, mode='task2')\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract future forcing (from the slice, not normalized data)\n",
    "        # Need to align with the sequences\n",
    "        n_samples = len(X_seq)\n",
    "        future_forcing = []\n",
    "        \n",
    "        # Get normalized future forcing\n",
    "        dyn_cols = [c for c in cfg.dynamic_features if c in df_slice.columns]\n",
    "        forcing_indices = [dyn_cols.index(c) for c in future_forcing_cols if c in dyn_cols]\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Sequence starts at index i, ends at i + seq_length\n",
    "            # Future is from i + seq_length to i + seq_length + predict_steps\n",
    "            future_start = cfg.seq_length + i\n",
    "            future_end = future_start + cfg.predict_steps\n",
    "            \n",
    "            if future_end <= len(data_matrix):\n",
    "                future_feats = data_matrix[future_start:future_end, forcing_indices]\n",
    "                future_forcing.append(future_feats)\n",
    "        \n",
    "        if len(future_forcing) != n_samples:\n",
    "            continue\n",
    "        \n",
    "        X_past_list.append(X_seq)\n",
    "        X_future_list.append(np.array(future_forcing))\n",
    "        Y_list.append(y_seq)\n",
    "        \n",
    "        # Static features for each sample\n",
    "        if static_norm is not None:\n",
    "            Static_list.append(np.tile(static_norm, (n_samples, 1)))\n",
    "        else:\n",
    "            Static_list.append(np.zeros((n_samples, len(cfg.static_features))))\n",
    "    \n",
    "    if not X_past_list:\n",
    "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    return (np.concatenate(X_past_list), np.concatenate(X_future_list),\n",
    "            np.concatenate(Static_list), np.concatenate(Y_list))\n",
    "\n",
    "print(\"Dataset generation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb964449",
   "metadata": {},
   "source": [
    "### 1.6 Training & Evaluation\n",
    "\n",
    "Training loops with early stopping, scheduler, gradient clipping. Evaluation with metrics and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b11f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions with toggleable DL techniques defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TRAINING FUNCTION FOR TASK 1 (with Toggleable DL Techniques)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_task1(cfg: Task1Config, model: nn.Module, \n",
    "                train_loader: DataLoader, val_loader: DataLoader) -> Tuple[nn.Module, Dict[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Train Task 1 LSTM model with early stopping and toggleable advanced techniques.\n",
    "    \n",
    "    Advanced Techniques (controlled by cfg):\n",
    "    - Weight Decay via AdamW (cfg.use_weight_decay)\n",
    "    - Learning Rate Scheduler (cfg.use_scheduler)\n",
    "    - Gradient Clipping (cfg.use_gradient_clipping)\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task1Config with training settings\n",
    "        model: LSTM model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model (with best weights loaded)\n",
    "        history: {'train_loss': [...], 'val_loss': [...]}\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING TASK 1: Single-Step LSTM\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Epochs: {cfg.epochs}, Patience: {cfg.patience}, LR: {cfg.learning_rate}\")\n",
    "    print(f\"Weight Decay: {cfg.use_weight_decay} (λ={cfg.weight_decay})\")\n",
    "    print(f\"LR Scheduler: {cfg.use_scheduler}\")\n",
    "    print(f\"Gradient Clipping: {cfg.use_gradient_clipping} (max_norm={cfg.gradient_clip_value})\")\n",
    "    \n",
    "    # === Optimizer Selection ===\n",
    "    if cfg.use_weight_decay:\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                                lr=cfg.learning_rate, \n",
    "                                weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    \n",
    "    # === Learning Rate Scheduler (optional) ===\n",
    "    scheduler = None\n",
    "    if cfg.use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', \n",
    "            factor=cfg.scheduler_factor, \n",
    "            patience=cfg.scheduler_patience\n",
    "        )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(cfg.device), y_batch.to(cfg.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # === Gradient Clipping (optional) ===\n",
    "            if cfg.use_gradient_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg.gradient_clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(cfg.device), y_batch.to(cfg.device)\n",
    "                predictions = model(X_batch).squeeze()\n",
    "                val_loss += criterion(predictions, y_batch).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        # === Step Scheduler (after validation) ===\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            save_marker = \" *** Best ***\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            save_marker = f\" (patience: {patience_counter}/{cfg.patience})\"\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d}/{cfg.epochs}: Train = {train_loss:.6f} | Val = {val_loss:.6f} | LR = {current_lr:.1e}{save_marker}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= cfg.patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training complete. Best Val Loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAINING FUNCTION FOR TASK 2 (with Toggleable DL Techniques)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_task2(cfg: Task2Config, model: nn.Module,\n",
    "                train_loader: DataLoader, val_loader: DataLoader) -> Tuple[nn.Module, Dict[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Train Task 2 Seq2Seq model with early stopping, teacher forcing decay, \n",
    "    and toggleable advanced techniques.\n",
    "    \n",
    "    Advanced Techniques (controlled by cfg):\n",
    "    - Weight Decay via AdamW (cfg.use_weight_decay)\n",
    "    - Learning Rate Scheduler (cfg.use_scheduler)\n",
    "    - Gradient Clipping (cfg.use_gradient_clipping)\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task2Config with training settings\n",
    "        model: LSTM_Seq2Seq model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model (with best weights loaded)\n",
    "        history: {'train_loss': [...], 'val_loss': [...]}\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING TASK 2: Multi-Step Seq2Seq with Attention\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Epochs: {cfg.epochs}, Patience: {cfg.patience}, LR: {cfg.learning_rate}\")\n",
    "    print(f\"Teacher Forcing: {cfg.teacher_forcing_ratio}, Decay: {cfg.tf_decay}\")\n",
    "    print(f\"Weight Decay: {cfg.use_weight_decay} (λ={cfg.weight_decay})\")\n",
    "    print(f\"LR Scheduler: {cfg.use_scheduler}\")\n",
    "    print(f\"Gradient Clipping: {cfg.use_gradient_clipping} (max_norm={cfg.gradient_clip_value})\")\n",
    "    \n",
    "    # === Optimizer Selection ===\n",
    "    if cfg.use_weight_decay:\n",
    "        optimizer = optim.AdamW(model.parameters(), \n",
    "                                lr=cfg.learning_rate, \n",
    "                                weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "    \n",
    "    # === Learning Rate Scheduler (optional) ===\n",
    "    scheduler = None\n",
    "    if cfg.use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', \n",
    "            factor=cfg.scheduler_factor, \n",
    "            patience=cfg.scheduler_patience\n",
    "        )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(cfg.epochs):\n",
    "        # Teacher forcing ratio (optionally decay)\n",
    "        if cfg.tf_decay:\n",
    "            tf_ratio = cfg.teacher_forcing_ratio * (1 - epoch / cfg.epochs)\n",
    "        else:\n",
    "            tf_ratio = cfg.teacher_forcing_ratio\n",
    "        \n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for x_past, x_future, static, y in train_loader:\n",
    "            x_past = x_past.to(cfg.device)\n",
    "            x_future = x_future.to(cfg.device)\n",
    "            static = static.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x_past, x_future, static, target_seq=y, teacher_forcing_ratio=tf_ratio)\n",
    "            loss = criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # === Gradient Clipping (optional) ===\n",
    "            if cfg.use_gradient_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg.gradient_clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # --- Validation (no teacher forcing) ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_past, x_future, static, y in val_loader:\n",
    "                x_past = x_past.to(cfg.device)\n",
    "                x_future = x_future.to(cfg.device)\n",
    "                static = static.to(cfg.device)\n",
    "                y = y.to(cfg.device)\n",
    "                \n",
    "                predictions = model(x_past, x_future, static, target_seq=None, teacher_forcing_ratio=0)\n",
    "                val_loss += criterion(predictions, y).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # === Step Scheduler (after validation) ===\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            save_marker = \" *** Best ***\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            save_marker = f\" (patience: {patience_counter}/{cfg.patience})\"\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d}/{cfg.epochs}: Train = {train_loss:.6f} | Val = {val_loss:.6f} | TF = {tf_ratio:.2f} | LR = {current_lr:.1e}{save_marker}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= cfg.patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training complete. Best Val Loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Training functions with toggleable DL techniques defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262ef9f",
   "metadata": {},
   "source": [
    "#### (Continued) Evaluation and Visualization\n",
    "\n",
    "Model evaluation on test set with metrics per step and visualization plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0201d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and visualization functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_task1(cfg: Task1Config, model: nn.Module, \n",
    "                   test_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate Task 1 model on test set.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task1Config\n",
    "        model: Trained LSTM model\n",
    "        test_loader: Test data loader\n",
    "        \n",
    "    Returns:\n",
    "        predictions: (N,) predicted values\n",
    "        targets: (N,) ground truth values\n",
    "        metrics: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(cfg.device)\n",
    "            predictions = model(X_batch).squeeze().cpu().numpy()\n",
    "            all_preds.append(predictions)\n",
    "            all_targets.append(y_batch.numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    metrics = calc_metrics(targets, preds)\n",
    "    \n",
    "    return preds, targets, metrics\n",
    "\n",
    "\n",
    "def evaluate_task2(cfg: Task2Config, model: nn.Module,\n",
    "                   test_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray, Dict[str, float], Dict[int, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Evaluate Task 2 model on test set.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task2Config\n",
    "        model: Trained Seq2Seq model\n",
    "        test_loader: Test data loader\n",
    "        \n",
    "    Returns:\n",
    "        predictions: (N, 5) predicted sequences\n",
    "        targets: (N, 5) ground truth sequences\n",
    "        metrics: Dictionary of averaged metrics\n",
    "        metrics_per_step: {1: {...}, 2: {...}, ...} metrics per horizon\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_past, x_future, static, y in test_loader:\n",
    "            x_past = x_past.to(cfg.device)\n",
    "            x_future = x_future.to(cfg.device)\n",
    "            static = static.to(cfg.device)\n",
    "            \n",
    "            predictions = model(x_past, x_future, static, target_seq=None, teacher_forcing_ratio=0)\n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds)\n",
    "    targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Calculate metrics per forecast horizon\n",
    "    metrics_per_step = {}\n",
    "    for step in range(cfg.predict_steps):\n",
    "        metrics_per_step[step + 1] = calc_metrics(targets[:, step], preds[:, step])\n",
    "    \n",
    "    # Average metrics across all steps\n",
    "    avg_metrics = {\n",
    "        'MSE': np.mean([m['MSE'] for m in metrics_per_step.values()]),\n",
    "        'RMSE': np.mean([m['RMSE'] for m in metrics_per_step.values()]),\n",
    "        'MAE': np.mean([m['MAE'] for m in metrics_per_step.values()]),\n",
    "        'NSE': np.mean([m['NSE'] for m in metrics_per_step.values()]),\n",
    "        'R2': np.mean([m['R2'] for m in metrics_per_step.values()])\n",
    "    }\n",
    "    \n",
    "    return preds, targets, avg_metrics, metrics_per_step\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_training_curves(history: Dict[str, List[float]], task_name: str, \n",
    "                         show_plots: bool = True) -> None:\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    if not show_plots:\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.plot(history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax.plot(history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    best_val = min(history['val_loss'])\n",
    "    ax.axhline(y=best_val, color='green', linestyle='--', alpha=0.5, label=f'Best Val: {best_val:.4f}')\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE Loss')\n",
    "    ax.set_title(f'{task_name}: Training & Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_task1_results(targets: np.ndarray, preds: np.ndarray, \n",
    "                       metrics: Dict[str, float], show_plots: bool = True) -> None:\n",
    "    \"\"\"Plot Task 1 evaluation results.\"\"\"\n",
    "    if not show_plots:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax = axes[0]\n",
    "    ax.scatter(targets, preds, alpha=0.3, s=10)\n",
    "    min_val, max_val = min(targets.min(), preds.min()), max(targets.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='1:1 Line')\n",
    "    ax.set_xlabel('Observed (Normalized)')\n",
    "    ax.set_ylabel('Predicted (Normalized)')\n",
    "    ax.set_title(f\"Task 1: Observed vs Predicted\\nNSE={metrics['NSE']:.4f}, R²={metrics['R2']:.4f}\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # Residuals histogram\n",
    "    ax = axes[1]\n",
    "    residuals = preds - targets\n",
    "    ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=0, color='red', linestyle='--')\n",
    "    ax.set_xlabel('Residual (Pred - Obs)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Residual Distribution\\nMean={np.mean(residuals):.4f}, Std={np.std(residuals):.4f}')\n",
    "    \n",
    "    # Time series sample\n",
    "    ax = axes[2]\n",
    "    n_show = min(500, len(targets))\n",
    "    ax.plot(range(n_show), targets[:n_show], 'b-', alpha=0.7, label='Observed', linewidth=1)\n",
    "    ax.plot(range(n_show), preds[:n_show], 'r-', alpha=0.7, label='Predicted', linewidth=1)\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Normalized Streamflow')\n",
    "    ax.set_title('Time Series Sample (First 500)')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_task2_results(targets: np.ndarray, preds: np.ndarray,\n",
    "                       metrics_per_step: Dict[int, Dict[str, float]], \n",
    "                       show_plots: bool = True) -> None:\n",
    "    \"\"\"Plot Task 2 evaluation results.\"\"\"\n",
    "    if not show_plots:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Per-step metrics\n",
    "    ax = axes[0, 0]\n",
    "    steps = list(metrics_per_step.keys())\n",
    "    nse_values = [metrics_per_step[s]['NSE'] for s in steps]\n",
    "    rmse_values = [metrics_per_step[s]['RMSE'] for s in steps]\n",
    "    \n",
    "    x = np.arange(len(steps))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, nse_values, width, label='NSE', color='steelblue')\n",
    "    ax.set_xlabel('Forecast Horizon (t+k)')\n",
    "    ax.set_ylabel('NSE')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f't+{s}' for s in steps])\n",
    "    ax.set_title('NSE by Forecast Horizon')\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.bar(steps, rmse_values, color='coral')\n",
    "    ax.set_xlabel('Forecast Horizon (t+k)')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.set_title('RMSE by Forecast Horizon')\n",
    "    \n",
    "    # Scatter plot (all steps combined)\n",
    "    ax = axes[0, 2]\n",
    "    ax.scatter(targets.flatten(), preds.flatten(), alpha=0.2, s=5)\n",
    "    min_val, max_val = targets.min(), targets.max()\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='1:1 Line')\n",
    "    ax.set_xlabel('Observed (Normalized)')\n",
    "    ax.set_ylabel('Predicted (Normalized)')\n",
    "    ax.set_title('All Steps: Observed vs Predicted')\n",
    "    \n",
    "    # Sample forecast visualization\n",
    "    for i, step in enumerate([1, 3, 5]):\n",
    "        if step <= len(steps):\n",
    "            ax = axes[1, i]\n",
    "            idx = step - 1\n",
    "            n_show = min(300, len(targets))\n",
    "            ax.plot(range(n_show), targets[:n_show, idx], 'b-', alpha=0.7, label='Observed', linewidth=1)\n",
    "            ax.plot(range(n_show), preds[:n_show, idx], 'r-', alpha=0.7, label='Predicted', linewidth=1)\n",
    "            ax.set_xlabel('Sample Index')\n",
    "            ax.set_ylabel('Normalized Flow')\n",
    "            ax.set_title(f't+{step}: NSE={metrics_per_step[step][\"NSE\"]:.3f}')\n",
    "            ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Evaluation and visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519ffa8",
   "metadata": {},
   "source": [
    "### 1.7 Results Saving & Pipeline Orchestrators\n",
    "\n",
    "Functions to save results to disk. Main orchestrators for Task 1 and Task 2 pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ded5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_task1_results and save_task2_results functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# RESULTS SAVING FUNCTIONS (Separate for Task 1 & Task 2)\n",
    "# ==============================================================================\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def save_task1_results(cfg: Task1Config, results: Task1Results) -> None:\n",
    "    \"\"\"\n",
    "    Save Task 1 pipeline results to disk.\n",
    "    \n",
    "    Creates:\n",
    "    - results/task1/models/task1_lstm.pt\n",
    "    - results/task1/metrics.json\n",
    "    - results/task1/predictions.npz\n",
    "    - results/task1/training_history.json\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task1Config\n",
    "        results: Task1Results containing all outputs\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SAVING TASK 1 RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(cfg.results_dir, exist_ok=True)\n",
    "    models_dir = os.path.join(cfg.results_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(results.model.state_dict(), os.path.join(models_dir, 'task1_lstm.pt'))\n",
    "    print(f\"✓ Model saved to {models_dir}/task1_lstm.pt\")\n",
    "    \n",
    "    # Save metrics with comprehensive config\n",
    "    metrics_data = {\n",
    "        'metrics': convert_to_serializable(results.metrics),\n",
    "        'config': {\n",
    "            'num_basins': len(results.basin_ids),\n",
    "            'epochs': cfg.epochs,\n",
    "            'batch_size': cfg.batch_size,\n",
    "            'seq_length': cfg.seq_length,\n",
    "            'predict_horizon': cfg.predict_horizon,\n",
    "            'patience': cfg.patience,\n",
    "            'dropout': cfg.dropout,\n",
    "            'learning_rate': cfg.learning_rate,\n",
    "            'hidden_dim': cfg.hidden_dim,\n",
    "            'bidirectional': cfg.use_bidirectional,\n",
    "            'self_attention': cfg.use_self_attention,\n",
    "            'attention_heads': cfg.num_attention_heads,\n",
    "            'layer_norm': cfg.use_layer_norm,\n",
    "            'weight_decay': cfg.weight_decay if cfg.use_weight_decay else None,\n",
    "            'use_scheduler': cfg.use_scheduler,\n",
    "            'use_gradient_clipping': cfg.use_gradient_clipping\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(cfg.results_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics_data, f, indent=2)\n",
    "    print(f\"✓ Metrics saved to {cfg.results_dir}/metrics.json\")\n",
    "    \n",
    "    # Save predictions\n",
    "    np.savez(os.path.join(cfg.results_dir, 'predictions.npz'),\n",
    "             predictions=results.predictions,\n",
    "             targets=results.targets)\n",
    "    print(f\"✓ Predictions saved to {cfg.results_dir}/predictions.npz\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_data = convert_to_serializable(results.history)\n",
    "    with open(os.path.join(cfg.results_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history_data, f, indent=2)\n",
    "    print(f\"✓ Training history saved\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def save_task2_results(cfg: Task2Config, results: Task2Results) -> None:\n",
    "    \"\"\"\n",
    "    Save Task 2 pipeline results to disk.\n",
    "    \n",
    "    Creates:\n",
    "    - results/task2/models/task2_seq2seq.pt\n",
    "    - results/task2/metrics.json\n",
    "    - results/task2/predictions.npz\n",
    "    - results/task2/training_history.json\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task2Config\n",
    "        results: Task2Results containing all outputs\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SAVING TASK 2 RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(cfg.results_dir, exist_ok=True)\n",
    "    models_dir = os.path.join(cfg.results_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(results.model.state_dict(), os.path.join(models_dir, 'task2_seq2seq.pt'))\n",
    "    print(f\"✓ Model saved to {models_dir}/task2_seq2seq.pt\")\n",
    "    \n",
    "    # Save metrics with comprehensive config\n",
    "    metrics_data = {\n",
    "        'metrics_average': convert_to_serializable(results.metrics),\n",
    "        'metrics_per_step': {str(k): convert_to_serializable(v) for k, v in results.metrics_per_step.items()},\n",
    "        'config': {\n",
    "            'num_basins': len(results.basin_ids),\n",
    "            'epochs': cfg.epochs,\n",
    "            'batch_size': cfg.batch_size,\n",
    "            'seq_length': cfg.seq_length,\n",
    "            'predict_steps': cfg.predict_steps,\n",
    "            'patience': cfg.patience,\n",
    "            'dropout': cfg.dropout,\n",
    "            'learning_rate': cfg.learning_rate,\n",
    "            'hidden_dim': cfg.hidden_dim,\n",
    "            'bidirectional_encoder': cfg.use_bidirectional_encoder,\n",
    "            'self_attention': cfg.use_self_attention,\n",
    "            'attention_heads': cfg.num_attention_heads,\n",
    "            'layer_norm': cfg.use_layer_norm,\n",
    "            'teacher_forcing_ratio': cfg.teacher_forcing_ratio,\n",
    "            'tf_decay': cfg.tf_decay,\n",
    "            'weight_decay': cfg.weight_decay if cfg.use_weight_decay else None,\n",
    "            'use_scheduler': cfg.use_scheduler,\n",
    "            'use_gradient_clipping': cfg.use_gradient_clipping\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(cfg.results_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics_data, f, indent=2)\n",
    "    print(f\"✓ Metrics saved to {cfg.results_dir}/metrics.json\")\n",
    "    \n",
    "    # Save predictions\n",
    "    np.savez(os.path.join(cfg.results_dir, 'predictions.npz'),\n",
    "             predictions=results.predictions,\n",
    "             targets=results.targets)\n",
    "    print(f\"✓ Predictions saved to {cfg.results_dir}/predictions.npz\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_data = convert_to_serializable(results.history)\n",
    "    with open(os.path.join(cfg.results_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history_data, f, indent=2)\n",
    "    print(f\"✓ Training history saved\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"save_task1_results and save_task2_results functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9e50d",
   "metadata": {},
   "source": [
    "#### (Continued) Pipeline Orchestrators\n",
    "\n",
    "Separate orchestrators for running Task 1 and Task 2 pipelines independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f91b1311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_task1_pipeline and run_task2_pipeline orchestrators defined.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TASK 1 PIPELINE ORCHESTRATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def run_task1_pipeline(cfg: Task1Config) -> Task1Results:\n",
    "    \"\"\"\n",
    "    Run the complete Task 1 pipeline: Single-step LSTM prediction (t+2).\n",
    "    \n",
    "    This is the main entry point for Task 1 that orchestrates:\n",
    "    1. Data loading and preparation\n",
    "    2. Dataset generation (train/val/test)\n",
    "    3. Model building (with weight initialization)\n",
    "    4. Training (with early stopping and DL techniques)\n",
    "    5. Evaluation and visualization\n",
    "    6. Results saving\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task1Config with all settings\n",
    "        \n",
    "    Returns:\n",
    "        Task1Results containing model, metrics, predictions, etc.\n",
    "    \"\"\"\n",
    "    # Print configuration\n",
    "    cfg.print_config()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Load and Prepare Data\n",
    "    # =========================================================================\n",
    "    dynamic_data, df_static, preprocessor, basin_ids = load_and_prepare_data(cfg)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Generate Datasets\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING TASK 1 DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nTask 1: Single-step prediction (t+{cfg.predict_horizon})\")\n",
    "    X_train, y_train = get_task1_dataset(cfg, dynamic_data, df_static, preprocessor, basin_ids, 'train')\n",
    "    X_val, y_val = get_task1_dataset(cfg, dynamic_data, df_static, preprocessor, basin_ids, 'val')\n",
    "    X_test, y_test = get_task1_dataset(cfg, dynamic_data, df_static, preprocessor, basin_ids, 'test')\n",
    "    print(f\"  Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Create DataLoaders\n",
    "    # =========================================================================\n",
    "    train_ds = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "    val_ds = TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val))\n",
    "    test_ds = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nDataLoaders created: {len(train_loader)} training batches\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Build Model (with Weight Initialization)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BUILDING TASK 1 MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    input_dim = X_train.shape[2]\n",
    "    model = LSTMWithAttention(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        dropout=cfg.dropout,\n",
    "        num_layers=cfg.num_lstm_layers,\n",
    "        use_bidirectional=cfg.use_bidirectional,\n",
    "        use_self_attention=cfg.use_self_attention,\n",
    "        num_attention_heads=cfg.num_attention_heads,\n",
    "        use_layer_norm=cfg.use_layer_norm\n",
    "    ).to(cfg.device)\n",
    "    \n",
    "    # Apply weight initialization\n",
    "    apply_weight_init(model)\n",
    "    \n",
    "    print(f\"\\nLSTMWithAttention:\")\n",
    "    print(f\"  Input: {input_dim}, Hidden: {cfg.hidden_dim}\")\n",
    "    print(f\"  LSTM Layers: {cfg.num_lstm_layers}\")\n",
    "    print(f\"  Bidirectional: {cfg.use_bidirectional}, Self-Attention: {cfg.use_self_attention}\")\n",
    "    print(f\"  LayerNorm: {cfg.use_layer_norm}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Train Model\n",
    "    # =========================================================================\n",
    "    print(\"\\n\")\n",
    "    model, history = train_task1(cfg, model, train_loader, val_loader)\n",
    "    plot_training_curves(history, \"Task 1\", cfg.show_plots)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: Evaluate Model\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATING TASK 1 MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    preds, targets, metrics = evaluate_task1(cfg, model, test_loader)\n",
    "    print(\"\\nTask 1 Test Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    plot_task1_results(targets, preds, metrics, cfg.show_plots)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: Create Results Object\n",
    "    # =========================================================================\n",
    "    results = Task1Results(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        metrics=metrics,\n",
    "        predictions=preds,\n",
    "        targets=targets,\n",
    "        basin_ids=basin_ids,\n",
    "        preprocessor=preprocessor\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 8: Save Results\n",
    "    # =========================================================================\n",
    "    save_task1_results(cfg, results)\n",
    "    \n",
    "    # Final summary\n",
    "    results.summary()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TASK 2 PIPELINE ORCHESTRATOR\n",
    "# ==============================================================================\n",
    "\n",
    "def run_task2_pipeline(cfg: Task2Config) -> Task2Results:\n",
    "    \"\"\"\n",
    "    Run the complete Task 2 pipeline: Multi-step Seq2Seq prediction (t+1 to t+5).\n",
    "    \n",
    "    This is the main entry point for Task 2 that orchestrates:\n",
    "    1. Data loading and preparation\n",
    "    2. Dataset generation (train/val/test) with future forcing\n",
    "    3. Model building (with weight initialization)\n",
    "    4. Training (with teacher forcing, early stopping, and DL techniques)\n",
    "    5. Evaluation and visualization\n",
    "    6. Results saving\n",
    "    \n",
    "    Args:\n",
    "        cfg: Task2Config with all settings\n",
    "        \n",
    "    Returns:\n",
    "        Task2Results containing model, metrics, predictions, etc.\n",
    "    \"\"\"\n",
    "    # Print configuration\n",
    "    cfg.print_config()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Load and Prepare Data\n",
    "    # =========================================================================\n",
    "    dynamic_data, df_static, preprocessor, basin_ids = load_and_prepare_data(cfg)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Generate Datasets\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING TASK 2 DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nTask 2: Multi-step prediction (t+1 to t+{cfg.predict_steps})\")\n",
    "    X_past_train, X_future_train, static_train, y_train = get_task2_dataset(\n",
    "        cfg, dynamic_data, df_static, preprocessor, basin_ids, 'train')\n",
    "    X_past_val, X_future_val, static_val, y_val = get_task2_dataset(\n",
    "        cfg, dynamic_data, df_static, preprocessor, basin_ids, 'val')\n",
    "    X_past_test, X_future_test, static_test, y_test = get_task2_dataset(\n",
    "        cfg, dynamic_data, df_static, preprocessor, basin_ids, 'test')\n",
    "    print(f\"  Train: X_past={X_past_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"  Val: X_past={X_past_val.shape}, Test: X_past={X_past_test.shape}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Create DataLoaders\n",
    "    # =========================================================================\n",
    "    train_ds = TensorDataset(\n",
    "        torch.Tensor(X_past_train), torch.Tensor(X_future_train),\n",
    "        torch.Tensor(static_train), torch.Tensor(y_train))\n",
    "    val_ds = TensorDataset(\n",
    "        torch.Tensor(X_past_val), torch.Tensor(X_future_val),\n",
    "        torch.Tensor(static_val), torch.Tensor(y_val))\n",
    "    test_ds = TensorDataset(\n",
    "        torch.Tensor(X_past_test), torch.Tensor(X_future_test),\n",
    "        torch.Tensor(static_test), torch.Tensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nDataLoaders created: {len(train_loader)} training batches\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Build Model (with Weight Initialization)\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BUILDING TASK 2 MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    input_dim = X_past_train.shape[2]\n",
    "    future_dim = X_future_train.shape[2]\n",
    "    static_dim = static_train.shape[1]\n",
    "    \n",
    "    model = LSTM_Seq2Seq(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        future_forcing_dim=future_dim,\n",
    "        static_dim=static_dim,\n",
    "        output_steps=cfg.predict_steps,\n",
    "        num_encoder_layers=cfg.num_encoder_layers,\n",
    "        num_decoder_layers=cfg.num_decoder_layers,\n",
    "        use_bidirectional_encoder=cfg.use_bidirectional_encoder,\n",
    "        use_self_attention=cfg.use_self_attention,\n",
    "        num_attention_heads=cfg.num_attention_heads,\n",
    "        use_layer_norm=cfg.use_layer_norm,\n",
    "        dropout=cfg.dropout\n",
    "    ).to(cfg.device)\n",
    "    \n",
    "    # Apply weight initialization\n",
    "    apply_weight_init(model)\n",
    "    \n",
    "    print(f\"\\nLSTM_Seq2Seq:\")\n",
    "    print(f\"  Input: {input_dim}, Hidden: {cfg.hidden_dim}\")\n",
    "    print(f\"  Encoder Layers: {cfg.num_encoder_layers}, Decoder Layers: {cfg.num_decoder_layers}\")\n",
    "    print(f\"  Future Forcing: {future_dim}, Static: {static_dim}\")\n",
    "    print(f\"  Bidirectional Encoder: {cfg.use_bidirectional_encoder}\")\n",
    "    print(f\"  Encoder Self-Attention: {cfg.use_self_attention}\")\n",
    "    print(f\"  LayerNorm: {cfg.use_layer_norm}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: Train Model\n",
    "    # =========================================================================\n",
    "    print(\"\\n\")\n",
    "    model, history = train_task2(cfg, model, train_loader, val_loader)\n",
    "    plot_training_curves(history, \"Task 2\", cfg.show_plots)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: Evaluate Model\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATING TASK 2 MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    preds, targets, metrics, metrics_per_step = evaluate_task2(cfg, model, test_loader)\n",
    "    print(\"\\nTask 2 Test Metrics (averaged):\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    plot_task2_results(targets, preds, metrics_per_step, cfg.show_plots)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 7: Create Results Object\n",
    "    # =========================================================================\n",
    "    results = Task2Results(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        metrics=metrics,\n",
    "        metrics_per_step=metrics_per_step,\n",
    "        predictions=preds,\n",
    "        targets=targets,\n",
    "        basin_ids=basin_ids,\n",
    "        preprocessor=preprocessor\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 8: Save Results\n",
    "    # =========================================================================\n",
    "    save_task2_results(cfg, results)\n",
    "    \n",
    "    # Final summary\n",
    "    results.summary()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"run_task1_pipeline and run_task2_pipeline orchestrators defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268471e",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2A: Run Task 1 Pipeline\n",
    "\n",
    "**Task 1: Single-step LSTM prediction (t+2)**\n",
    "\n",
    "This driver cell runs the complete Task 1 pipeline independently. Modify `Task1Config` parameters below and execute.\n",
    "\n",
    "Model Architecture Options:\n",
    "- **Bidirectional LSTM**: Captures forward and backward temporal context\n",
    "- **Multi-Head Self-Attention**: Learns global dependencies across the sequence  \n",
    "- **Layer Normalization**: Stabilizes training and improves convergence\n",
    "\n",
    "Advanced Training Techniques (Toggleable):\n",
    "- **Weight Decay (AdamW)**: L2 regularization via `use_weight_decay`\n",
    "- **LR Scheduler (ReduceLROnPlateau)**: Adaptive learning rate via `use_scheduler`\n",
    "- **Gradient Clipping**: Prevents exploding gradients via `use_gradient_clipping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eff497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any previous model from GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d17d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TASK 1 CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "--- Experiment Settings ---\n",
      "Number of Basins: 150 (limited)\n",
      "Use Static Features: True\n",
      "\n",
      "--- Training Hyperparameters ---\n",
      "Epochs: 30, Batch Size: 1024\n",
      "Learning Rate: 0.001, Dropout: 0.2\n",
      "Early Stopping Patience: 10\n",
      "\n",
      "--- Model Architecture ---\n",
      "LSTM Layers: 1\n",
      "Bidirectional LSTM: False\n",
      "Self-Attention: False\n",
      "Layer Normalization: False\n",
      "\n",
      "--- Advanced Training Techniques ---\n",
      "Weight Decay (AdamW): True (λ=0.01)\n",
      "LR Scheduler: True (patience=3, factor=0.95)\n",
      "Gradient Clipping: False\n",
      "\n",
      "--- Device ---\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "\n",
      "--- Sequence Parameters ---\n",
      "Sequence Length: 60 days\n",
      "Prediction Horizon: t+2\n",
      "============================================================\n",
      "============================================================\n",
      "LOADING AND PREPARING DATA\n",
      "============================================================\n",
      "\n",
      "Total valid basins found: 608\n",
      "Selecting first 150 basins for experiment...\n",
      "Selected basins: ['01013500', '01022500', '01030500', '01031500', '01047000']...\n",
      "\n",
      "Static attributes loaded: (150, 9)\n",
      "\n",
      "Loading and preprocessing 150 basins...\n",
      "Processing order: Clean Outliers → Feature Engineering → Interpolation → Date Features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c46b6333b9b4bb399135bdcad4313d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading basins:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 150 basins\n",
      "\n",
      "============================================================\n",
      "FITTING PREPROCESSOR\n",
      "============================================================\n",
      "Computing global statistics from training data...\n",
      "Computing basin-specific target statistics...\n",
      "\n",
      "============================================================\n",
      "GENERATING TASK 1 DATASETS\n",
      "============================================================\n",
      "\n",
      "Task 1: Single-step prediction (t+2)\n",
      "  Train: (790409, 60, 22), Val: (264900, 60, 22), Test: (538650, 60, 22)\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURE TASK 1 EXPERIMENT ---\n",
    "cfg1 = Task1Config(\n",
    "    # === EXPERIMENT SETTINGS ===\n",
    "    num_basins=150,              # 0 = ALL basins, >0 = limit for quick testing\n",
    "    use_static=True,            # Include static catchment attributes\n",
    "    \n",
    "    # === TRAINING HYPERPARAMETERS ===\n",
    "    epochs=30,\n",
    "    batch_size=1024,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.2,\n",
    "    patience=10,\n",
    "    \n",
    "    # === MODEL ARCHITECTURE ===\n",
    "    hidden_dim=64,\n",
    "    num_lstm_layers = 1,\n",
    "    use_bidirectional=False,           # Bidirectional LSTM\n",
    "    use_self_attention=False,          # Multi-head self-attention\n",
    "    num_attention_heads=4,\n",
    "    use_layer_norm=False,              # Layer normalization\n",
    "    \n",
    "    # === ADVANCED TRAINING TECHNIQUES (TOGGLEABLE) ===\n",
    "    use_weight_decay=True,            # Use AdamW with weight decay\n",
    "    weight_decay=0.01,                # L2 regularization strength\n",
    "    use_scheduler=True,               # Use ReduceLROnPlateau\n",
    "    scheduler_patience=3,\n",
    "    scheduler_factor=0.95,\n",
    "    use_gradient_clipping=False,       # Clip gradients\n",
    "    gradient_clip_value=1.0,\n",
    "    \n",
    "    # === VISUALIZATION ===\n",
    "    show_plots=True,\n",
    "    \n",
    "    # === PATHS ===\n",
    "    results_dir='./results/task1'\n",
    ")\n",
    "\n",
    "# --- RUN TASK 1 PIPELINE ---\n",
    "results_task1 = run_task1_pipeline(cfg1)\n",
    "\n",
    "# ==============================================================================\n",
    "# ACCESS TASK 1 RESULTS\n",
    "# ==============================================================================\n",
    "# results_task1.model           # Trained LSTMWithAttention model\n",
    "# results_task1.metrics         # Test metrics: {'MSE', 'RMSE', 'MAE', 'NSE', 'R2'}\n",
    "# results_task1.predictions     # Predictions array\n",
    "# results_task1.targets         # Ground truth array\n",
    "# results_task1.history         # Training history: {'train_loss', 'val_loss'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f58b44",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2B: Run Task 2 Pipeline\n",
    "\n",
    "**Task 2: Multi-step Seq2Seq prediction (t+1 to t+5)**\n",
    "\n",
    "This driver cell runs the complete Task 2 pipeline independently. Modify `Task2Config` parameters below and execute.\n",
    "\n",
    "Model Architecture Options:\n",
    "- **Bidirectional Encoder**: Captures forward and backward context in encoder\n",
    "- **Multi-Head Self-Attention**: Encoder self-attention for global dependencies\n",
    "- **Cross-Attention**: Decoder attends to encoder hidden states\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "- **Teacher Forcing**: With optional decay over epochs\n",
    "\n",
    "Advanced Training Techniques (Toggleable):\n",
    "- **Weight Decay (AdamW)**: L2 regularization via `use_weight_decay`\n",
    "- **LR Scheduler (ReduceLROnPlateau)**: Adaptive learning rate via `use_scheduler`\n",
    "- **Gradient Clipping**: Prevents exploding gradients via `use_gradient_clipping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d66fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 2 CONFIGURATION\n",
      "TASK 2 CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "--- Experiment Settings ---\n",
      "Number of Basins: 10 (limited)\n",
      "Use Static Features: True\n",
      "\n",
      "--- Training Hyperparameters ---\n",
      "Epochs: 100, Batch Size: 256\n",
      "Learning Rate: 0.001, Dropout: 0.2\n",
      "Early Stopping Patience: 15\n",
      "Encoder Layers: 1, Decoder Layers: 1\n",
      "Bidirectional Encoder: True\n",
      "Encoder Self-Attention: True (Heads: 4)\n",
      "Layer Normalization: True\n",
      "Teacher Forcing: 0.5 (Decay: True)\n",
      "\n",
      "--- Advanced Training Techniques ---\n",
      "Weight Decay (AdamW): True (λ=0.01)\n",
      "LR Scheduler: True (patience=5, factor=0.5)\n",
      "Gradient Clipping: True (max_norm=1.0)\n",
      "\n",
      "--- Device ---\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "\n",
      "--- Sequence Parameters ---\n",
      "Sequence Length: 60 days\n",
      "Prediction Steps: 5 (t+1 to t+5)\n",
      "============================================================\n",
      "============================================================\n",
      "============================================================\n",
      "LOADING AND PREPARING DATA\n",
      "============================================================\n",
      "\n",
      "Total valid basins found: 608\n",
      "Selecting first 10 basins for experiment...\n",
      "Selected basins: ['01013500', '01022500', '01030500', '01031500', '01047000']...\n",
      "\n",
      "Static attributes loaded: (10, 9)\n",
      "\n",
      "Loading and preprocessing 10 basins...\n",
      "Processing order: Clean Outliers → Feature Engineering → Interpolation → Date Features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf1d07785824e84a33c117e24df4e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading basins:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 10 basins\n",
      "\n",
      "============================================================\n",
      "FITTING PREPROCESSOR\n",
      "============================================================\n",
      "Computing global statistics from training data...\n",
      "Computing basin-specific target statistics...\n",
      "\n",
      "============================================================\n",
      "GENERATING TASK 2 DATASETS\n",
      "============================================================\n",
      "\n",
      "Task 2: Multi-step prediction (t+1 to t+5)\n",
      "  Train: X_past=(54140, 60, 22), y=(54140, 5)\n",
      "  Val: X_past=(17630, 60, 22), Test: X_past=(35880, 60, 22)\n",
      "\n",
      "DataLoaders created: 212 training batches\n",
      "\n",
      "============================================================\n",
      "BUILDING TASK 2 MODEL\n",
      "============================================================\n",
      "\n",
      "LSTM_Seq2Seq:\n",
      "  Input: 22, Hidden: 64\n",
      "  Encoder Layers: 1, Decoder Layers: 1\n",
      "  Future Forcing: 5, Static: 9\n",
      "  Bidirectional Encoder: True\n",
      "  Encoder Self-Attention: True\n",
      "  LayerNorm: True\n",
      "  Parameters: 174,017\n",
      "\n",
      "\n",
      "============================================================\n",
      "TRAINING TASK 2: Multi-Step Seq2Seq with Attention\n",
      "============================================================\n",
      "Epochs: 100, Patience: 15, LR: 0.001\n",
      "Teacher Forcing: 0.5, Decay: True\n",
      "Weight Decay: True (λ=0.01)\n",
      "LR Scheduler: True\n",
      "Gradient Clipping: True (max_norm=1.0)\n",
      "  Train: X_past=(54140, 60, 22), y=(54140, 5)\n",
      "  Val: X_past=(17630, 60, 22), Test: X_past=(35880, 60, 22)\n",
      "\n",
      "DataLoaders created: 212 training batches\n",
      "\n",
      "============================================================\n",
      "BUILDING TASK 2 MODEL\n",
      "============================================================\n",
      "\n",
      "LSTM_Seq2Seq:\n",
      "  Input: 22, Hidden: 64\n",
      "  Encoder Layers: 1, Decoder Layers: 1\n",
      "  Future Forcing: 5, Static: 9\n",
      "  Bidirectional Encoder: True\n",
      "  Encoder Self-Attention: True\n",
      "  LayerNorm: True\n",
      "  Parameters: 174,017\n",
      "\n",
      "\n",
      "============================================================\n",
      "TRAINING TASK 2: Multi-Step Seq2Seq with Attention\n",
      "============================================================\n",
      "Epochs: 100, Patience: 15, LR: 0.001\n",
      "Teacher Forcing: 0.5, Decay: True\n",
      "Weight Decay: True (λ=0.01)\n",
      "LR Scheduler: True\n",
      "Gradient Clipping: True (max_norm=1.0)\n",
      "Epoch 01/100: Train = 0.615029 | Val = 0.464791 | TF = 0.50 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 01/100: Train = 0.615029 | Val = 0.464791 | TF = 0.50 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 02/100: Train = 0.228405 | Val = 0.340846 | TF = 0.49 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 02/100: Train = 0.228405 | Val = 0.340846 | TF = 0.49 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 03/100: Train = 0.155906 | Val = 0.314548 | TF = 0.49 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 03/100: Train = 0.155906 | Val = 0.314548 | TF = 0.49 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 04/100: Train = 0.127255 | Val = 0.292062 | TF = 0.48 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 04/100: Train = 0.127255 | Val = 0.292062 | TF = 0.48 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 05/100: Train = 0.106986 | Val = 0.300098 | TF = 0.48 | LR = 1.0e-03 (patience: 1/15)\n",
      "Epoch 05/100: Train = 0.106986 | Val = 0.300098 | TF = 0.48 | LR = 1.0e-03 (patience: 1/15)\n",
      "Epoch 06/100: Train = 0.092996 | Val = 0.299918 | TF = 0.47 | LR = 1.0e-03 (patience: 2/15)\n",
      "Epoch 06/100: Train = 0.092996 | Val = 0.299918 | TF = 0.47 | LR = 1.0e-03 (patience: 2/15)\n",
      "Epoch 07/100: Train = 0.083007 | Val = 0.281572 | TF = 0.47 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 07/100: Train = 0.083007 | Val = 0.281572 | TF = 0.47 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 08/100: Train = 0.074454 | Val = 0.277133 | TF = 0.46 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 08/100: Train = 0.074454 | Val = 0.277133 | TF = 0.46 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 09/100: Train = 0.071861 | Val = 0.272571 | TF = 0.46 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 09/100: Train = 0.071861 | Val = 0.272571 | TF = 0.46 | LR = 1.0e-03 *** Best ***\n",
      "Epoch 10/100: Train = 0.065887 | Val = 0.289020 | TF = 0.46 | LR = 1.0e-03 (patience: 1/15)\n",
      "Epoch 10/100: Train = 0.065887 | Val = 0.289020 | TF = 0.46 | LR = 1.0e-03 (patience: 1/15)\n",
      "Epoch 11/100: Train = 0.061527 | Val = 0.276613 | TF = 0.45 | LR = 1.0e-03 (patience: 2/15)\n",
      "Epoch 11/100: Train = 0.061527 | Val = 0.276613 | TF = 0.45 | LR = 1.0e-03 (patience: 2/15)\n",
      "Epoch 12/100: Train = 0.057711 | Val = 0.277854 | TF = 0.45 | LR = 1.0e-03 (patience: 3/15)\n",
      "Epoch 12/100: Train = 0.057711 | Val = 0.277854 | TF = 0.45 | LR = 1.0e-03 (patience: 3/15)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     10\u001b[39m cfg2 = Task2Config(\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# === EXPERIMENT SETTINGS ===\u001b[39;00m\n\u001b[32m     12\u001b[39m     num_basins=\u001b[32m10\u001b[39m,              \u001b[38;5;66;03m# 0 = ALL basins, >0 = limit for quick testing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     results_dir=\u001b[33m'\u001b[39m\u001b[33m./results/task2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# --- RUN TASK 2 PIPELINE ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m results_task2 = \u001b[43mrun_task2_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# ACCESS TASK 2 RESULTS\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# results_task2.targets         # Ground truth array (N, 5)\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# results_task2.history         # Training history: {'train_loss', 'val_loss'}\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mrun_task2_pipeline\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# STEP 5: Train Model\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m model, history = \u001b[43mtrain_task2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m plot_training_curves(history, \u001b[33m\"\u001b[39m\u001b[33mTask 2\u001b[39m\u001b[33m\"\u001b[39m, cfg.show_plots)\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# STEP 6: Evaluate Model\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mtrain_task2\u001b[39m\u001b[34m(cfg, model, train_loader, val_loader)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x_past, x_future, static, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m    195\u001b[39m     x_past = x_past.to(cfg.device)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     x_future = \u001b[43mx_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     static = static.to(cfg.device)\n\u001b[32m    198\u001b[39m     y = y.to(cfg.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- CONFIGURE TASK 2 EXPERIMENT ---\n",
    "cfg2 = Task2Config(\n",
    "    # === EXPERIMENT SETTINGS ===\n",
    "    num_basins=10,              # 0 = ALL basins, >0 = limit for quick testing\n",
    "    use_static=True,            # Include static catchment attributes\n",
    "    \n",
    "    # === TRAINING HYPERPARAMETERS ===\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.2,\n",
    "    patience=15,\n",
    "    \n",
    "    # === MODEL ARCHITECTURE ===\n",
    "    hidden_dim=64,\n",
    "    use_bidirectional_encoder=True,   # Bidirectional encoder\n",
    "    use_self_attention=True,          # Encoder self-attention\n",
    "    num_attention_heads=4,\n",
    "    use_layer_norm=True,              # Layer normalization\n",
    "    num_encoder_layers = 2,\n",
    "    num_decoder_layers = 2,\n",
    "\n",
    "    \n",
    "    # === TEACHER FORCING ===\n",
    "    teacher_forcing_ratio=0.9,        # Initial TF probability\n",
    "    tf_decay=True,                    # Decay TF over epochs\n",
    "    \n",
    "    # === ADVANCED TRAINING TECHNIQUES (TOGGLEABLE) ===\n",
    "    use_weight_decay=True,            # Use AdamW with weight decay\n",
    "    weight_decay=0.01,                # L2 regularization strength\n",
    "    use_scheduler=True,               # Use ReduceLROnPlateau\n",
    "    scheduler_patience=5,\n",
    "    scheduler_factor=0.5,\n",
    "    use_gradient_clipping=True,       # Clip gradients\n",
    "    gradient_clip_value=1.0,\n",
    "    \n",
    "    # === VISUALIZATION ===\n",
    "    show_plots=True,\n",
    "    \n",
    "    # === PATHS ===\n",
    "    results_dir='./results/task2'\n",
    ")\n",
    "\n",
    "# --- RUN TASK 2 PIPELINE ---\n",
    "# results_task2 = run_task2_pipeline(cfg2)\n",
    "\n",
    "# ==============================================================================\n",
    "# ACCESS TASK 2 RESULTS\n",
    "# ==============================================================================\n",
    "# results_task2.model           # Trained LSTM_Seq2Seq model\n",
    "# results_task2.metrics         # Averaged test metrics across all steps\n",
    "# results_task2.metrics_per_step  # Per-step metrics: {1: {...}, 2: {...}, ...}\n",
    "# results_task2.predictions     # Predictions array (N, 5)\n",
    "# results_task2.targets         # Ground truth array (N, 5)\n",
    "# results_task2.history         # Training history: {'train_loss', 'val_loss'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
