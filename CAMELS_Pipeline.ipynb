{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4008c0d7",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Plot settings\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d00d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# All constants and hyperparameters in one place for easy modification\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the CAMELS prediction pipeline.\"\"\"\n",
    "    \n",
    "    # === PATHS ===\n",
    "    BASE_DIR = '../basin_dataset_public'\n",
    "    FORCING_DIR = os.path.join(BASE_DIR, 'basin_mean_forcing', 'nldas')\n",
    "    FLOW_DIR = os.path.join(BASE_DIR, 'usgs_streamflow')\n",
    "    BAD_BASINS_FILE = os.path.join(BASE_DIR, 'basin_size_errors_10_percent.txt')\n",
    "    RESULTS_DIR = 'results'\n",
    "    \n",
    "    # Static attribute files\n",
    "    STATIC_FILES = [\n",
    "        'camels_topo.txt',   # Elevation, Slope, Area\n",
    "        'camels_soil.txt',   # Sand, Clay, Porosity\n",
    "        'camels_clim.txt',   # Precip, Aridity, Snow Fraction\n",
    "        'camels_vege.txt',   # Forest fraction, LAI\n",
    "        'camels_geol.txt',   # Rock types, Permeability\n",
    "    ]\n",
    "    \n",
    "    # === CONSTANTS ===\n",
    "    CFS_TO_CMS = 0.0283168  # Cubic feet/s to Cubic meters/s\n",
    "    \n",
    "    # === EXPERIMENT SETTINGS ===\n",
    "    USE_STATIC = True       # Include static catchment attributes\n",
    "    NUM_BASINS = 5          # Number of basins to load (0 = all)\n",
    "    \n",
    "    # === MODEL HYPERPARAMETERS ===\n",
    "    SEQ_LENGTH = 60         # Lookback window (days)\n",
    "    PREDICT_HORIZON = 2     # Task 1: Predict t+k\n",
    "    PREDICT_STEPS = 5       # Task 2: Predict sequence t+1...t+5\n",
    "    \n",
    "    HIDDEN_DIM = 64         # LSTM hidden units\n",
    "    DROPOUT = 0.2           # Dropout rate\n",
    "    LEARNING_RATE = 0.001   # Adam learning rate\n",
    "    TEACHER_FORCING = 0.5   # Teacher forcing ratio for Seq2Seq\n",
    "    \n",
    "    # === TRAINING SETTINGS ===\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 256\n",
    "    USE_GPU = True\n",
    "    \n",
    "    # === DATA SPLIT (Hydrological Years) ===\n",
    "    # Standard CAMELS split:\n",
    "    TRAIN_START = '1980-10-01'\n",
    "    TRAIN_END   = '1995-09-30'\n",
    "    VAL_START   = '1995-10-01'\n",
    "    VAL_END     = '2000-09-30'\n",
    "    TEST_START  = '2000-10-01'\n",
    "    TEST_END    = '2010-09-30'\n",
    "    \n",
    "    # === FEATURE SELECTION ===\n",
    "    # Dynamic Inputs (Time-Series)\n",
    "    DYNAMIC_FEATURES = [\n",
    "        'PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp',   # Original Forcing\n",
    "        'PRCP_roll3', 'PRCP_roll7',             # Rolling Stats\n",
    "        'Q_lag1', 'Q_lag2', 'Q_lag3'            # Lag Features\n",
    "    ]\n",
    "    \n",
    "    # Static Inputs (Basin Attributes)\n",
    "    STATIC_FEATURES = [\n",
    "        'area_gages2',  # Catchment Area (Log transformed)\n",
    "        'elev_mean',    # Mean Elevation\n",
    "        'slope_mean',   # Topography\n",
    "        'sand_frac',    # Soil Type\n",
    "        'clay_frac',    # Soil Type\n",
    "        'frac_forest',  # Vegetation\n",
    "        'lai_max',      # Leaf Area Index\n",
    "        'p_mean',       # Long-term climate\n",
    "        'aridity'       # Climate Index\n",
    "    ]\n",
    "    \n",
    "    TARGET = 'Q_cms'  # Target variable (streamflow)\n",
    "\n",
    "# Initialize config\n",
    "cfg = Config()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if cfg.USE_GPU and torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECK DATASET AVAILABILITY ===\n",
    "dataset_exists = os.path.exists(cfg.BASE_DIR)\n",
    "\n",
    "if dataset_exists:\n",
    "    print(f\"âœ“ Dataset found at: {os.path.abspath(cfg.BASE_DIR)}\")\n",
    "    # List contents\n",
    "    contents = os.listdir(cfg.BASE_DIR)\n",
    "    print(f\"  Contents: {contents[:10]}{'...' if len(contents) > 10 else ''}\")\n",
    "else:\n",
    "    print(f\"âœ— Dataset NOT found at: {os.path.abspath(cfg.BASE_DIR)}\")\n",
    "    print(\"\\nPlease download the CAMELS dataset from:\")\n",
    "    print(\"  https://zenodo.org/records/15529996\")\n",
    "    print(f\"\\nAnd extract it to: {os.path.abspath(cfg.BASE_DIR)}\")\n",
    "    print(\"\\nExpected structure:\")\n",
    "    print(\"  basin_dataset_public/\")\n",
    "    print(\"  â”œâ”€â”€ basin_mean_forcing/nldas/\")\n",
    "    print(\"  â”œâ”€â”€ usgs_streamflow/\")\n",
    "    print(\"  â”œâ”€â”€ camels_topo.txt\")\n",
    "    print(\"  â”œâ”€â”€ camels_soil.txt\")\n",
    "    print(\"  â””â”€â”€ ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4950d62",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading\n",
    "\n",
    "The `CamelsLoader` class handles all data I/O operations:\n",
    "- Loading basin lists and filtering bad basins\n",
    "- Loading dynamic time series (streamflow + forcing)\n",
    "- Loading static catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADER CLASS ===\n",
    "\n",
    "class CamelsLoader:\n",
    "    \"\"\"Handles loading of CAMELS basin data.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.cfg = config\n",
    "    \n",
    "    def load_bad_basins(self):\n",
    "        \"\"\"Returns a list of basin IDs to exclude due to area errors.\"\"\"\n",
    "        if not os.path.exists(self.cfg.BAD_BASINS_FILE):\n",
    "            return []\n",
    "        \n",
    "        bad_ids = []\n",
    "        try:\n",
    "            with open(self.cfg.BAD_BASINS_FILE, 'r') as f:\n",
    "                next(f)  # Skip header\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        bad_ids.append(parts[1])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse bad basins file: {e}\")\n",
    "        return bad_ids\n",
    "    \n",
    "    def get_basin_list(self):\n",
    "        \"\"\"Scans directories and returns DataFrame of valid basins.\"\"\"\n",
    "        bad_basins = self.load_bad_basins()\n",
    "        \n",
    "        search_path = os.path.join(self.cfg.FLOW_DIR, '**', '*_streamflow_qc.txt')\n",
    "        files = glob.glob(search_path, recursive=True)\n",
    "        \n",
    "        basins = []\n",
    "        for f in files:\n",
    "            parts = f.split(os.sep)\n",
    "            region = parts[-2]\n",
    "            gauge_id = parts[-1].split('_')[0]\n",
    "            \n",
    "            if gauge_id not in bad_basins:\n",
    "                basins.append({'gauge_id': gauge_id, 'region': region})\n",
    "        \n",
    "        return pd.DataFrame(basins)\n",
    "    \n",
    "    def load_dynamic_data(self, gauge_id, region):\n",
    "        \"\"\"\n",
    "        Loads Streamflow + Forcing data for a single basin.\n",
    "        Returns cleaned DataFrame with standard column names.\n",
    "        \"\"\"\n",
    "        # 1. Load Streamflow\n",
    "        flow_path = os.path.join(self.cfg.FLOW_DIR, region, f'{gauge_id}_streamflow_qc.txt')\n",
    "        try:\n",
    "            df_flow = pd.read_csv(flow_path, delim_whitespace=True, header=None,\n",
    "                                  names=['gauge_id', 'Year', 'Month', 'Day', 'Q_cfs', 'QC'])\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        df_flow['Date'] = pd.to_datetime(df_flow[['Year', 'Month', 'Day']])\n",
    "        df_flow.set_index('Date', inplace=True)\n",
    "        # Convert to CMS and mark missing values\n",
    "        df_flow['Q_cms'] = df_flow['Q_cfs'].replace(-999, np.nan) * self.cfg.CFS_TO_CMS\n",
    "        \n",
    "        # 2. Load Forcing (NLDAS)\n",
    "        forcing_path = os.path.join(self.cfg.FORCING_DIR, region, \n",
    "                                    f'{gauge_id}_lump_nldas_forcing_leap.txt')\n",
    "        if not os.path.exists(forcing_path):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df_force = pd.read_csv(forcing_path, delim_whitespace=True, skiprows=3)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        # Normalize column names (handle variations)\n",
    "        col_map = {\n",
    "            'Mnth': 'Month', 'month': 'Month', 'mo': 'Month',\n",
    "            'year': 'Year', 'yr': 'Year',\n",
    "            'day': 'Day', 'dy': 'Day',\n",
    "            'prcp(mm/day)': 'PRCP', 'srad(w/m2)': 'SRAD',\n",
    "            'tmax(c)': 'Tmax', 'tmin(c)': 'Tmin', 'vp(pa)': 'Vp'\n",
    "        }\n",
    "        \n",
    "        new_cols = {}\n",
    "        for c in df_force.columns:\n",
    "            clean = c.strip()\n",
    "            if clean.lower() in col_map:\n",
    "                new_cols[c] = col_map[clean.lower()]\n",
    "            elif clean in col_map:\n",
    "                new_cols[c] = col_map[clean]\n",
    "        \n",
    "        df_force.rename(columns=new_cols, inplace=True)\n",
    "        \n",
    "        try:\n",
    "            df_force['Date'] = pd.to_datetime(df_force[['Year', 'Month', 'Day']])\n",
    "            df_force.set_index('Date', inplace=True)\n",
    "        except KeyError:\n",
    "            return None\n",
    "        \n",
    "        # 3. Merge flow and forcing\n",
    "        base_forcing = ['PRCP', 'SRAD', 'Tmax', 'Tmin', 'Vp']\n",
    "        cols_to_use = [c for c in base_forcing if c in df_force.columns]\n",
    "        df_merged = df_flow[['Q_cms']].join(df_force[cols_to_use], how='inner')\n",
    "        \n",
    "        return df_merged\n",
    "    \n",
    "    def load_static_attributes(self, basins_list=None):\n",
    "        \"\"\"\n",
    "        Loads all attribute files, merges them, and filters for requested features.\n",
    "        \"\"\"\n",
    "        dfs = []\n",
    "        for filename in self.cfg.STATIC_FILES:\n",
    "            path = os.path.join(self.cfg.BASE_DIR, filename)\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    df = pd.read_csv(path, sep=';')\n",
    "                    df.columns = [c.strip() for c in df.columns]\n",
    "                    if 'gauge_id' in df.columns:\n",
    "                        df['gauge_id'] = df['gauge_id'].astype(str).str.zfill(8)\n",
    "                        df.set_index('gauge_id', inplace=True)\n",
    "                        dfs.append(df)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if not dfs:\n",
    "            return None\n",
    "        \n",
    "        # Merge all static files\n",
    "        df_static = pd.concat(dfs, axis=1)\n",
    "        df_static = df_static.loc[:, ~df_static.columns.duplicated()]\n",
    "        \n",
    "        # Filter for configured features\n",
    "        available_feats = [f for f in self.cfg.STATIC_FEATURES if f in df_static.columns]\n",
    "        df_final = df_static[available_feats]\n",
    "        \n",
    "        if basins_list is not None:\n",
    "            df_final = df_final.reindex(basins_list)\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "# Initialize loader\n",
    "loader = CamelsLoader(cfg)\n",
    "print(\"CamelsLoader initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD BASIN LIST ===\n",
    "if dataset_exists:\n",
    "    df_basins = loader.get_basin_list()\n",
    "    \n",
    "    print(f\"Total valid basins found: {len(df_basins)}\")\n",
    "    print(f\"\\nRegion distribution:\")\n",
    "    print(df_basins['region'].value_counts().head(10))\n",
    "    \n",
    "    # Preview\n",
    "    display(df_basins.head())\n",
    "else:\n",
    "    print(\"Dataset not available. Please download first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7dbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD SAMPLE BASIN DATA ===\n",
    "if dataset_exists and len(df_basins) > 0:\n",
    "    # Load one basin for inspection\n",
    "    sample_row = df_basins.iloc[0]\n",
    "    sample_id = sample_row['gauge_id']\n",
    "    sample_region = sample_row['region']\n",
    "    \n",
    "    print(f\"Loading sample basin: {sample_id} (Region: {sample_region})\")\n",
    "    df_sample = loader.load_dynamic_data(sample_id, sample_region)\n",
    "    \n",
    "    if df_sample is not None:\n",
    "        print(f\"\\nShape: {df_sample.shape}\")\n",
    "        print(f\"Date range: {df_sample.index.min()} to {df_sample.index.max()}\")\n",
    "        print(f\"\\nColumns: {df_sample.columns.tolist()}\")\n",
    "        print(f\"\\nMissing values:\")\n",
    "        print(df_sample.isna().sum())\n",
    "        print(f\"\\n\")\n",
    "        display(df_sample.head(10))\n",
    "    else:\n",
    "        print(\"Failed to load sample basin.\")\n",
    "else:\n",
    "    print(\"Dataset not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfe08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD STATIC ATTRIBUTES ===\n",
    "if dataset_exists:\n",
    "    df_static = loader.load_static_attributes()\n",
    "    \n",
    "    if df_static is not None:\n",
    "        print(f\"Static attributes shape: {df_static.shape}\")\n",
    "        print(f\"\\nFeatures loaded: {df_static.columns.tolist()}\")\n",
    "        print(f\"\\n\")\n",
    "        display(df_static.head())\n",
    "    else:\n",
    "        print(\"No static attribute files found.\")\n",
    "else:\n",
    "    print(\"Dataset not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd5446",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualizations to understand:\n",
    "- Single basin time series characteristics\n",
    "- Flow duration curves\n",
    "- Seasonality patterns\n",
    "- Autocorrelation and cross-correlation (lag analysis)\n",
    "- Global statistics across all basins\n",
    "- Static feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7974ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.1 SINGLE BASIN ANALYSIS ===\n",
    "if dataset_exists and df_sample is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Hydrograph (2 year subset)\n",
    "    try:\n",
    "        subset = df_sample['1995':'1996']\n",
    "        if subset.empty:\n",
    "            raise ValueError(\"Empty slice\")\n",
    "        title_text = \"Hydrograph (1995-1996)\"\n",
    "    except:\n",
    "        subset = df_sample.iloc[:730]\n",
    "        title_text = \"Hydrograph (First 2 Years)\"\n",
    "    \n",
    "    axes[0].plot(subset.index, subset['Q_cms'], label='Flow', color='steelblue')\n",
    "    axes[0].set_title(title_text)\n",
    "    axes[0].set_ylabel(\"Flow (mÂ³/s)\")\n",
    "    axes[0].set_xlabel(\"Date\")\n",
    "    \n",
    "    # 2. Flow Duration Curve (FDC)\n",
    "    sorted_flow = np.sort(df_sample['Q_cms'].dropna())[::-1]\n",
    "    exceedance = np.arange(1, len(sorted_flow)+1) / len(sorted_flow) * 100\n",
    "    axes[1].semilogy(exceedance, sorted_flow, color='teal')\n",
    "    axes[1].set_title(\"Flow Duration Curve (Log Scale)\")\n",
    "    axes[1].set_xlabel(\"Exceedance Probability (%)\")\n",
    "    axes[1].set_ylabel(\"Flow (mÂ³/s)\")\n",
    "    axes[1].grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
    "    \n",
    "    # 3. Seasonality Check (Tmax vs Day of Year)\n",
    "    if 'Tmax' in df_sample.columns:\n",
    "        axes[2].scatter(df_sample.index.dayofyear, df_sample['Tmax'], \n",
    "                       s=1, alpha=0.3, c='coral')\n",
    "        axes[2].set_title(\"Seasonality Check: Tmax vs Day of Year\")\n",
    "        axes[2].set_xlabel(\"Day of Year\")\n",
    "        axes[2].set_ylabel(\"Temperature (Â°C)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dataset not available for EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df730526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.2 LAG ANALYSIS FOR FORECASTING ===\n",
    "if dataset_exists and df_sample is not None:\n",
    "    clean_df = df_sample.dropna()\n",
    "    lags = 10\n",
    "    \n",
    "    # Calculate correlations\n",
    "    auto_corr = acf(clean_df['Q_cms'], nlags=lags)\n",
    "    cross_corr = [clean_df['Q_cms'].corr(clean_df['PRCP'].shift(i)) for i in range(lags)]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Autocorrelation (Memory)\n",
    "    ax[0].bar(range(len(auto_corr)), auto_corr, color='teal')\n",
    "    ax[0].set_title(\"Target Autocorrelation (How dependent is Q on past Q?)\")\n",
    "    ax[0].set_xlabel(\"Lag (Days)\")\n",
    "    ax[0].set_ylabel(\"Correlation\")\n",
    "    ax[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Cross Correlation (Response Time)\n",
    "    ax[1].stem(range(lags), cross_corr)\n",
    "    ax[1].set_title(\"Precip-Flow Cross Correlation (Rain-to-River Lag)\")\n",
    "    ax[1].set_xlabel(\"Lag (Days)\")\n",
    "    ax[1].set_ylabel(\"Correlation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\nðŸ“Š Key Insights:\")\n",
    "    print(f\"  â€¢ Lag-1 autocorrelation: {auto_corr[1]:.3f} (High = Strong memory)\")\n",
    "    print(f\"  â€¢ Peak PRCP-Q correlation at lag: {np.argmax(cross_corr)} days\")\n",
    "else:\n",
    "    print(\"Dataset not available for lag analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.3 GLOBAL STATISTICS (ALL BASINS) ===\n",
    "# This cell processes multiple basins - may take a few minutes\n",
    "\n",
    "if dataset_exists:\n",
    "    global_stats = []\n",
    "    basins_to_process = df_basins.head(min(100, len(df_basins)))  # Limit for speed\n",
    "    \n",
    "    print(f\"Processing {len(basins_to_process)} basins for global statistics...\")\n",
    "    \n",
    "    for idx, row in tqdm(basins_to_process.iterrows(), total=len(basins_to_process)):\n",
    "        df = loader.load_dynamic_data(row['gauge_id'], row['region'])\n",
    "        \n",
    "        if df is None or len(df) < 365:\n",
    "            continue\n",
    "        \n",
    "        total_days = len(df)\n",
    "        missing_q = df['Q_cms'].isna().sum()\n",
    "        neg_flows = (df['Q_cms'] < 0).sum()\n",
    "        \n",
    "        mean_q = df['Q_cms'].mean()\n",
    "        std_q = df['Q_cms'].std()\n",
    "        cv_q = std_q / mean_q if mean_q > 0 else 0\n",
    "        \n",
    "        lag1_corr = df['Q_cms'].corr(df['Q_cms'].shift(1))\n",
    "        \n",
    "        global_stats.append({\n",
    "            'gauge_id': row['gauge_id'],\n",
    "            'region': row['region'],\n",
    "            'missing_pct': (missing_q / total_days) * 100,\n",
    "            'negative_q_count': neg_flows,\n",
    "            'mean_flow_cms': mean_q,\n",
    "            'cv_flow': cv_q,\n",
    "            'lag1_autocorr': lag1_corr\n",
    "        })\n",
    "    \n",
    "    df_global = pd.DataFrame(global_stats)\n",
    "    \n",
    "    print(\"\\\\n[Global Data Quality Summary]\")\n",
    "    display(df_global.describe().T[['mean', 'min', '50%', 'max']])\n",
    "else:\n",
    "    print(\"Dataset not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.4 GLOBAL DISTRIBUTION PLOTS ===\n",
    "if dataset_exists and len(global_stats) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Missing Data Histogram\n",
    "    sns.histplot(df_global['missing_pct'], bins=30, ax=axes[0,0], color='salmon')\n",
    "    axes[0,0].set_title(\"Distribution of Missing Data (%)\")\n",
    "    axes[0,0].set_xlabel(\"% Missing Days\")\n",
    "    \n",
    "    # 2. Basin Scale (Mean Flow - Log)\n",
    "    sns.histplot(df_global['mean_flow_cms'], bins=50, log_scale=True, ax=axes[0,1], color='steelblue')\n",
    "    axes[0,1].set_title(\"Distribution of Mean Streamflow (Log Scale)\")\n",
    "    axes[0,1].set_xlabel(\"Mean Flow (mÂ³/s)\")\n",
    "    \n",
    "    # 3. System Memory (Lag-1 Autocorrelation)\n",
    "    sns.histplot(df_global['lag1_autocorr'], bins=30, ax=axes[1,0], color='green')\n",
    "    axes[1,0].set_title(\"Distribution of Lag-1 Autocorrelation (System Memory)\")\n",
    "    axes[1,0].set_xlabel(\"Correlation r(t, t-1)\")\n",
    "    \n",
    "    # 4. Flow Variability (CV)\n",
    "    sns.histplot(df_global['cv_flow'], bins=30, ax=axes[1,1], color='purple')\n",
    "    axes[1,1].set_title(\"Distribution of Flow Coefficient of Variation\")\n",
    "    axes[1,1].set_xlabel(\"CV (std/mean)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Global statistics not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c299f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3.5 STATIC FEATURES ANALYSIS ===\n",
    "if dataset_exists and df_static is not None:\n",
    "    # Key features to visualize\n",
    "    key_features = ['area_gages2', 'elev_mean', 'aridity', 'sand_frac', 'frac_forest', 'p_mean']\n",
    "    plot_feats = [f for f in key_features if f in df_static.columns]\n",
    "    \n",
    "    if plot_feats:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, feat in enumerate(plot_feats):\n",
    "            sns.histplot(df_static[feat].dropna(), bins=30, ax=axes[i], color='teal')\n",
    "            axes[i].set_title(f\"Distribution of {feat}\")\n",
    "            axes[i].set_xlabel(feat)\n",
    "        \n",
    "        plt.suptitle(\"Key Static Feature Distributions\", fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Static features not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad901e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering\n",
    "\n",
    "Create derived features to improve model performance:\n",
    "- **Rolling Statistics**: 3-day and 7-day precipitation averages (soil moisture proxy)\n",
    "- **Lag Features**: Previous streamflow values (autoregressive input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38556afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE ENGINEER CLASS ===\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Creates derived features for the prediction model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.cfg = config\n",
    "    \n",
    "    def add_rolling_features(self, df):\n",
    "        \"\"\"\n",
    "        Calculates rolling statistics for Precipitation.\n",
    "        Purpose: Represents accumulated soil moisture / wetness.\n",
    "        \"\"\"\n",
    "        if 'PRCP' in df.columns:\n",
    "            # 3-Day Rolling Mean (Short-term wetness)\n",
    "            df['PRCP_roll3'] = df['PRCP'].rolling(window=3, min_periods=1).mean()\n",
    "            # 7-Day Rolling Mean (Medium-term saturation)\n",
    "            df['PRCP_roll7'] = df['PRCP'].rolling(window=7, min_periods=1).mean()\n",
    "        return df\n",
    "    \n",
    "    def add_lag_features(self, df):\n",
    "        \"\"\"\n",
    "        Adds explicit lag features for Streamflow.\n",
    "        Purpose: Give model explicit access to recent flow values.\n",
    "        \"\"\"\n",
    "        target = self.cfg.TARGET\n",
    "        \n",
    "        if target in df.columns:\n",
    "            df['Q_lag1'] = df[target].shift(1)\n",
    "            df['Q_lag2'] = df[target].shift(2)\n",
    "            df['Q_lag3'] = df[target].shift(3)\n",
    "            \n",
    "            # Backfill NaNs created by shifting\n",
    "            for col in ['Q_lag1', 'Q_lag2', 'Q_lag3']:\n",
    "                df[col] = df[col].bfill()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Run all engineering steps.\n",
    "        Should be run AFTER cleaning, BEFORE normalization.\n",
    "        \"\"\"\n",
    "        df = self.add_rolling_features(df)\n",
    "        df = self.add_lag_features(df)\n",
    "        return df\n",
    "\n",
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer(cfg)\n",
    "print(\"FeatureEngineer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST FEATURE ENGINEERING ===\n",
    "if dataset_exists and df_sample is not None:\n",
    "    # Apply feature engineering to sample\n",
    "    df_engineered = engineer.transform(df_sample.copy())\n",
    "    \n",
    "    print(\"New features added:\")\n",
    "    new_cols = ['PRCP_roll3', 'PRCP_roll7', 'Q_lag1', 'Q_lag2', 'Q_lag3']\n",
    "    print(f\"  {[c for c in new_cols if c in df_engineered.columns]}\")\n",
    "    \n",
    "    print(f\"\\\\nShape before: {df_sample.shape}\")\n",
    "    print(f\"Shape after:  {df_engineered.shape}\")\n",
    "    \n",
    "    display(df_engineered.head(10))\n",
    "else:\n",
    "    print(\"Dataset not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b1ae4",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Preprocessing\n",
    "\n",
    "The `CamelsPreprocessor` handles:\n",
    "- **Physical Outlier Cleaning**: Remove impossible values (negative rain, extreme temps)\n",
    "- **Missing Data Handling**: Interpolation and fill strategies\n",
    "- **Date Features**: Cyclical sin/cos encoding for day-of-year\n",
    "- **Normalization**: Global dynamic normalization + basin-specific target normalization\n",
    "- **Sequence Creation**: Sliding window sequences for LSTM input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69296557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPROCESSOR CLASS ===\n",
    "\n",
    "class CamelsPreprocessor:\n",
    "    \"\"\"Handles data cleaning, normalization, and sequence creation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.cfg = config\n",
    "        self.scalers = {}        # Global normalization stats\n",
    "        self.basin_scalers = {}  # Per-basin target stats\n",
    "        \n",
    "        # Physical Constraints\n",
    "        self.PHYSICAL_LIMITS = {\n",
    "            'PRCP': {'min': 0.0, 'max': None},\n",
    "            'Q_cms': {'min': 0.0, 'max': None},\n",
    "            'Tmax': {'min': -60.0, 'max': 60.0},\n",
    "            'Tmin': {'min': -60.0, 'max': 60.0}\n",
    "        }\n",
    "        self.MAX_INTERPOLATE_GAP = 5\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        \"\"\"Add cyclical day-of-year features.\"\"\"\n",
    "        day_of_year = df.index.dayofyear\n",
    "        df['sin_doy'] = np.sin(2 * np.pi * day_of_year / 365.0)\n",
    "        df['cos_doy'] = np.cos(2 * np.pi * day_of_year / 365.0)\n",
    "        return df\n",
    "    \n",
    "    def clean_physical_outliers(self, df):\n",
    "        \"\"\"Remove physically impossible values.\"\"\"\n",
    "        # Negative Rain/Flow -> 0\n",
    "        for col in ['PRCP', self.cfg.TARGET]:\n",
    "            if col in df.columns:\n",
    "                mask = df[col] < 0\n",
    "                if mask.any():\n",
    "                    df.loc[mask, col] = 0.0\n",
    "        \n",
    "        # Unrealistic Temp -> NaN\n",
    "        for col in ['Tmax', 'Tmin']:\n",
    "            if col in df.columns:\n",
    "                limits = self.PHYSICAL_LIMITS[col]\n",
    "                mask = (df[col] < limits['min']) | (df[col] > limits['max'])\n",
    "                if mask.any():\n",
    "                    df.loc[mask, col] = np.nan\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_data(self, df):\n",
    "        \"\"\"Interpolate and fill missing values.\"\"\"\n",
    "        cols_to_fix = [self.cfg.TARGET] + self.cfg.DYNAMIC_FEATURES\n",
    "        cols_to_fix = [c for c in cols_to_fix if c in df.columns]\n",
    "        \n",
    "        for col in cols_to_fix:\n",
    "            df[col] = df[col].interpolate(method='linear', \n",
    "                                          limit=self.MAX_INTERPOLATE_GAP, \n",
    "                                          limit_direction='both')\n",
    "            df[col] = df[col].ffill().bfill()\n",
    "        return df\n",
    "    \n",
    "    def fit(self, dynamic_data_dict, static_df=None):\n",
    "        \"\"\"Compute normalization statistics from training data.\"\"\"\n",
    "        print(\"Computing global statistics...\")\n",
    "        \n",
    "        # 1. Dynamic Stats (Global)\n",
    "        dyn_vals = []\n",
    "        for gid, df in dynamic_data_dict.items():\n",
    "            train_slice = df.loc[self.cfg.TRAIN_START:self.cfg.TRAIN_END]\n",
    "            if not train_slice.empty:\n",
    "                available_cols = [c for c in self.cfg.DYNAMIC_FEATURES if c in train_slice.columns]\n",
    "                valid_rows = train_slice[available_cols].dropna()\n",
    "                if not valid_rows.empty:\n",
    "                    dyn_vals.append(valid_rows.values)\n",
    "        \n",
    "        if dyn_vals:\n",
    "            all_dyn = np.vstack(dyn_vals)\n",
    "            self.scalers['dynamic_mean'] = np.mean(all_dyn, axis=0)\n",
    "            self.scalers['dynamic_std'] = np.std(all_dyn, axis=0) + 1e-6\n",
    "        else:\n",
    "            self.scalers['dynamic_mean'] = 0\n",
    "            self.scalers['dynamic_std'] = 1\n",
    "        \n",
    "        # 2. Static Stats (if provided)\n",
    "        if static_df is not None:\n",
    "            static_copy = static_df.copy()\n",
    "            if 'area_gages2' in static_copy.columns:\n",
    "                static_copy['area_gages2'] = np.log10(np.maximum(static_copy['area_gages2'], 1e-3))\n",
    "            self.scalers['static_mean'] = static_copy.mean().values\n",
    "            self.scalers['static_std'] = static_copy.std().values + 1e-6\n",
    "        else:\n",
    "            print(\"-> Skipping Static Stats (not provided)\")\n",
    "        \n",
    "        # 3. Basin-Specific Target Stats\n",
    "        print(\"Computing basin-specific target statistics...\")\n",
    "        for gid, df in dynamic_data_dict.items():\n",
    "            train_slice = df.loc[self.cfg.TRAIN_START:self.cfg.TRAIN_END]\n",
    "            clean_target = train_slice[self.cfg.TARGET].dropna()\n",
    "            \n",
    "            if not clean_target.empty:\n",
    "                self.basin_scalers[gid] = {\n",
    "                    'mean': clean_target.mean(),\n",
    "                    'std': clean_target.std() + 1e-6\n",
    "                }\n",
    "            else:\n",
    "                self.basin_scalers[gid] = {'mean': 0, 'std': 1}\n",
    "    \n",
    "    def transform(self, df_dynamic, df_static=None, gauge_id=None):\n",
    "        \"\"\"\n",
    "        Normalize data and prepare for sequence creation.\n",
    "        Returns: (data_matrix, static_norm_vector)\n",
    "        \"\"\"\n",
    "        # 1. Normalize Dynamic Features\n",
    "        dyn_cols = [c for c in self.cfg.DYNAMIC_FEATURES if c in df_dynamic.columns]\n",
    "        X_dyn = df_dynamic[dyn_cols].values\n",
    "        X_dyn_norm = (X_dyn - self.scalers['dynamic_mean']) / self.scalers['dynamic_std']\n",
    "        \n",
    "        # 2. Normalize Target (Basin-Specific)\n",
    "        target = df_dynamic[self.cfg.TARGET].values\n",
    "        b_stats = self.basin_scalers.get(gauge_id, {'mean': 0, 'std': 1})\n",
    "        y_norm = (target - b_stats['mean']) / b_stats['std']\n",
    "        \n",
    "        # 3. Normalize Static (if available)\n",
    "        X_stat_norm = None\n",
    "        if df_static is not None and gauge_id in df_static.index:\n",
    "            static_vals = df_static.loc[gauge_id].values.copy()\n",
    "            if 'area_gages2' in df_static.columns:\n",
    "                area_idx = df_static.columns.get_loc('area_gages2')\n",
    "                static_vals[area_idx] = np.log10(np.maximum(static_vals[area_idx], 1e-3))\n",
    "            X_stat_norm = (static_vals - self.scalers['static_mean']) / self.scalers['static_std']\n",
    "        \n",
    "        # 4. Date Features\n",
    "        date_feats = df_dynamic[['sin_doy', 'cos_doy']].values\n",
    "        \n",
    "        # Combine: [Dynamic_Norm, Date, Target_Norm]\n",
    "        data_matrix = np.column_stack([X_dyn_norm, date_feats, y_norm])\n",
    "        \n",
    "        return data_matrix, X_stat_norm\n",
    "    \n",
    "    def create_sequences(self, data_matrix, static_vec=None, mode='task1'):\n",
    "        \"\"\"\n",
    "        Creates sliding window sequences for LSTM.\n",
    "        mode='task1': Single-step prediction (t+2)\n",
    "        mode='task2': Multi-step sequence prediction (t+1..t+5)\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        seq_len = self.cfg.SEQ_LENGTH\n",
    "        total_samples = len(data_matrix)\n",
    "        \n",
    "        use_static = (static_vec is not None)\n",
    "        if use_static:\n",
    "            static_repeated = np.tile(static_vec, (seq_len, 1))\n",
    "        \n",
    "        if mode == 'task1':\n",
    "            horizon = self.cfg.PREDICT_HORIZON\n",
    "            for t in range(seq_len, total_samples - horizon + 1):\n",
    "                window_data = data_matrix[t-seq_len:t, :]\n",
    "                target_val = data_matrix[t + horizon - 1, -1]\n",
    "                \n",
    "                if np.isnan(window_data).any() or np.isnan(target_val):\n",
    "                    continue\n",
    "                \n",
    "                if use_static:\n",
    "                    full_X = np.hstack([window_data, static_repeated])\n",
    "                else:\n",
    "                    full_X = window_data\n",
    "                \n",
    "                X_seq.append(full_X)\n",
    "                y_seq.append(target_val)\n",
    "        \n",
    "        elif mode == 'task2':\n",
    "            steps = self.cfg.PREDICT_STEPS\n",
    "            for t in range(seq_len, total_samples - steps + 1):\n",
    "                window_data = data_matrix[t-seq_len:t, :]\n",
    "                target_seq = data_matrix[t:t+steps, -1]\n",
    "                \n",
    "                if np.isnan(window_data).any() or np.isnan(target_seq).any():\n",
    "                    continue\n",
    "                \n",
    "                if use_static:\n",
    "                    full_X = np.hstack([window_data, static_repeated])\n",
    "                else:\n",
    "                    full_X = window_data\n",
    "                \n",
    "                X_seq.append(full_X)\n",
    "                y_seq.append(target_seq)\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = CamelsPreprocessor(cfg)\n",
    "print(\"CamelsPreprocessor initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d09b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Definitions\n",
    "\n",
    "### Model Architectures\n",
    "\n",
    "**Task 1 - LSTM (Single-Step Prediction)**\n",
    "```\n",
    "Input (Batch, Seq_Len, Features) \n",
    "  â†’ LSTM(hidden=64) \n",
    "  â†’ Last Hidden State \n",
    "  â†’ Dropout(0.2) \n",
    "  â†’ Linear(1) \n",
    "  â†’ Output (flow at t+2)\n",
    "```\n",
    "\n",
    "**Task 2 - Encoder-Decoder with Cross-Attention (Multi-Step)**\n",
    "```\n",
    "Encoder: Past sequence â†’ LSTM â†’ Hidden States\n",
    "Decoder: For each step t in [1..5]:\n",
    "  Input: [prev_flow, future_forcing[t], static]\n",
    "  â†’ LSTMCell \n",
    "  â†’ CrossAttention(encoder_outputs) \n",
    "  â†’ Linear(1) \n",
    "  â†’ prediction[t]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1218eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL: LSTM (Task 1 - Single Step) ===\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"Standard LSTM for Single-Step Prediction (t+2).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq_Len, Features)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Take last hidden state\n",
    "        last_hidden = out[:, -1, :]  # (Batch, Hidden)\n",
    "        \n",
    "        out = self.dropout(last_hidden)\n",
    "        prediction = self.head(out)  # (Batch, 1)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "print(\"LSTM model class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eff700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL: Cross-Attention Mechanism ===\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Bahdanau-style attention for Seq2Seq decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (Batch, Hidden) - current decoder state\n",
    "        # encoder_outputs: (Batch, Seq_Len, Hidden)\n",
    "        \n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repeat decoder hidden for all encoder timesteps\n",
    "        hidden_expanded = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Calculate attention energy\n",
    "        combined = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "        attention = self.v(energy).squeeze(2)  # (Batch, Seq_Len)\n",
    "        \n",
    "        # Softmax weights\n",
    "        weights = F.softmax(attention, dim=1)\n",
    "        \n",
    "        # Context vector (weighted sum)\n",
    "        context = torch.bmm(weights.unsqueeze(1), encoder_outputs)\n",
    "        \n",
    "        return context.squeeze(1), weights\n",
    "\n",
    "print(\"CrossAttention module defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bdd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL: LSTM_Seq2Seq (Task 2 - Multi-Step) ===\n",
    "\n",
    "class LSTM_Seq2Seq(nn.Module):\n",
    "    \"\"\"Encoder-Decoder with Cross-Attention for sequence prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, future_forcing_dim, static_dim, output_steps=5):\n",
    "        super(LSTM_Seq2Seq, self).__init__()\n",
    "        self.output_steps = output_steps\n",
    "        self.static_dim = static_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Decoder components\n",
    "        decoder_input_dim = 1 + future_forcing_dim + static_dim\n",
    "        self.decoder_cell = nn.LSTMCell(decoder_input_dim, hidden_dim)\n",
    "        self.attention = CrossAttention(hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, 1)\n",
    "    \n",
    "    def forward(self, x_past, x_future_forcing, static_features, \n",
    "                target_seq=None, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_past: (Batch, Past_Seq, Input_Dim)\n",
    "            x_future_forcing: (Batch, 5, Future_Forcing_Dim)\n",
    "            static_features: (Batch, Static_Dim)\n",
    "            target_seq: (Batch, 5) - Ground truth for teacher forcing\n",
    "            teacher_forcing_ratio: Probability of using ground truth\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (Batch, 5) - Predicted flow sequence\n",
    "        \"\"\"\n",
    "        batch_size = x_past.size(0)\n",
    "        \n",
    "        # 1. Encode past sequence\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(x_past)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        cell = cell.squeeze(0)\n",
    "        \n",
    "        # Initialize outputs\n",
    "        outputs = torch.zeros(batch_size, self.output_steps).to(x_past.device)\n",
    "        \n",
    "        # First decoder input: last observed flow\n",
    "        decoder_input_flow = x_past[:, -1, -1].unsqueeze(1)\n",
    "        \n",
    "        # 2. Decode loop\n",
    "        for t in range(self.output_steps):\n",
    "            # Prepare decoder input\n",
    "            current_forcing = x_future_forcing[:, t, :]\n",
    "            inputs_list = [decoder_input_flow, current_forcing]\n",
    "            \n",
    "            if self.static_dim > 0 and static_features is not None:\n",
    "                inputs_list.append(static_features)\n",
    "            \n",
    "            dec_input = torch.cat(inputs_list, dim=1)\n",
    "            \n",
    "            # Run decoder cell\n",
    "            hidden, cell = self.decoder_cell(dec_input, (hidden, cell))\n",
    "            \n",
    "            # Cross attention\n",
    "            context, _ = self.attention(hidden, encoder_outputs)\n",
    "            \n",
    "            # Predict\n",
    "            combined = torch.cat((hidden, context), dim=1)\n",
    "            prediction = self.fc_out(combined)\n",
    "            \n",
    "            outputs[:, t] = prediction.squeeze(1)\n",
    "            \n",
    "            # Teacher forcing\n",
    "            if target_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                decoder_input_flow = target_seq[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input_flow = prediction\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"LSTM_Seq2Seq model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa91d1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training & Evaluation\n",
    "\n",
    "### Evaluation Metric: Nash-Sutcliffe Efficiency (NSE)\n",
    "\n",
    "$$NSE = 1 - \\frac{\\sum(Q_{obs} - Q_{sim})^2}{\\sum(Q_{obs} - \\bar{Q}_{obs})^2}$$\n",
    "\n",
    "- **NSE = 1**: Perfect prediction\n",
    "- **NSE = 0**: Model is as good as predicting the mean\n",
    "- **NSE < 0**: Model is worse than the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UTILITY FUNCTIONS ===\n",
    "\n",
    "def calc_nse(obs, sim):\n",
    "    \"\"\"Calculate Nash-Sutcliffe Efficiency.\"\"\"\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2) + 1e-6\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def save_results(results, params, output_dir):\n",
    "    \"\"\"Save metrics and parameters to JSON.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'params.json'), 'w') as f:\n",
    "        params_clean = {k: str(v) for k, v in params.items()}\n",
    "        json.dump(params_clean, f, indent=4)\n",
    "    \n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA PREPARATION PIPELINE ===\n",
    "\n",
    "def prepare_data(config, loader, engineer, preprocessor, num_basins=5):\n",
    "    \"\"\"\n",
    "    Full data preparation pipeline:\n",
    "    1. Load basin list\n",
    "    2. Load dynamic data for each basin\n",
    "    3. Apply cleaning, feature engineering, date features\n",
    "    4. Fit preprocessor (compute normalization stats)\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Loading Data ---\")\n",
    "    df_basins = loader.get_basin_list()\n",
    "    \n",
    "    if df_basins.empty:\n",
    "        raise ValueError(\"No basins found or all filtered out.\")\n",
    "    \n",
    "    total_found = len(df_basins)\n",
    "    if num_basins > 0:\n",
    "        print(f\"Found {total_found} basins. Loading first {num_basins} for experiment...\")\n",
    "        df_basins = df_basins.head(num_basins)\n",
    "    else:\n",
    "        print(f\"Loading all {total_found} basins...\")\n",
    "    \n",
    "    basin_ids = df_basins['gauge_id'].tolist()\n",
    "    df_static = loader.load_static_attributes(basin_ids) if config.USE_STATIC else None\n",
    "    \n",
    "    dynamic_data = {}\n",
    "    print(\"Loading and preprocessing dynamic data...\")\n",
    "    for _, row in tqdm(df_basins.iterrows(), total=len(df_basins)):\n",
    "        gid = row['gauge_id']\n",
    "        region = row['region']\n",
    "        \n",
    "        df = loader.load_dynamic_data(gid, region)\n",
    "        if df is not None:\n",
    "            df = preprocessor.clean_physical_outliers(df)\n",
    "            df = preprocessor.handle_missing_data(df)\n",
    "            df = engineer.transform(df)\n",
    "            df = preprocessor.add_date_features(df)\n",
    "            dynamic_data[gid] = df\n",
    "    \n",
    "    print(f\"Successfully loaded {len(dynamic_data)} basins.\")\n",
    "    \n",
    "    print(\"\\\\n--- 2. Fitting Preprocessor ---\")\n",
    "    preprocessor.fit(dynamic_data, df_static)\n",
    "    \n",
    "    return dynamic_data, df_static, basin_ids\n",
    "\n",
    "print(\"Data preparation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET GENERATORS ===\n",
    "\n",
    "def get_task1_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, split='train'):\n",
    "    \"\"\"Generate dataset for Task 1 (single-step prediction).\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    if split == 'train':\n",
    "        start, end = config.TRAIN_START, config.TRAIN_END\n",
    "    elif split == 'val':\n",
    "        start, end = config.VAL_START, config.VAL_END\n",
    "    else:\n",
    "        start, end = config.TEST_START, config.TEST_END\n",
    "    \n",
    "    for gid in basin_ids:\n",
    "        if gid not in dynamic_data:\n",
    "            continue\n",
    "        df = dynamic_data[gid].loc[start:end]\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        data_matrix, static_vec = preprocessor.transform(df, df_static, gid)\n",
    "        X, y = preprocessor.create_sequences(\n",
    "            data_matrix, \n",
    "            static_vec if config.USE_STATIC else None, \n",
    "            mode='task1'\n",
    "        )\n",
    "        \n",
    "        if len(X) > 0:\n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "    \n",
    "    if not X_list:\n",
    "        return None, None\n",
    "    return np.concatenate(X_list), np.concatenate(y_list)\n",
    "\n",
    "\n",
    "def get_task2_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, split='train'):\n",
    "    \"\"\"Generate dataset for Task 2 (multi-step sequence prediction).\"\"\"\n",
    "    X_past_list, X_future_list, Static_list, Y_list = [], [], [], []\n",
    "    \n",
    "    if split == 'train':\n",
    "        start, end = config.TRAIN_START, config.TRAIN_END\n",
    "    elif split == 'val':\n",
    "        start, end = config.VAL_START, config.VAL_END\n",
    "    else:\n",
    "        start, end = config.TEST_START, config.TEST_END\n",
    "    \n",
    "    # Indices for forcing features (first 5 dynamic features + date features)\n",
    "    forcing_indices = [0, 1, 2, 3, 4]  # PRCP, SRAD, Tmax, Tmin, Vp\n",
    "    n_dyn = len(config.DYNAMIC_FEATURES)\n",
    "    future_feat_indices = forcing_indices + [n_dyn, n_dyn + 1]  # + sin_doy, cos_doy\n",
    "    \n",
    "    for gid in basin_ids:\n",
    "        if gid not in dynamic_data:\n",
    "            continue\n",
    "        df = dynamic_data[gid].loc[start:end]\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        data_matrix, static_vec = preprocessor.transform(df, df_static, gid)\n",
    "        \n",
    "        seq_len = config.SEQ_LENGTH\n",
    "        steps = config.PREDICT_STEPS\n",
    "        total = len(data_matrix)\n",
    "        \n",
    "        static_repeated = np.tile(static_vec, (seq_len, 1)) if (config.USE_STATIC and static_vec is not None) else None\n",
    "        \n",
    "        c_xp, c_xf, c_st, c_y = [], [], [], []\n",
    "        \n",
    "        for t in range(seq_len, total - steps + 1):\n",
    "            past_window = data_matrix[t-seq_len:t, :]\n",
    "            future_window = data_matrix[t:t+steps, :][:, future_feat_indices]\n",
    "            target_seq = data_matrix[t:t+steps, -1]\n",
    "            \n",
    "            if np.isnan(past_window).any() or np.isnan(future_window).any() or np.isnan(target_seq).any():\n",
    "                continue\n",
    "            \n",
    "            if static_repeated is not None:\n",
    "                full_past = np.hstack([past_window, static_repeated])\n",
    "            else:\n",
    "                full_past = past_window\n",
    "            \n",
    "            c_xp.append(full_past)\n",
    "            c_xf.append(future_window)\n",
    "            c_st.append(static_vec if static_vec is not None else np.zeros(1))\n",
    "            c_y.append(target_seq)\n",
    "        \n",
    "        if len(c_xp) > 0:\n",
    "            X_past_list.append(np.array(c_xp))\n",
    "            X_future_list.append(np.array(c_xf))\n",
    "            Static_list.append(np.array(c_st))\n",
    "            Y_list.append(np.array(c_y))\n",
    "    \n",
    "    if not X_past_list:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    return (np.concatenate(X_past_list), np.concatenate(X_future_list),\n",
    "            np.concatenate(Static_list), np.concatenate(Y_list))\n",
    "\n",
    "print(\"Dataset generator functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TASK 1: TRAINING FUNCTION ===\n",
    "\n",
    "def run_task1(config, device, dynamic_data, df_static, preprocessor, basin_ids):\n",
    "    \"\"\"Train and evaluate Task 1 (Single-Step Prediction).\"\"\"\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\" TASK 1: Single Step Prediction (t+2)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Prepare datasets\n",
    "    print(\"\\\\nPreparing datasets...\")\n",
    "    X_train, y_train = get_task1_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, 'train')\n",
    "    X_val, y_val = get_task1_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, 'val')\n",
    "    \n",
    "    if X_train is None:\n",
    "        print(\"Not enough data for Task 1.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Train samples: {len(X_train)}, Val samples: {len(X_val)}\")\n",
    "    \n",
    "    train_ds = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "    val_ds = TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val))\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 2. Initialize model\n",
    "    input_dim = X_train.shape[2]\n",
    "    model = LSTM(input_dim=input_dim, hidden_dim=config.HIDDEN_DIM, dropout=config.DROPOUT).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(f\"\\\\nModel input dim: {input_dim}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 3. Training loop\n",
    "    best_loss = float('inf')\n",
    "    save_path = os.path.join(config.RESULTS_DIR, 'task1', 'best_model.pth')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    print(f\"\\\\nTraining for {config.EPOCHS} epochs...\")\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_b, y_b in train_loader:\n",
    "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_b).squeeze(), y_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in val_loader:\n",
    "                X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "                val_loss += criterion(model(X_b).squeeze(), y_b).item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d}: Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f}\", end=\"\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\" âœ“ Best\")\n",
    "        else:\n",
    "            print()\n",
    "    \n",
    "    # 4. Evaluate on test set\n",
    "    print(\"\\\\nEvaluating on Test Set...\")\n",
    "    X_test, y_test = get_task1_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, 'test')\n",
    "    \n",
    "    if X_test is not None:\n",
    "        model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(torch.Tensor(X_test).to(device)).cpu().numpy().squeeze()\n",
    "        \n",
    "        nse = calc_nse(y_test, preds)\n",
    "        mse = float(np.mean((y_test - preds) ** 2))\n",
    "        \n",
    "        print(f\"\\\\nðŸ“Š Task 1 Results:\")\n",
    "        print(f\"   Test NSE: {nse:.4f}\")\n",
    "        print(f\"   Test MSE: {mse:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        save_results({'NSE': nse, 'Test_MSE': mse}, \n",
    "                    {'epochs': config.EPOCHS, 'batch_size': config.BATCH_SIZE}, \n",
    "                    os.path.join(config.RESULTS_DIR, 'task1'))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'test_nse': nse,\n",
    "            'predictions': preds,\n",
    "            'targets': y_test\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Task 1 training function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00149759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TASK 2: TRAINING FUNCTION ===\n",
    "\n",
    "def run_task2(config, device, dynamic_data, df_static, preprocessor, basin_ids):\n",
    "    \"\"\"Train and evaluate Task 2 (Multi-Step Sequence Prediction).\"\"\"\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\" TASK 2: Multi-Step Sequence (t+1..t+5)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Prepare datasets\n",
    "    print(\"\\\\nPreparing datasets...\")\n",
    "    tr_data = get_task2_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, 'train')\n",
    "    val_data = get_task2_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, 'val')\n",
    "    \n",
    "    if tr_data[0] is None:\n",
    "        print(\"Not enough data for Task 2.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Train samples: {len(tr_data[0])}, Val samples: {len(val_data[0])}\")\n",
    "    \n",
    "    train_ds = TensorDataset(*[torch.Tensor(x) for x in tr_data])\n",
    "    val_ds = TensorDataset(*[torch.Tensor(x) for x in val_data])\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 2. Initialize model\n",
    "    input_dim = tr_data[0].shape[2]\n",
    "    future_dim = tr_data[1].shape[2]\n",
    "    static_dim = tr_data[2].shape[1]\n",
    "    \n",
    "    model = LSTM_Seq2Seq(\n",
    "        input_dim, config.HIDDEN_DIM, future_dim, static_dim, config.PREDICT_STEPS\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(f\"\\\\nModel dims - Input: {input_dim}, Future: {future_dim}, Static: {static_dim}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 3. Training loop\n",
    "    best_loss = float('inf')\n",
    "    save_path = os.path.join(config.RESULTS_DIR, 'task2', 'best_model.pth')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    print(f\"\\\\nTraining for {config.EPOCHS} epochs...\")\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for xp, xf, st, y in train_loader:\n",
    "            xp, xf, st, y = xp.to(device), xf.to(device), st.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xp, xf, st, target_seq=y, teacher_forcing_ratio=config.TEACHER_FORCING)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xp, xf, st, y in val_loader:\n",
    "                xp, xf, st, y = xp.to(device), xf.to(device), st.to(device), y.to(device)\n",
    "                preds = model(xp, xf, st, target_seq=None, teacher_forcing_ratio=0)\n",
    "                val_loss += criterion(preds, y).item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d}: Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f}\", end=\"\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\" âœ“ Best\")\n",
    "        else:\n",
    "            print()\n",
    "    \n",
    "    # 4. Evaluate on test set\n",
    "    print(\"\\\\nEvaluating on Test Set...\")\n",
    "    te_data = get_task2_dataset(config, dynamic_data, df_static, preprocessor, basin_ids, 'test')\n",
    "    \n",
    "    if te_data[0] is not None:\n",
    "        test_ds = TensorDataset(*[torch.Tensor(x) for x in te_data])\n",
    "        test_loader = DataLoader(test_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "        model.eval()\n",
    "        \n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for xp, xf, st, y in test_loader:\n",
    "                xp, xf, st = xp.to(device), xf.to(device), st.to(device)\n",
    "                out = model(xp, xf, st, target_seq=None, teacher_forcing_ratio=0)\n",
    "                all_preds.append(out.cpu().numpy())\n",
    "                all_targets.append(y.numpy())\n",
    "        \n",
    "        preds_flat = np.concatenate(all_preds).flatten()\n",
    "        targets_flat = np.concatenate(all_targets).flatten()\n",
    "        nse = calc_nse(targets_flat, preds_flat)\n",
    "        \n",
    "        print(f\"\\\\nðŸ“Š Task 2 Results:\")\n",
    "        print(f\"   Test NSE: {nse:.4f}\")\n",
    "        \n",
    "        save_results({'NSE': nse}, \n",
    "                    {'epochs': config.EPOCHS, 'batch_size': config.BATCH_SIZE},\n",
    "                    os.path.join(config.RESULTS_DIR, 'task2'))\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'test_nse': nse,\n",
    "            'predictions': np.concatenate(all_preds),\n",
    "            'targets': np.concatenate(all_targets)\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Task 2 training function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50921c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN FULL PIPELINE ===\n",
    "# This cell executes the complete training pipeline\n",
    "\n",
    "if dataset_exists:\n",
    "    # Prepare data\n",
    "    dynamic_data, df_static_train, basin_ids = prepare_data(\n",
    "        cfg, loader, engineer, preprocessor, \n",
    "        num_basins=cfg.NUM_BASINS\n",
    "    )\n",
    "    \n",
    "    # Train Task 1\n",
    "    task1_results = run_task1(cfg, device, dynamic_data, df_static_train, preprocessor, basin_ids)\n",
    "    \n",
    "    # Train Task 2\n",
    "    task2_results = run_task2(cfg, device, dynamic_data, df_static_train, preprocessor, basin_ids)\n",
    "else:\n",
    "    print(\"Dataset not available. Please download the CAMELS dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae92ea",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Results & Analysis\n",
    "\n",
    "Visualize training progress and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.1 TRAINING CURVES ===\n",
    "\n",
    "def plot_training_curves(results, task_name):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    if results is None:\n",
    "        print(f\"No results available for {task_name}\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    \n",
    "    epochs = range(1, len(results['train_losses']) + 1)\n",
    "    ax.plot(epochs, results['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax.plot(epochs, results['val_losses'], 'r-', label='Val Loss', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE Loss')\n",
    "    ax.set_title(f'{task_name} - Training Progress (Test NSE: {results[\"test_nse\"]:.4f})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot if results exist\n",
    "if 'task1_results' in dir() and task1_results is not None:\n",
    "    plot_training_curves(task1_results, \"Task 1: Single-Step (t+2)\")\n",
    "\n",
    "if 'task2_results' in dir() and task2_results is not None:\n",
    "    plot_training_curves(task2_results, \"Task 2: Multi-Step (t+1..t+5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128cd885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.2 PREDICTED VS OBSERVED ===\n",
    "\n",
    "def plot_predictions(results, task_name, n_samples=500):\n",
    "    \"\"\"Plot predicted vs observed scatter plot.\"\"\"\n",
    "    if results is None:\n",
    "        print(f\"No results available for {task_name}\")\n",
    "        return\n",
    "    \n",
    "    preds = results['predictions'].flatten()[:n_samples]\n",
    "    targets = results['targets'].flatten()[:n_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(targets, preds, alpha=0.3, s=10, c='steelblue')\n",
    "    \n",
    "    # 1:1 line\n",
    "    min_val = min(targets.min(), preds.min())\n",
    "    max_val = max(targets.max(), preds.max())\n",
    "    axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='1:1 Line')\n",
    "    \n",
    "    axes[0].set_xlabel('Observed (Normalized)')\n",
    "    axes[0].set_ylabel('Predicted (Normalized)')\n",
    "    axes[0].set_title(f'{task_name} - Predicted vs Observed')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual histogram\n",
    "    residuals = preds - targets\n",
    "    axes[1].hist(residuals, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Residual (Pred - Obs)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'{task_name} - Residual Distribution')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\\\nðŸ“Š Residual Statistics:\")\n",
    "    print(f\"   Mean: {np.mean(residuals):.4f}\")\n",
    "    print(f\"   Std:  {np.std(residuals):.4f}\")\n",
    "    print(f\"   Min:  {np.min(residuals):.4f}\")\n",
    "    print(f\"   Max:  {np.max(residuals):.4f}\")\n",
    "\n",
    "# Plot if results exist\n",
    "if 'task1_results' in dir() and task1_results is not None:\n",
    "    plot_predictions(task1_results, \"Task 1\")\n",
    "\n",
    "if 'task2_results' in dir() and task2_results is not None:\n",
    "    plot_predictions(task2_results, \"Task 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.3 TIME SERIES VISUALIZATION ===\n",
    "\n",
    "def plot_time_series(results, task_name, n_points=200):\n",
    "    \"\"\"Plot a segment of predicted vs observed time series.\"\"\"\n",
    "    if results is None:\n",
    "        print(f\"No results available for {task_name}\")\n",
    "        return\n",
    "    \n",
    "    preds = results['predictions'].flatten()[:n_points]\n",
    "    targets = results['targets'].flatten()[:n_points]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    \n",
    "    x = range(len(targets))\n",
    "    ax.plot(x, targets, 'b-', label='Observed', linewidth=1.5, alpha=0.8)\n",
    "    ax.plot(x, preds, 'r-', label='Predicted', linewidth=1.5, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Flow (Normalized)')\n",
    "    ax.set_title(f'{task_name} - Predicted vs Observed Time Series')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot if results exist\n",
    "if 'task1_results' in dir() and task1_results is not None:\n",
    "    plot_time_series(task1_results, \"Task 1\")\n",
    "\n",
    "if 'task2_results' in dir() and task2_results is not None:\n",
    "    plot_time_series(task2_results, \"Task 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3285bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.4 SUMMARY ===\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\\\nðŸ“ Configuration:\")\n",
    "print(f\"   Dataset: {cfg.BASE_DIR}\")\n",
    "print(f\"   Basins used: {cfg.NUM_BASINS}\")\n",
    "print(f\"   Sequence length: {cfg.SEQ_LENGTH} days\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "print(f\"\\\\nðŸ”§ Hyperparameters:\")\n",
    "print(f\"   Hidden dim: {cfg.HIDDEN_DIM}\")\n",
    "print(f\"   Dropout: {cfg.DROPOUT}\")\n",
    "print(f\"   Learning rate: {cfg.LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {cfg.EPOCHS}\")\n",
    "print(f\"   Batch size: {cfg.BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\\\nðŸ“Š Results:\")\n",
    "if 'task1_results' in dir() and task1_results is not None:\n",
    "    print(f\"   Task 1 (t+2) NSE: {task1_results['test_nse']:.4f}\")\n",
    "else:\n",
    "    print(\"   Task 1: Not run\")\n",
    "\n",
    "if 'task2_results' in dir() and task2_results is not None:\n",
    "    print(f\"   Task 2 (t+1..t+5) NSE: {task2_results['test_nse']:.4f}\")\n",
    "else:\n",
    "    print(\"   Task 2: Not run\")\n",
    "\n",
    "print(f\"\\\\nðŸ“‚ Results saved to: {cfg.RESULTS_DIR}/\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
